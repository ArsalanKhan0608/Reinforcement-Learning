{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90970623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 3.90251828e+01  6.85884161e+01  1.14233630e+00  1.46486740e+01]\n",
      "  [ 4.49134695e+01  7.02914859e+01  1.39407148e+01  3.10686781e+01]\n",
      "  [ 1.83563426e+01  7.20116491e+01  3.90689374e-01  4.59735714e+01]\n",
      "  [ 2.14644782e+01  7.37491551e+01  1.54248198e+01  4.88479437e+01]\n",
      "  [ 3.83787901e+01  1.15518943e+00  7.55042028e+01  3.20898970e+01]\n",
      "  [-3.15008834e+00 -3.17983619e+00  2.32352744e+01 -3.23555157e+00]\n",
      "  [-2.75115378e+00 -2.72092491e+00  1.99741086e+00 -2.70497108e+00]\n",
      "  [-2.36198355e+00 -2.35832290e+00 -2.33694648e+00 -2.30457284e+00]\n",
      "  [-2.07417514e+00 -2.08133187e+00 -2.01688352e+00 -2.01783863e+00]\n",
      "  [-1.97139021e+00 -1.98111352e+00 -1.99329691e+00 -1.93833626e+00]]\n",
      "\n",
      " [[ 2.79494143e+01 -6.35081395e+00 -6.27073080e+00 -6.27442960e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 2.79059657e+00  1.66530790e+01 -4.74915299e+00 -4.77039628e+00]\n",
      "  [-4.06173113e+00  5.99850798e+01 -4.06482455e+00 -4.23832954e+00]\n",
      "  [ 5.13395173e+01  2.26827293e+01  7.72769740e+01  5.63696066e+00]\n",
      "  [-3.13674117e+00 -2.86864546e+00  7.01094248e+01  3.17385306e-01]\n",
      "  [-2.55708998e+00 -2.41469118e+00  5.76004160e+01 -2.45441869e+00]\n",
      "  [-2.15180821e+00 -2.11406874e+00 -2.04722052e+00 -2.06471365e+00]\n",
      "  [-1.74808619e+00 -1.80576284e+00 -1.08579251e+00 -1.77571304e+00]\n",
      "  [-1.82638542e+00 -1.76297231e+00 -1.81529708e+00 -1.75776622e+00]]\n",
      "\n",
      " [[-5.49122660e+00 -5.50924365e+00 -5.51200014e+00 -5.50373224e+00]\n",
      "  [-4.95142677e+00 -4.96380473e+00 -4.92204955e+00 -5.03227255e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [-3.61653058e+00  5.05389693e+01  1.10711676e+00 -3.53770815e+00]\n",
      "  [ 5.28397017e+01  7.90676508e+01  1.29556173e+00  1.86853329e+01]\n",
      "  [ 2.38290628e+01  8.08764150e+01  2.99849197e+01  4.75349400e+01]\n",
      "  [ 2.55563232e+01  7.57974593e+00  8.27034495e+01  4.00538779e+01]\n",
      "  [-1.77790880e+00 -1.67117719e+00  2.62407057e+01  1.39356921e+01]\n",
      "  [-1.43069758e+00 -1.54688590e+00  4.57820866e+00 -1.56269156e+00]\n",
      "  [-1.58348205e+00 -1.48954536e+00 -1.52370411e+00 -9.60834694e-01]]\n",
      "\n",
      " [[-4.80451145e+00 -4.76510173e+00 -4.76973893e+00 -4.78422140e+00]\n",
      "  [-4.31977547e+00 -4.26650542e+00 -3.65442448e+00 -4.32499140e+00]\n",
      "  [-3.80601147e+00 -3.72931794e+00 -2.84545048e+00 -3.66218008e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [-2.52653692e+00 -2.45230002e+00  2.12601527e+01 -2.46437125e+00]\n",
      "  [-5.24148618e-02 -2.06161199e+00  6.63935204e+01 -2.23708407e+00]\n",
      "  [ 5.13521421e+01  4.23180951e+01  8.45489389e+01  2.09835072e+01]\n",
      "  [-1.47609871e+00  2.04961519e+00  8.08070551e+01  1.45964020e+01]\n",
      "  [-1.15694949e+00 -1.16441006e+00  6.22625165e+01 -1.16364852e+00]\n",
      "  [-1.27155776e+00 -1.19342195e+00 -1.04546497e+00 -1.20936299e+00]]\n",
      "\n",
      " [[-4.10564138e+00 -3.55741474e+00 -4.09505660e+00 -4.09778988e+00]\n",
      "  [-3.67718918e+00  1.18305109e+00 -3.62539079e+00 -3.87887331e+00]\n",
      "  [-3.08096188e+00  1.12329262e+01 -3.14567699e+00 -3.26496885e+00]\n",
      "  [-6.53593370e-01  2.86569277e+01 -2.46680002e+00 -2.51575092e+00]\n",
      "  [-2.13547830e+00  6.35910316e+01 -2.09818556e+00 -2.15961562e+00]\n",
      "  [-6.36920387e-01  8.41070349e+01 -1.65555982e+00 -1.74410573e+00]\n",
      "  [ 6.59226217e+01  8.64130696e+01  2.47204142e+00  5.24463560e+01]\n",
      "  [ 4.35540878e+01  8.82960299e+01  4.13394495e+01  5.57293478e+01]\n",
      "  [ 2.43257432e+01  1.74631550e+01  9.01980100e+01  6.04792959e+01]\n",
      "  [-9.72270264e-01 -1.08020282e+00 -9.59339707e-01  5.26873618e+01]]\n",
      "\n",
      " [[-3.56843399e+00 -3.51439624e+00 -3.46368829e+00 -3.52891205e+00]\n",
      "  [-3.23929844e+00 -3.11552186e+00 -2.81430215e+00 -3.12632634e+00]\n",
      "  [-2.68809625e+00 -2.64907279e+00 -2.26801080e+00 -2.77111613e+00]\n",
      "  [-2.22951045e+00 -2.15618668e+00 -2.15515506e+00 -2.36015862e+00]\n",
      "  [-1.78995976e+00 -1.78248466e+00 -1.64454429e+00 -1.86648459e+00]\n",
      "  [-1.36563825e+00 -9.97827870e-01 -1.32296435e+00 -1.54764501e+00]\n",
      "  [-9.51841332e-01  2.55227012e+01 -1.03573891e+00 -1.03862099e+00]\n",
      "  [ 7.99863657e+00  8.18867773e+01 -6.89039601e-01  7.17668028e-01]\n",
      "  [ 6.55195427e+01  3.04630318e+01  9.21192020e+01  3.27107958e+01]\n",
      "  [-7.50333381e-01 -6.88052898e-01 -6.35908931e-01  6.53945261e+01]]\n",
      "\n",
      " [[-2.97331016e+00 -2.79576266e+00 -2.89064677e+00 -2.94519383e+00]\n",
      "  [-2.69305902e+00 -2.54531957e+00  1.87752988e+00 -2.77889374e+00]\n",
      "  [-2.18057965e+00 -2.15467008e+00  2.39086644e+00 -2.24349195e+00]\n",
      "  [-1.73094158e+00 -8.40736012e-01 -1.74083663e+00 -1.81643299e+00]\n",
      "  [-1.49088935e+00 -1.33722618e+00  5.08253091e+00 -1.41633795e+00]\n",
      "  [-1.07777588e+00 -9.34248188e-01 -9.79621999e-01 -1.09137136e+00]\n",
      "  [-7.43884799e-01 -5.92854749e-01  4.42456857e-01 -7.55229466e-01]\n",
      "  [-5.07176861e-01 -4.80141451e-01  4.46057595e+01 -5.27139258e-01]\n",
      "  [ 6.78284024e+01  1.78824518e+01  9.40598000e+01  1.48215146e+01]\n",
      "  [-4.26165007e-01 -3.99400400e-01  5.38307651e+01 -4.07586035e-01]]\n",
      "\n",
      " [[-2.41877935e+00 -1.63524456e+00 -2.43510573e+00 -2.46760287e+00]\n",
      "  [-2.01997613e+00  1.07398295e+01 -2.16696555e+00 -2.33710076e+00]\n",
      "  [-1.80873368e+00 -1.70855718e+00  2.48817171e+01 -1.27959274e+00]\n",
      "  [-1.45601018e+00 -1.40971619e+00  5.91539701e+00 -1.52093916e+00]\n",
      "  [-1.06725423e+00 -1.03743637e+00  2.30068242e+01 -1.06099549e+00]\n",
      "  [-7.21691141e-01 -7.10374875e-01  3.42755319e+00 -7.57392629e-01]\n",
      "  [-4.10708361e-01  2.25103859e+01 -4.45993540e-01 -4.33669830e-01]\n",
      "  [-3.00582190e-01  8.56338024e+01  6.48930768e+00  2.70463557e+00]\n",
      "  [ 6.24122051e+01  6.88313492e+01  9.60200000e+01  3.84411119e+01]\n",
      "  [ 3.25639602e+00 -1.99900000e-01  2.64194900e+01  9.23930752e+01]]\n",
      "\n",
      " [[-2.12481892e+00 -1.97151515e+00 -2.05644393e+00 -2.05316732e+00]\n",
      "  [-2.00148767e+00  1.60530030e+00 -1.87694139e+00 -1.84216778e+00]\n",
      "  [-2.92690381e-01  4.30541702e+01 -1.53029907e+00 -1.70603541e+00]\n",
      "  [-1.22334148e+00  6.30357279e+01 -1.34097685e+00 -1.29168103e+00]\n",
      "  [ 9.15395532e-01  7.74269843e+01 -8.78487873e-01  4.14366070e-01]\n",
      "  [-5.92107405e-01  8.68879602e+01 -6.79392885e-01 -6.22810467e-01]\n",
      "  [ 1.43857411e+00  9.26146038e+01 -1.05377789e-01 -3.29056697e-01]\n",
      "  [ 2.46500588e+00  9.59741471e+01  2.39967447e+00 -1.09900000e-01]\n",
      "  [ 4.00609923e+01  9.80000000e+01  2.08380652e+01  7.44815998e+01]\n",
      "  [ 5.27341282e+01  7.21747542e+01  1.00000000e+02  5.88973363e+01]]\n",
      "\n",
      " [[-1.96838319e+00 -1.89282420e+00 -1.87648297e+00 -1.88299651e+00]\n",
      "  [-1.79018925e+00 -1.80475967e+00 -1.78478129e+00 -1.74975621e+00]\n",
      "  [-1.48909084e+00 -1.52670288e+00 -1.57509973e+00 -1.55417652e+00]\n",
      "  [ 4.22267986e-01 -1.23880873e+00 -1.28440751e+00 -1.29236636e+00]\n",
      "  [-8.90603453e-01 -9.36415697e-01 -8.87551621e-01 -9.70605788e-01]\n",
      "  [-4.37270432e-01 -6.81891370e-01 -6.80067517e-01 -6.33391856e-01]\n",
      "  [ 9.85561483e+00 -3.16313482e-01 -3.99400400e-01 -4.61018756e-01]\n",
      "  [ 2.55976987e+01 -1.00000000e-01 -1.00000000e-01 -1.09900000e-01]\n",
      "  [ 5.89235273e+01  0.00000000e+00  0.00000000e+00 -1.04390704e-01]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n",
      "Q-Learning completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MazeEnv:\n",
    "    def __init__(self, width=10, height=10, start=(0, 0), goal=(9, 9), obstacles=None):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.start = start\n",
    "        self.current_position = start\n",
    "        self.goal = goal\n",
    "        if obstacles is None:\n",
    "            obstacles = []\n",
    "        self.obstacles = obstacles\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_position = self.start\n",
    "        return self.current_position\n",
    "\n",
    "    def step(self, action):\n",
    "        next_position = list(self.current_position)\n",
    "        if action == 0:  # Up\n",
    "            next_position[1] -= 1\n",
    "        elif action == 1:  # Right\n",
    "            next_position[0] += 1\n",
    "        elif action == 2:  # Down\n",
    "            next_position[1] += 1\n",
    "        elif action == 3:  # Left\n",
    "            next_position[0] -= 1\n",
    "\n",
    "        if (0 <= next_position[0] < self.width and\n",
    "            0 <= next_position[1] < self.height and\n",
    "            tuple(next_position) not in self.obstacles):\n",
    "            self.current_position = tuple(next_position)\n",
    "\n",
    "        reward = -1  # default reward\n",
    "        done = False\n",
    "        if self.current_position == self.goal:\n",
    "            reward = 100\n",
    "            done = True\n",
    "\n",
    "        return self.current_position, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        maze = np.zeros((self.height, self.width))\n",
    "        for obstacle in self.obstacles:\n",
    "            maze[obstacle[1]][obstacle[0]] = -1\n",
    "        maze[self.goal[1]][self.goal[0]] = 0.5\n",
    "        maze[self.current_position[1]][self.current_position[0]] = 1\n",
    "        plt.imshow(maze)\n",
    "        plt.show()\n",
    "\n",
    "def q_learning(env, episodes=500, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    q_table = np.zeros((env.height, env.width, 4))\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.choice([0, 1, 2, 3])\n",
    "            else:\n",
    "                action = np.argmax(q_table[state[1], state[0]])\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            old_value = q_table[state[1], state[0], action]\n",
    "            next_max = np.max(q_table[next_state[1], next_state[0]])\n",
    "\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state[1], state[0], action] = new_value\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return q_table\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obstacles = [(1, 1), (2, 2), (3, 3)]\n",
    "    env = MazeEnv(width=10, height=10, start=(0, 0), goal=(9, 9), obstacles=obstacles)\n",
    "    q_table = q_learning(env)\n",
    "    print(q_table)\n",
    "    print(\"Q-Learning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "class ComplexMazeEnv:\n",
    "    def __init__(self, size=10, start=(0, 0), goal=None, complexity=0.3, density=0.3):\n",
    "        self.size = size\n",
    "        self.start = start\n",
    "        self.complexity = complexity\n",
    "        self.density = density\n",
    "        self.maze = self._generate_maze()\n",
    "        self.goal = goal if goal else (size - 1, size - 1)\n",
    "        self.current_position = start\n",
    "\n",
    "    def _generate_maze(self):\n",
    "        shape = ((self.size // 2) * 2 + 1, (self.size // 2) * 2 + 1)\n",
    "        complexity = int(self.complexity * (5 * (shape[0] + shape[1])))\n",
    "        density = int(self.density * ((shape[0] // 2) * (shape[1] // 2)))\n",
    "        maze = np.zeros(shape, dtype=bool)\n",
    "\n",
    "        maze[0, :] = maze[-1, :] = maze[:, 0] = maze[:, -1] = 1\n",
    "        for i in range(density):\n",
    "            x, y = random.randint(0, shape[1] // 2) * 2, random.randint(0, shape[0] // 2) * 2\n",
    "            maze[y, x] = 1\n",
    "            for j in range(complexity):\n",
    "                neighbours = []\n",
    "                if x > 1:           neighbours.append((y, x - 2))\n",
    "                if x < shape[1] - 2: neighbours.append((y, x + 2))\n",
    "                if y > 1:           neighbours.append((y - 2, x))\n",
    "                if y < shape[0] - 2: neighbours.append((y + 2, x))\n",
    "                if len(neighbours):\n",
    "                    y_, x_ = neighbours[random.randint(0, len(neighbours) - 1)]\n",
    "                    if maze[y_, x_] == 0:\n",
    "                        maze[y_, x_] = 1\n",
    "                        maze[y_ + (y - y_) // 2, x_ + (x - x_) // 2] = 1\n",
    "                        x, y = x_, y_\n",
    "        return maze\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_position = self.start\n",
    "        return self.current_position\n",
    "\n",
    "    def step(self, action):\n",
    "        y, x = self.current_position\n",
    "        if action == 0 and y > 0 and not self.maze[y - 1, x]:  # Up\n",
    "            self.current_position = (y - 1, x)\n",
    "        if action == 1 and x < self.size - 1 and not self.maze[y, x + 1]:  # Right\n",
    "            self.current_position = (y, x + 1)\n",
    "        if action == 2 and y < self.size - 1 and not self.maze[y + 1, x]:  # Down\n",
    "            self.current_position = (y + 1, x)\n",
    "        if action == 3 and x > 0 and not self.maze[y, x - 1]:  # Left\n",
    "            self.current_position = (y, x - 1)\n",
    "\n",
    "        reward = -0.1\n",
    "        done = False\n",
    "        if self.current_position == self.goal:\n",
    "            reward = 10\n",
    "            done = True\n",
    "\n",
    "        return self.current_position, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(self.maze, cmap='binary')\n",
    "        y, x = self.start\n",
    "        gy, gx = self.goal\n",
    "        plt.scatter([x, gx], [y, gy], color=['green', 'red'])  # start and goal\n",
    "        cy, cx = self.current_position\n",
    "        plt.scatter(cx, cy, color='blue')  # current position\n",
    "        plt.show()\n",
    "\n",
    "def q_learning(env, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    q_table = np.random.uniform(low=-1, high=1, size=(env.size, env.size, 4))\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.randint(0, 3)\n",
    "            else:\n",
    "                action = np.argmax(q_table[state[0], state[1]])\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            old_value = q_table[state[0], state[1], action]\n",
    "            next_max = np.max(q_table[next_state[0], next_state[1]])\n",
    "\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state[0], state[1], action] = new_value\n",
    "            state = next_state\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode + 1}/{episodes} completed\")\n",
    "\n",
    "    return q_table\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = ComplexMazeEnv(size=15, complexity=0.2, density=0.3)\n",
    "    q_table = q_learning(env, episodes=1000)\n",
    "    print(\"Complex Q-Learning with custom maze environment completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d8d46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# --- Custom Maze Environment ---\n",
    "\n",
    "class MazeEnv:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.maze = np.zeros((size, size))\n",
    "        self.generate_maze()\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (size - 1, size - 1)\n",
    "\n",
    "    def generate_maze(self):\n",
    "        # Create obstacles (example)\n",
    "        self.maze[1:3, 2] = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = self.start\n",
    "        return self.state_to_index(self.agent_pos)\n",
    "\n",
    "    def step(self, action):\n",
    "        new_row, new_col = self.agent_pos\n",
    "\n",
    "        if action == 0:  # Up\n",
    "            new_row -= 1\n",
    "        elif action == 1:  # Down\n",
    "            new_row += 1\n",
    "        elif action == 2:  # Left\n",
    "            new_col -= 1\n",
    "        elif action == 3:  # Right\n",
    "            new_col += 1\n",
    "\n",
    "        new_pos = (new_row, new_col)\n",
    "\n",
    "        if self.is_valid_move(new_pos):\n",
    "            self.agent_pos = new_pos\n",
    "            if new_pos == self.goal:\n",
    "                reward = 100\n",
    "                done = True\n",
    "            else:\n",
    "                reward = -1\n",
    "                done = False\n",
    "        else:\n",
    "            reward = -5  # Penalty for invalid move\n",
    "            done = False\n",
    "\n",
    "        return self.state_to_index(new_pos), reward, done\n",
    "\n",
    "    def is_valid_move(self, pos):\n",
    "        row, col = pos\n",
    "        return (0 <= row < self.size and 0 <= col < self.size and\n",
    "                self.maze[row, col] == 0)\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        row, col = state\n",
    "        return min(row * self.size + col, self.size * self.size - 1)\n",
    "\n",
    "# --- Q-Learning Implementation ---\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.8, discount_factor=0.95):\n",
    "        self.env = env\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.actions = [0, 1, 2, 3]  # Up, Down, Left, Right\n",
    "        self.q_table = np.zeros((env.size * env.size, len(self.actions)))\n",
    "\n",
    "    def choose_action(self, state, epsilon=0.1):\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        current_q = self.q_table[state, action]\n",
    "        max_future_q = np.max(self.q_table[next_state])\n",
    "        new_q = (1 - self.lr) * current_q + self.lr * (reward + self.gamma * max_future_q)\n",
    "        self.q_table[state, action] = new_q\n",
    "\n",
    "# --- Training ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = MazeEnv()\n",
    "    agent = QLearningAgent(env)\n",
    "    episodes = 5000\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ab2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
