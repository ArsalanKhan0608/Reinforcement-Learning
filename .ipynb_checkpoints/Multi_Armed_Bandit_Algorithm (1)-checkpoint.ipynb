{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36eb9c67-498d-4e1f-ba7b-0706c322cf95",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "# The Multi-Armed Bandit Algorithm\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Imagine you're in a casino, standing in front of a row of slot machines, commonly known as 'one-armed bandits'. Each machine provides a different rate of return, but you don't know what these rates are. Your goal is to maximize your total reward by pulling the arms of these machines in some sequence. This scenario is a classic problem in probability theory and is known as the Multi-Armed Bandit Problem.\n",
    "\n",
    "In this notebook, we'll explore the Multi-Armed Bandit Algorithm, its importance, drawbacks, and real-world applications. We'll also provide exercises along with solutions to deepen your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499adb2d-8f2f-4d08-ad65-55c865da22aa",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## What is the Multi-Armed Bandit Algorithm?\n",
    "\n",
    "The Multi-Armed Bandit Algorithm is a decision-making algorithm used to solve the Multi-Armed Bandit Problem. The problem involves a gambler who has to decide which arms of multi-armed bandits to pull to maximize his reward. Each arm provides a reward drawn from a probability distribution specific to that arm. The gambler doesn't know these distributions and has to learn them through trial and error.\n",
    "\n",
    "The algorithm aims to balance the trade-off between exploration (trying out each arm to find out its reward distribution) and exploitation (pulling the arm that is currently known to give the best average reward).\n",
    "\n",
    "![Multi-Armed Bandit](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fc/Multi-armed_bandit.svg/500px-Multi-armed_bandit.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24b24b-d64b-470c-9793-de8cd2c5af41",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Importance of the Multi-Armed Bandit Algorithm\n",
    "\n",
    "The Multi-Armed Bandit Algorithm is crucial for various applications:\n",
    "\n",
    "- **Online Advertising**: It helps in selecting the most effective ad to display to a user.\n",
    "- **Clinical Trials**: It aids in choosing the most effective treatment among multiple options.\n",
    "- **Web Page Optimization**: It is used in A/B testing to dynamically allocate traffic to different versions of a web page.\n",
    "- **Robotics**: In robotic arms, it helps in selecting the most efficient sequence of movements.\n",
    "- **Finance**: It is used in portfolio optimization to select the best-performing stocks.\n",
    "\n",
    "The algorithm is favored for its ability to make optimal decisions in real-time, adapt to changing conditions, and minimize regret (the difference between the actual reward and the best possible reward)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da624cb9-0371-4bc2-b67a-d1134dadc995",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Drawbacks of the Multi-Armed Bandit Algorithm\n",
    "\n",
    "While the algorithm is powerful, it has its limitations:\n",
    "\n",
    "- **Computational Complexity**: Some versions of the algorithm can be computationally intensive.\n",
    "- **Initial Exploration**: Too much initial exploration can lead to suboptimal results.\n",
    "- **Non-Stationarity**: The algorithm assumes that the reward distributions are stationary, which may not be the case in real-world scenarios.\n",
    "- **Delayed Feedback**: In some applications, the reward feedback may be delayed, affecting the algorithm's performance.\n",
    "\n",
    "Understanding these drawbacks is essential for effectively applying the algorithm in various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc61c97-c374-42e7-b964-632082eb5909",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Real-World Applications\n",
    "\n",
    "Let's delve into some real-world applications where the Multi-Armed Bandit Algorithm shines:\n",
    "\n",
    "- **Healthcare**: In personalized medicine, the algorithm helps in tailoring treatments to individual patients.\n",
    "- **E-commerce**: It is used for product recommendation systems to suggest products that are likely to be purchased.\n",
    "- **Natural Resource Exploration**: In oil drilling, it helps in deciding where to drill next.\n",
    "- **Game Playing**: In games like poker, it aids in making optimal betting decisions.\n",
    "\n",
    "These applications showcase the algorithm's versatility and its capability to adapt and optimize in different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb2ee0e-4e0e-404c-8020-af6c57678fac",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "To deepen your understanding, let's go through some exercises:\n",
    "\n",
    "1. **Greedy vs Epsilon-Greedy**: Implement both the Greedy and Epsilon-Greedy algorithms and compare their performance.\n",
    "2. **Softmax Exploration**: Implement the Softmax Exploration strategy and compare it with Epsilon-Greedy.\n",
    "3. **UCB (Upper Confidence Bound)**: Implement the UCB algorithm and compare its performance with Epsilon-Greedy.\n",
    "\n",
    "Try to solve these exercises before moving on to the solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c74897-e647-4253-bef1-6e3eaf578871",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulated slot machines (bandit arms)\n",
    "true_means = [0.1, 0.5, 0.8]\n",
    "\n",
    "# Function to pull an arm\n",
    "def pull_arm(mean):\n",
    "    return np.random.normal(mean, 1)\n",
    "\n",
    "# Greedy Algorithm\n",
    "def greedy(true_means, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        best_arm = np.argmax(estimated_means)\n",
    "        reward = pull_arm(true_means[best_arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[best_arm] += 1\n",
    "        estimated_means[best_arm] = ((n_pulls[best_arm] - 1) * estimated_means[best_arm] + reward) / n_pulls[best_arm]\n",
    "    return np.sum(rewards)\n",
    "\n",
    "# Epsilon-Greedy Algorithm\n",
    "def epsilon_greedy(true_means, epsilon=0.1, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        if np.random.rand() < epsilon:\n",
    "            arm = np.random.randint(0, 3)\n",
    "        else:\n",
    "            arm = np.argmax(estimated_means)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba155a8e-4b7b-4935-84a2-46e76b96dd28",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Run the Greedy and Epsilon-Greedy algorithms and compare their performance\n",
    "greedy_reward = greedy(true_means)\n",
    "epsilon_greedy_reward = epsilon_greedy(true_means)\n",
    "greedy_reward, epsilon_greedy_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212337c9-cf21-4a3f-b924-0751e4968a9e",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to simulate pulling an arm\n",
    "def pull_arm(mean):\n",
    "    return np.random.normal(mean, 1)\n",
    "\n",
    "# Greedy Algorithm\n",
    "def greedy(true_means, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        best_arm = np.argmax(estimated_means)\n",
    "        reward = pull_arm(true_means[best_arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[best_arm] += 1\n",
    "        estimated_means[best_arm] = ((n_pulls[best_arm] - 1) * estimated_means[best_arm] + reward) / n_pulls[best_arm]\n",
    "    return np.sum(rewards), rewards\n",
    "\n",
    "# Epsilon-Greedy Algorithm\n",
    "def epsilon_greedy(true_means, epsilon=0.1, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        if np.random.rand() < epsilon:\n",
    "            arm = np.random.randint(0, 3)\n",
    "        else:\n",
    "            arm = np.argmax(estimated_means)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "    return np.sum(rewards), rewards\n",
    "\n",
    "# Simulated slot machines (bandit arms)\n",
    "true_means = [0.1, 0.5, 0.8]\n",
    "\n",
    "# Compare Greedy and Epsilon-Greedy\n",
    "greedy_reward, greedy_rewards = greedy(true_means)\n",
    "epsilon_greedy_reward, epsilon_greedy_rewards = epsilon_greedy(true_means)\n",
    "greedy_reward, epsilon_greedy_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a603fee-4f38-4014-bff7-18bb29df19f2",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Solutions to Exercises\n",
    "\n",
    "### Solution to Exercise 1: Greedy vs Epsilon-Greedy\n",
    "\n",
    "We implemented both the Greedy and Epsilon-Greedy algorithms and ran them to compare their performance. The Greedy algorithm always chooses the arm with the highest estimated mean reward, while the Epsilon-Greedy algorithm chooses a random arm with probability \\(\\epsilon\\) and the best arm otherwise.\n",
    "\n",
    "#### Code Explanation\n",
    "\n",
    "- `true_means`: The true mean rewards for each arm.\n",
    "- `pull_arm(mean)`: Function to simulate pulling an arm with a given mean reward.\n",
    "- `greedy(true_means, n_rounds=100)`: Greedy algorithm implementation.\n",
    "- `epsilon_greedy(true_means, epsilon=0.1, n_rounds=100)`: Epsilon-Greedy algorithm implementation.\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "We ran the code, but it seems the cell execution is taking longer than expected. You can check the cell's status in your [Noteable notebook](https://app.noteable.io/f/63498666-7e92-4902-beb8-5b18fcdd8f3f/?cellID=ba155a8e-4b7b-4935-84a2-46e76b96dd28).\n",
    "\n",
    "In general, the Epsilon-Greedy algorithm is expected to perform better in the long run as it balances exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c019c7-79f1-472a-81fd-767def30f143",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Exercise 1: Greedy vs Epsilon-Greedy - Explanation and Results\n",
    "\n",
    "In the above code, we implemented both the Greedy and Epsilon-Greedy algorithms and compared their performance. The Greedy algorithm always chooses the arm with the highest estimated mean reward, while the Epsilon-Greedy algorithm explores random arms with probability epsilon and exploits the best-known arm with probability 1-epsilon.\n",
    "\n",
    "We simulated the scenario with three slot machines having true mean rewards of 0.1, 0.5, and 0.8. After running 100 rounds, we observed the total rewards for both algorithms.\n",
    "\n",
    "To view the results, you can check the code cell output [here](https://app.noteable.io/f/63498666-7e92-4902-beb8-5b18fcdd8f3f/?cellID=212337c9-cf21-4a3f-b924-0751e4968a9e).\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "You'll likely notice that the Epsilon-Greedy algorithm performs better in the long run as it balances exploration and exploitation. The Greedy algorithm might get stuck exploiting a suboptimal arm if it doesn't explore enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ada7a7-2e7c-4f8b-8097-67e8d77f4496",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Softmax Exploration Algorithm\n",
    "def softmax_exploration(true_means, temperature=0.1, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        softmax_prob = np.exp(estimated_means / temperature) / np.sum(np.exp(estimated_means / temperature))\n",
    "        arm = np.random.choice([0, 1, 2], p=softmax_prob)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "    return np.sum(rewards), rewards\n",
    "\n",
    "# Compare Softmax Exploration with Epsilon-Greedy\n",
    "softmax_reward, softmax_rewards = softmax_exploration(true_means)\n",
    "softmax_reward, epsilon_greedy_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa9d67-a0c0-4f14-8dfc-d4359ef73c29",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Exercise 2: Softmax Exploration - Explanation and Results\n",
    "\n",
    "In the above code, we implemented the Softmax Exploration strategy. Unlike Epsilon-Greedy, which makes a binary choice between exploration and exploitation, Softmax Exploration uses a probabilistic approach. It assigns a probability of being chosen to each arm, based on its estimated value. The temperature parameter controls the level of exploration: a higher temperature leads to more exploration and a lower temperature to more exploitation.\n",
    "\n",
    "We compared the Softmax Exploration strategy with the Epsilon-Greedy algorithm using the same simulated slot machines.\n",
    "\n",
    "To view the results, you can check the code cell output [here](https://app.noteable.io/f/63498666-7e92-4902-beb8-5b18fcdd8f3f/?cellID=42ada7a7-2e7c-4f8b-8097-67e8d77f4496).\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "Softmax Exploration is generally more nuanced than Epsilon-Greedy and can perform better when the reward distributions are close to each other, requiring a more delicate balance between exploration and exploitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5578fd8-42e8-4927-a74f-f43d9d543400",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "# UCB (Upper Confidence Bound) Algorithm\n",
    "import math\n",
    "\n",
    "def ucb(true_means, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "    for t in range(1, n_rounds + 1):\n",
    "        ucb_values = [estimated_means[i] + math.sqrt(2 * math.log(t) / (n_pulls[i] + 1e-5)) for i in range(3)]\n",
    "        arm = np.argmax(ucb_values)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "    return np.sum(rewards), rewards\n",
    "\n",
    "# Compare UCB with Epsilon-Greedy\n",
    "ucb_reward, ucb_rewards = ucb(true_means)\n",
    "ucb_reward, epsilon_greedy_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5e6c2-8ece-4ec6-8028-84460975c3b0",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Exercise 3: UCB (Upper Confidence Bound) - Explanation and Results\n",
    "\n",
    "In the above code, we implemented the UCB (Upper Confidence Bound) algorithm. UCB is another strategy to solve the Multi-Armed Bandit problem that takes into account both the estimated reward and the uncertainty around that estimate. The algorithm selects the arm with the highest upper confidence bound, calculated as the sum of the estimated mean reward and a confidence interval.\n",
    "\n",
    "We compared the UCB algorithm with the Epsilon-Greedy algorithm using the same simulated slot machines.\n",
    "\n",
    "To view the results, you can check the code cell output [here](https://app.noteable.io/f/63498666-7e92-4902-beb8-5b18fcdd8f3f/?cellID=e5578fd8-42e8-4927-a74f-f43d9d543400).\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "UCB tends to perform well when there is high uncertainty in the estimated rewards. It dynamically adjusts the level of exploration based on the confidence interval, making it a robust choice for various applications."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "noteable": {
   "last_delta_id": "b6ed82da-8f24-4c78-959c-ed2bb90409de"
  },
  "noteable-chatgpt": {
   "create_notebook": {
    "openai_conversation_id": "8e9f926a-ce11-5aa9-95c1-bd26017668fc",
    "openai_ephemeral_user_id": "670508ae-3062-521d-b2f7-a8582dcb1409",
    "openai_subdivision1_iso_code": "PK-PB"
   }
  },
  "nteract": {
   "version": "noteable@2.9.0"
  },
  "selected_hardware_size": "small"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
