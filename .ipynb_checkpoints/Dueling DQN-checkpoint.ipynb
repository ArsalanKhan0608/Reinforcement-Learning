{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309678fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class MazeEnv:\n",
    "    def __init__(self, size=5, obstacles=2, goal=(4, 4)):\n",
    "        self.size = size\n",
    "        self.obstacles = obstacles\n",
    "        self.goal = goal\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        self.maze = np.zeros((self.size, self.size))\n",
    "        self._add_obstacles()\n",
    "        self.maze[self.goal] = 0.5  # Mark goal on the maze\n",
    "        return self.state\n",
    "\n",
    "    def _add_obstacles(self):\n",
    "        for _ in range(self.obstacles):\n",
    "            obs = (np.random.randint(self.size), np.random.randint(self.size))\n",
    "            while obs == self.goal or obs == (0, 0):\n",
    "                obs = (np.random.randint(self.size), np.random.randint(self.size))\n",
    "            self.maze[obs] = -1\n",
    "\n",
    "    def step(self, action):\n",
    "        moves = [(0, -1), (0, 1), (-1, 0), (1, 0)]\n",
    "        next_state = tuple(np.add(self.state, moves[action]))\n",
    "\n",
    "        if 0 <= next_state[0] < self.size and 0 <= next_state[1] < self.size:\n",
    "            if self.maze[next_state] != -1:  # Check for obstacles\n",
    "                self.state = next_state\n",
    "\n",
    "        done = self.state == self.goal\n",
    "        reward = 1 if done else -0.1\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        maze_copy = self.maze.copy()\n",
    "        maze_copy[self.state] = 0.75  # Mark agent on the maze\n",
    "        print(maze_copy)\n",
    "\n",
    "\n",
    "class DuelingDQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = layers.Input(shape=(self.state_size,))\n",
    "        fc1 = layers.Dense(24, activation='relu')(inputs)\n",
    "        fc2 = layers.Dense(24, activation='relu')(fc1)\n",
    "\n",
    "        # Dueling Networks\n",
    "        fc_value = layers.Dense(24, activation='relu')(fc2)\n",
    "        value = layers.Dense(1, activation='linear')(fc_value)\n",
    "\n",
    "        fc_advantages = layers.Dense(24, activation='relu')(fc2)\n",
    "        advantages = layers.Dense(self.action_size, activation='linear')(fc_advantages)\n",
    "\n",
    "        output = value + (advantages - tf.reduce_mean(advantages, axis=1, keepdims=True))\n",
    "\n",
    "        model = models.Model(inputs=inputs, outputs=output)\n",
    "        model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        target = self.model.predict(state)\n",
    "        if done:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            Q_future = max(self.model.predict(next_state)[0])\n",
    "            target[0][action] = reward + Q_future * 0.95\n",
    "\n",
    "        self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "    def act(self, state):\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = MazeEnv(size=5, obstacles=2)\n",
    "    state_size = 2\n",
    "    action_size = 4\n",
    "    episodes = 1000\n",
    "\n",
    "    agent = DuelingDQN(state_size, action_size)\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.train(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "        if (e + 1) % 100 == 0:\n",
    "            print(f\"Episode: {e + 1}/{episodes}\")\n",
    "\n",
    "    print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f49c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MazeEnv:\n",
    "    def __init__(self, maze_layout, start_state, goal_state):\n",
    "        self.maze = maze_layout\n",
    "        self.rows = len(maze_layout)\n",
    "        self.cols = len(maze_layout[0])\n",
    "        self.start_state = start_state\n",
    "        self.goal_state = goal_state\n",
    "        self.state = start_state\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        new_row = self.state[0] + (action == 0) - (action == 2)  # Up/Down\n",
    "        new_col = self.state[1] + (action == 1) - (action == 3)  # Right/Left\n",
    "\n",
    "        # Check bounds\n",
    "        if 0 <= new_row < self.rows and 0 <= new_col < self.cols:\n",
    "            if self.maze[new_row][new_col] != 1:  # Not a wall\n",
    "                self.state = (new_row, new_col)\n",
    "\n",
    "        reward = -1 # Default move penalty\n",
    "        done = False\n",
    "        if self.state == self.goal_state:\n",
    "            reward = 10 # Reward for reaching the goal\n",
    "            done = True\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        maze_copy = np.copy(self.maze)\n",
    "        maze_copy[self.state[0]][self.state[1]] = 'A'  # Mark the agent's position\n",
    "        for row in maze_copy:\n",
    "            print(\" \".join(['_' if x == 0 else 'X' for x in row]))\n",
    "\n",
    "    \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Input, concatenate\n",
    "from collections import deque\n",
    "\n",
    "class DuelingDQNAgent:\n",
    "    def __init__(self, num_states, num_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = Input(shape=(2,))  # Update this line\n",
    "        hidden = Dense(32, activation='relu')(inputs)\n",
    "        value = Dense(1)(hidden) # State Value\n",
    "        advantage = Dense(self.num_actions)(hidden) # Advantage for each action\n",
    "\n",
    "        # Normalization \n",
    "        advantage = advantage - tf.reduce_mean(advantage, axis=1, keepdims=True) \n",
    "\n",
    "        output = value + advantage \n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=output)\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            q_values = self.model.predict(np.expand_dims(state, axis=0))[0] \n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            q_update = reward \n",
    "            if not done:\n",
    "                q_update += self.gamma * np.amax(self.model.predict(np.expand_dims(next_state, axis=0))[0]) \n",
    "\n",
    "            q_values = self.model.predict(np.expand_dims(state, axis=0))\n",
    "            q_values[0][action] = q_update \n",
    "            self.model.fit(np.expand_dims(state, axis=0), q_values, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129bb698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "# --- Create your maze ---\n",
    "maze_layout = [\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 1], \n",
    "    [0, 0, 0, 0, 0],\n",
    "]\n",
    "start_state = (0, 0)\n",
    "goal_state = (4, 4)\n",
    "\n",
    "# --- Initialize classes ---\n",
    "env = MazeEnv(maze_layout, start_state, goal_state)\n",
    "num_states = env.rows * env.cols  \n",
    "num_actions = 4  \n",
    "agent = DuelingDQNAgent(num_states, num_actions)\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_episodes = 500\n",
    "batch_size = 32\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        # Start replay only if there's enough experience\n",
    "        if len(agent.memory) >= batch_size: \n",
    "            agent.replay(batch_size)\n",
    "\n",
    "    print(f\"Episode {episode} finished\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130033f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
