{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc703057",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/home/arsalan/anaconda3/lib/python3.8/site-packages/tensorboard/compat/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 34.5     |\n",
      "|    ep_rew_mean        | 34.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 567      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.68    |\n",
      "|    explained_variance | -0.53    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 2.05     |\n",
      "|    value_loss         | 13.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 31.5     |\n",
      "|    ep_rew_mean        | 31.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 590      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.622   |\n",
      "|    explained_variance | 0.109    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.6      |\n",
      "|    value_loss         | 10.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 33       |\n",
      "|    ep_rew_mean        | 33       |\n",
      "| time/                 |          |\n",
      "|    fps                | 607      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.601   |\n",
      "|    explained_variance | -0.0311  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 1.53     |\n",
      "|    value_loss         | 7.12     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 34.6     |\n",
      "|    ep_rew_mean        | 34.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 615      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.572   |\n",
      "|    explained_variance | -0.0218  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 1        |\n",
      "|    value_loss         | 5.85     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 36.2     |\n",
      "|    ep_rew_mean        | 36.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 615      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.491   |\n",
      "|    explained_variance | -0.0164  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 1.29     |\n",
      "|    value_loss         | 5.67     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 38.2     |\n",
      "|    ep_rew_mean        | 38.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 619      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.414   |\n",
      "|    explained_variance | -0.0127  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 1.48     |\n",
      "|    value_loss         | 5.09     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 40.3     |\n",
      "|    ep_rew_mean        | 40.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 619      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.449   |\n",
      "|    explained_variance | 0.0193   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 1.6      |\n",
      "|    value_loss         | 4.27     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 43.4     |\n",
      "|    ep_rew_mean        | 43.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 612      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.5     |\n",
      "|    explained_variance | -0.00206 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.54     |\n",
      "|    value_loss         | 3.92     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 45.7     |\n",
      "|    ep_rew_mean        | 45.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 609      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.442   |\n",
      "|    explained_variance | -0.0124  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 1.01     |\n",
      "|    value_loss         | 3.39     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 49.8      |\n",
      "|    ep_rew_mean        | 49.8      |\n",
      "| time/                 |           |\n",
      "|    fps                | 605       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.527    |\n",
      "|    explained_variance | -0.000374 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 0.419     |\n",
      "|    value_loss         | 2.91      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 53.8     |\n",
      "|    ep_rew_mean        | 53.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 605      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.4     |\n",
      "|    explained_variance | 0.00102  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.469    |\n",
      "|    value_loss         | 2.46     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 57.1     |\n",
      "|    ep_rew_mean        | 57.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 606      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.362   |\n",
      "|    explained_variance | 0.0013   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.403    |\n",
      "|    value_loss         | 2.05     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 60.4     |\n",
      "|    ep_rew_mean        | 60.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 608      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.319   |\n",
      "|    explained_variance | -0.00049 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.623    |\n",
      "|    value_loss         | 1.68     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 62.2     |\n",
      "|    ep_rew_mean        | 62.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 610      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.458   |\n",
      "|    explained_variance | -7.5e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.397    |\n",
      "|    value_loss         | 1.36     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 65.2     |\n",
      "|    ep_rew_mean        | 65.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 612      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.373   |\n",
      "|    explained_variance | -0.00128 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.539    |\n",
      "|    value_loss         | 1.05     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 69.9     |\n",
      "|    ep_rew_mean        | 69.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 612      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.451   |\n",
      "|    explained_variance | 5.28e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.307    |\n",
      "|    value_loss         | 0.769    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 73.3      |\n",
      "|    ep_rew_mean        | 73.3      |\n",
      "| time/                 |           |\n",
      "|    fps                | 613       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.355    |\n",
      "|    explained_variance | -0.000374 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 0.105     |\n",
      "|    value_loss         | 0.533     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 77.9     |\n",
      "|    ep_rew_mean        | 77.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 613      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.341   |\n",
      "|    explained_variance | 0.000569 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.175    |\n",
      "|    value_loss         | 0.339    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 84.7     |\n",
      "|    ep_rew_mean        | 84.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 610      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.303   |\n",
      "|    explained_variance | 0.000175 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.222    |\n",
      "|    value_loss         | 0.191    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 88.3     |\n",
      "|    ep_rew_mean        | 88.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 607      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.207   |\n",
      "|    explained_variance | 0.000244 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.0137   |\n",
      "|    value_loss         | 0.0852   |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10_000)\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")\n",
    "    # VecEnv resets automatically\n",
    "    # if done:\n",
    "    obs = vec_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd25879",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000, log_interval=4)\n",
    "model.save(\"dqn_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = DQN.load(\"dqn_cartpole\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becab3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    print(f\"action: {action}\")\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"observation: {observation}, reward: {reward}, Terminated: {terminated}, Truncated: {truncated}, info: {info} \")\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3309ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space  # agent policy that uses the observation and info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58cd76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f417f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registry.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d416fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registry.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae38877",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registry.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd453e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registry.keys().make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748879cd",
   "metadata": {},
   "source": [
    "# Making Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de523f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f3093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34d95fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf3f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f5cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeGameEnv(gym.Env):\n",
    "    def __init__(self,maze):\n",
    "        self.maze=np.array(maze) #maze represented as a 2D numpy array\n",
    "        print(self.maze)\n",
    "        self.start_pos=np.where(self.maze=='S')\n",
    "        print(self.start_pos)\n",
    "        self.goal_pos=np.where(self.maze=='G') # Goal position\n",
    "        print(self.goal_pos)\n",
    "        self.current_pos=self.start_pos # Starting position is the current position initially\n",
    "        print(self.current_pos)\n",
    "        self.num_rows,self.num_cols=self.maze.shape\n",
    "        print(f\"num rows: {self.num_rows} and num columns: {self.num_cols}\")\n",
    "        #4 possible actions, 0=up, 1=down,2=left,3=right\n",
    "        self.action_space=spaces.discrete(4)\n",
    "        print(f\"Current Action: {self.action_space}\")\n",
    "        \n",
    "        # observation_space is a grid of size: rowsxcolumns\n",
    "        self.observation_space=spaces.Tuple((spaces.Discrete(self.num_rows),spaces.Discrete(self.num_cols)))\n",
    "        print(f\"Observation Space: {self.observation_space}\")\n",
    "        \n",
    "        #initialize the pygame\n",
    "        pygame.init()\n",
    "        self.cell_size=125\n",
    "        \n",
    "        #setting Display Size\n",
    "        self.screen=pygame.display.set_mode((self.num_cols*self.cell_size, self.num_rows*self.cell_size))\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_pos=self.start_pos\n",
    "        return self.current_pos\n",
    "    \n",
    "    def step(self,action):\n",
    "        #Move the agent based on the selected action\n",
    "        new_pos=np.array(self.current_pos)\n",
    "        \n",
    "        if action==0: #Up\n",
    "            new_pos[0] -=1\n",
    "            print(f\"new position after UP: {new_pos}\")\n",
    "        elif action==1:\n",
    "            new_pos[0] +=1\n",
    "            print(f\"new position after down: {new_pos}\")\n",
    "            \n",
    "        \n",
    "        elif action==2: # Left\n",
    "            new_pos[1]-=1\n",
    "            print(f\"new position after Left: {new_pos}\")\n",
    "            \n",
    "        \n",
    "        elif action==3: #Right\n",
    "            new_pos[1]+=1\n",
    "            print(f\"new position after Right: {new_pos[0]}\")\n",
    "            \n",
    "        \n",
    "        #Check if the new position is valid\n",
    "        if self.is_valid_position(new_pos):\n",
    "            self.current_pos=new_pos\n",
    "            \n",
    "        #Reward Function\n",
    "        if np.array_equal(self.current_pos,self.goal_pos):\n",
    "            reward=1.0\n",
    "            done=True\n",
    "        else:\n",
    "            reward=0.0\n",
    "            done=False\n",
    "        return self.current_pos,reward,done, {}\n",
    "    \n",
    "    def is_valid_position(self, pos):\n",
    "        row,col=pos\n",
    "        # if agent goes out of the grid\n",
    "        if row<0 or col<0 or row>=self.num_rows or col>=self.num_cols:\n",
    "            return false\n",
    "        \n",
    "        #if the agent hits an obstacle\n",
    "        if self.maze[row,col]=='#':\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def render(self):\n",
    "        #clear the screen\n",
    "        self.screen.fill((255,255,255))\n",
    "        \n",
    "        # Draw env elements one cell at a time\n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                cell_left=col*self.cell_size\n",
    "                cell_top=row*self.cell_size\n",
    "                \n",
    "                try:\n",
    "                    print(np.array(self.current_pos)==np.array([row,col]).reshape(-1,1))\n",
    "                except Exception as e:\n",
    "                    print('Initial State')\n",
    "                    \n",
    "                if self.maze[row,col]=='#': #Obstacle\n",
    "                    pygame.draw.rect(self.screen,(0,0,0),(cell_left,cell_top,self.cell_size,self.cell_size))\n",
    "                \n",
    "                elif self.maze[row,col]=='S': # Starting postion\n",
    "                    pygame.draw.rect(self.screen,(0,255,0),(cell_left,cell_top,self.cell_size,self.cell_size))\n",
    "                \n",
    "                elif self.maze[row,col]=='G': # Goal postion\n",
    "                    pygame.draw.rect(self.screen,(0,0,255),(cell_left,cell_top,self.cell_size,self.cell_size))\n",
    "        pygame.display.update() #update the display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f51fb8e",
   "metadata": {},
   "source": [
    "# Register the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f98b564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(id='MazeGame-v0',entry_point='mazegame:MazeGameEnv',kwargs={'maze':None})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f46b4e",
   "metadata": {},
   "source": [
    "# Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd2b07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze= [\n",
    "    ['S','','.','.'],\n",
    "    ['.','#','.','#'],\n",
    "    ['.','.','.','.'],\n",
    "    ['#',\".\",\"#\",'G']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad155f59",
   "metadata": {},
   "source": [
    "# Test the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19ec98ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' '' '.' '.']\n",
      " ['.' '#' '.' '#']\n",
      " ['.' '.' '.' '.']\n",
      " ['#' '.' '#' 'G']]\n",
      "(array([0]), array([0]))\n",
      "(array([3]), array([3]))\n",
      "(array([0]), array([0]))\n",
      "num rows: 4 and num columns: 4\n",
      "Current Action: Discrete(4)\n",
      "Observation Space: Tuple(Discrete(4), Discrete(4))\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('MazeGame-v0',maze=maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324dc9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
