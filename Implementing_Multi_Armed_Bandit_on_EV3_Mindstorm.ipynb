{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing The Multi-Armed Bandit Algorithm on EV3 Mindstorm\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, we'll explore how to implement the Multi-Armed Bandit Algorithm on an EV3 Mindstorm robot. The Multi-Armed Bandit problem is a classic problem in probability theory and statistics, often used as a simplified model for optimizing resource allocation. EV3 Mindstorm, on the other hand, is a programmable robot kit that provides a hands-on introduction to robotics and programming.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [What is a Multi-Armed Bandit?](#What-is-a-Multi-Armed-Bandit?)\n",
        "2. [Importance of Multi-Armed Bandit](#Importance-of-Multi-Armed-Bandit)\n",
        "3. [Drawbacks and Limitations](#Drawbacks-and-Limitations)\n",
        "4. [Real-world Applications](#Real-world-Applications)\n",
        "5. [Implementing on EV3 Mindstorm](#Implementing-on-EV3-Mindstorm)\n",
        "6. [Exercises](#Exercises)\n",
        "7. [Conclusion](#Conclusion)"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "02ba76d6-e1e6-4107-a79a-a225efacb954"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a Multi-Armed Bandit?\n",
        "\n",
        "Imagine you're in a casino, and you're faced with a row of slot machines, also known as 'one-armed bandits.' Each machine provides a different but unknown reward when you play it. Your goal is to maximize your total reward over a series of rounds. This is the essence of the Multi-Armed Bandit problem. In a more formal setting, you have multiple options (the arms), each providing a different but unknown reward. Your task is to devise a strategy to maximize your total reward over a series of trials.\n",
        "\n",
        "In the context of EV3 Mindstorm, think of each 'arm' as a different path or action the robot could take. Each path has an associated reward, like reaching the destination faster or avoiding obstacles more efficiently. The Multi-Armed Bandit algorithm can help the robot learn the best path to take over time.\n",
        "\n",
        "![Multi-Armed Bandit](https://miro.medium.com/max/1400/1*2r3xYjppoUO1jqHXHcE2_Q.jpeg)\n",
        "\n",
        "Image Source: [Medium Article by Fedor Parfenov](https://medium.com/expedia-group-tech/how-we-optimized-hero-images-on-hotels-com-using-multi-armed-bandit-algorithms-4503c2c32eae)"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "6e859130-0f55-417c-901e-ef2c93959c7d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importance of Multi-Armed Bandit\n",
        "\n",
        "The Multi-Armed Bandit algorithm is not just a theoretical construct; it has practical implications in various fields. For instance, in online advertising, it can be used to determine which ad to display to maximize click-through rates. In healthcare, it can be used to allocate limited resources, such as deciding which treatment to give to a patient to maximize the likelihood of recovery.\n",
        "\n",
        "For our EV3 Mindstorm robot, the importance lies in efficient learning and decision-making. Traditional methods might require the robot to try all paths multiple times before deciding the best one, which is time-consuming and may wear out the hardware. The Multi-Armed Bandit algorithm allows the robot to learn more efficiently, saving both time and resources.\n",
        "\n",
        "![Importance of Multi-Armed Bandit](https://miro.medium.com/max/1400/1*2r3xYjppoUO1jqHXHcE2_Q.jpeg)\n",
        "\n",
        "Image Source: [Medium Article by Fedor Parfenov](https://medium.com/expedia-group-tech/how-we-optimized-hero-images-on-hotels-com-using-multi-armed-bandit-algorithms-4503c2c32eae)"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "ac63958c-416a-4c26-9351-5983449e55c9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drawbacks and Limitations\n",
        "\n",
        "While the Multi-Armed Bandit algorithm is powerful, it's not without its drawbacks. One of the main limitations is the assumption that the reward distributions are stationary, meaning they do not change over time. In many real-world scenarios, this is not the case. For example, the best path for the EV3 Mindstorm robot may change due to external factors like new obstacles.\n",
        "\n",
        "Another drawback is the computational complexity involved in updating the reward estimates, especially as the number of arms increases. This can be a concern for devices with limited computational resources, like the EV3 Mindstorm.\n",
        "\n",
        "![Drawbacks of Multi-Armed Bandit](https://miro.medium.com/max/1400/1*2r3xYjppoUO1jqHXHcE2_Q.jpeg)\n",
        "\n",
        "Image Source: [Medium Article by Fedor Parfenov](https://medium.com/expedia-group-tech/how-we-optimized-hero-images-on-hotels-com-using-multi-armed-bandit-algorithms-4503c2c32eae)"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "5b9841c3-b4bb-4755-9a08-7168307bfe64"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real-world Applications\n",
        "\n",
        "The Multi-Armed Bandit algorithm has found applications in various domains:\n",
        "\n",
        "- **Online Advertising**: To optimize the selection of ads to display.\n",
        "- **Healthcare**: For personalized medicine and treatment recommendations.\n",
        "- **Finance**: In trading algorithms to maximize returns.\n",
        "- **Robotics**: For path optimization, as we will see with the EV3 Mindstorm.\n",
        "\n",
        "In the context of our EV3 Mindstorm robot, the algorithm could be used in a maze-solving scenario where the robot has to find the quickest path to the exit. It could also be used in a search-and-rescue operation where the robot has to find the most efficient path to reach people in need.\n",
        "\n",
        "![Real-world Applications of Multi-Armed Bandit](https://miro.medium.com/max/1400/1*2r3xYjppoUO1jqHXHcE2_Q.jpeg)\n",
        "\n",
        "Image Source: [Medium Article by Fedor Parfenov](https://medium.com/expedia-group-tech/how-we-optimized-hero-images-on-hotels-com-using-multi-armed-bandit-algorithms-4503c2c32eae)"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "05898b69-1825-483a-8a11-f3479982d0a1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing on EV3 Mindstorm\n",
        "\n",
        "Now that we have a good understanding of the Multi-Armed Bandit algorithm and its applications, let's dive into its implementation on an EV3 Mindstorm robot.\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "Let's assume our EV3 Mindstorm robot is placed in a maze with multiple paths. Each path leads to the exit but with varying levels of difficulty and time required. The robot has to find the most efficient path to the exit.\n",
        "\n",
        "### Algorithm Steps\n",
        "\n",
        "1. Initialize estimated rewards for each path to zero.\n",
        "2. For each round:\n",
        "    1. Select a path based on estimated rewards (using Epsilon-Greedy or another strategy).\n",
        "    2. Navigate the robot through the selected path.\n",
        "    3. Observe the actual reward (time taken, obstacles encountered, etc.).\n",
        "    4. Update the estimated reward for the selected path.\n",
        "\n",
        "Let's proceed to code this algorithm."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "75089c9c-62bb-40f0-ad94-fe5e44ef479a"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Simulated rewards for each path (in seconds; lower is better)\n",
        "true_rewards = [10, 8, 12, 7]\n",
        "\n",
        "# Initialize estimated rewards\n",
        "estimated_rewards = [0, 0, 0, 0]\n",
        "n_trials = [0, 0, 0, 0]\n",
        "\n",
        "# Epsilon-Greedy strategy\n",
        "epsilon = 0.1\n",
        "\n",
        "# Number of rounds\n",
        "n_rounds = 20\n",
        "\n",
        "# Simulate the rounds\n",
        "for i in range(n_rounds):\n",
        "    if np.random.rand() < epsilon:\n",
        "        # Explore: choose a random path\n",
        "        path = np.random.randint(0, 4)\n",
        "    else:\n",
        "        # Exploit: choose the path with the lowest estimated time\n",
        "        path = np.argmin(estimated_rewards)\n",
        "\n",
        "    # Simulate navigating the robot through the selected path\n",
        "    actual_reward = np.random.normal(true_rewards[path], 1)\n",
        "\n",
        "    # Update estimated reward for the selected path\n",
        "    n_trials[path] += 1\n",
        "    estimated_rewards[path] = ((n_trials[path] - 1) * estimated_rewards[path] + actual_reward) / n_trials[path]\n",
        "\n",
        "    print(f'Round {i+1}: Selected path {path+1}, Actual time {actual_reward:.2f}s, Estimated time {estimated_rewards[path]:.2f}s')\n",
        "\n",
        "    # Simulate a delay (e.g., for the robot to navigate)\n",
        "    time.sleep(1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "1cfa0726-317c-4602-8082-4d499bc46d90"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Explanation\n",
        "\n",
        "In the above code, we simulated the scenario where an EV3 Mindstorm robot has to choose between four paths in a maze. Each path has a 'true' average time to navigate, which is unknown to the robot. The robot uses an Epsilon-Greedy strategy to choose paths over 20 rounds.\n",
        "\n",
        "Here's a breakdown of the code:\n",
        "\n",
        "1. **Initialization**: We initialize the true rewards for each path and set the estimated rewards to zero.\n",
        "2. **Epsilon-Greedy Strategy**: We use an epsilon of 0.1, meaning the robot will explore a random path 10% of the time and exploit the best-known path 90% of the time.\n",
        "3. **Simulation Loop**: For each round, the robot chooses a path based on the Epsilon-Greedy strategy, simulates navigating through it, and updates the estimated reward for that path.\n",
        "\n",
        "The output shows the path chosen in each round, the actual time taken, and the updated estimated time for that path."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "dc3f9991-81d5-44e9-ae62-301dc1865ffa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n",
        "\n",
        "### Exercise 1: Different Strategies\n",
        "\n",
        "Modify the code to implement a different strategy for arm selection, such as UCB (Upper Confidence Bound). Compare the results with the Epsilon-Greedy strategy.\n",
        "\n",
        "### Exercise 2: Non-Stationary Rewards\n",
        "\n",
        "Modify the code to simulate a non-stationary environment, where the true rewards for each path change over time. How does the algorithm perform?\n",
        "\n",
        "### Exercise 3: Real-world Testing\n",
        "\n",
        "If you have access to an EV3 Mindstorm robot, implement the algorithm and test it in a real-world scenario. Share your observations and results."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "3cb85473-c3e5-4c58-a1d8-0c31b6e56a97"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Explanation\n",
        "\n",
        "Let's break down the code to understand how it works:\n",
        "\n",
        "1. **Initialization**: We initialize the true rewards for each path and set the estimated rewards to zero. The true rewards are simulated to represent the time taken for each path. Lower is better.\n",
        "2. **Epsilon-Greedy Strategy**: We use an Epsilon-Greedy strategy with \\(\\epsilon = 0.1\\). This means that 10% of the time, the robot will explore a random path, and 90% of the time, it will exploit the best-known path.\n",
        "3. **Simulation Loop**: We run a loop for a fixed number of rounds. In each round, the robot selects a path based on the Epsilon-Greedy strategy, simulates navigating through it, and updates the estimated reward for that path.\n",
        "\n",
        "The `time.sleep(1)` at the end simulates the time it would take for the robot to navigate the path. In a real-world implementation, this would be replaced by the actual navigation code."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "d9838ceb-ed10-4149-926b-26633c4efb11"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n",
        "\n",
        "### Exercise 1: Different Strategies\n",
        "\n",
        "Modify the code to implement different strategies like UCB (Upper Confidence Bound) and Thompson Sampling. Compare the performance with Epsilon-Greedy.\n",
        "\n",
        "### Exercise 2: Non-Stationary Rewards\n",
        "\n",
        "Modify the code to simulate non-stationary rewards (i.e., rewards that change over time). How does this affect the performance of the algorithm?\n",
        "\n",
        "### Exercise 3: Real-world Testing\n",
        "\n",
        "Implement the algorithm on an actual EV3 Mindstorm robot and test it in a real-world scenario. Compare the results with the simulation."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "c2e83e38-4c12-4d61-9b39-4c416c1afc10"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions to Exercises\n",
        "\n",
        "### Solution to Exercise 1: Different Strategies\n",
        "\n",
        "To implement the UCB (Upper Confidence Bound) strategy, you can modify the arm selection part of the code as follows:\n",
        "\n",
        "```python\n",
        "import math\n",
        "\n",
        "# Initialize upper confidence bounds\n",
        "ucbs = [float('inf')] * 4\n",
        "\n",
        "# In the simulation loop\n",
        "for i in range(n_rounds):\n",
        "    path = np.argmax(ucbs)\n",
        "    # ... (rest of the code remains the same)\n",
        "    # Update UCB for the selected path\n",
        "    ucbs[path] = estimated_rewards[path] + math.sqrt(2 * math.log(i+1) / n_trials[path])\n",
        "```\n",
        "\n",
        "### Solution to Exercise 2: Non-Stationary Rewards\n",
        "\n",
        "To simulate a non-stationary environment, you can add a random noise to the true rewards at each round. For example:\n",
        "\n",
        "```python\n",
        "true_rewards = np.random.normal(true_rewards, 0.1)\n",
        "```\n",
        "\n",
        "### Solution to Exercise 3: Real-world Testing\n",
        "\n",
        "For real-world testing on an EV3 Mindstorm robot, you would need to replace the simulated reward with the actual time taken by the robot to navigate through the selected path. You can use the EV3 Python API to control the robot and measure the time."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "e5094f7e-f3e9-4a1d-af82-5a60b4e48f32"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we explored the Multi-Armed Bandit algorithm, its importance, drawbacks, and real-world applications. We then implemented a simple version of the algorithm to solve a maze navigation problem for an EV3 Mindstorm robot. Through exercises and solutions, we also looked at how to adapt the algorithm for different strategies and environments.\n",
        "\n",
        "The Multi-Armed Bandit algorithm offers a robust framework for solving complex decision-making problems, and its implementation on platforms like the EV3 Mindstorm opens up exciting possibilities for robotics and automation."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "236f2508-fec1-4bb4-a533-378fcc189fe2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise Solutions\n",
        "\n",
        "### Solution to Exercise 1: Different Strategies\n",
        "\n",
        "For UCB (Upper Confidence Bound), the selection strategy can be modified as follows:\n",
        "\n",
        "```python\n",
        "import math\n",
        "\n",
        "# Initialize\n",
        "ucb_values = [0, 0, 0, 0]\n",
        "\n",
        "# In the simulation loop\n",
        "if i == 0:\n",
        "    path = np.random.randint(0, 4)\n",
        "else:\n",
        "    path = np.argmax(ucb_values)\n",
        "\n",
        "# After observing the reward\n",
        "ucb_values[path] = estimated_rewards[path] + math.sqrt(2 * math.log(i+1) / n_trials[path])\n",
        "```\n",
        "\n",
        "For Thompson Sampling, you would maintain a Beta distribution for each path and sample from it to select a path.\n",
        "\n",
        "### Solution to Exercise 2: Non-Stationary Rewards\n",
        "\n",
        "To simulate non-stationary rewards, you can add a time-dependent factor to the `true_rewards`. For example:\n",
        "\n",
        "```python\n",
        "true_rewards = [10 + np.sin(i/10), 8, 12, 7]\n",
        "```\n",
        "\n",
        "To adapt to non-stationary rewards, you can use a sliding window to update the estimated rewards.\n",
        "\n",
        "### Solution to Exercise 3: Real-world Testing\n",
        "\n",
        "For real-world testing, you would replace the simulated `actual_reward` with the time taken by the EV3 Mindstorm robot to navigate the selected path. The rest of the algorithm remains the same."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "4baad783-7534-4477-9514-0c5bdc6eab8e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we explored the Multi-Armed Bandit algorithm and its implementation on an EV3 Mindstorm robot. We discussed the importance, drawbacks, and real-world applications of the algorithm. We also provided exercises and their solutions to deepen your understanding and encourage further exploration.\n",
        "\n",
        "The Multi-Armed Bandit algorithm offers a robust and efficient way for the EV3 Mindstorm robot to learn and make decisions. While the notebook provides a simulated environment, the real test would be to implement it on an actual robot. So go ahead, take your EV3 Mindstorm robot for a spin and let it learn the ways of the Multi-Armed Bandit!"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "5d67fc51-d275-4f29-b140-6197560c0acf"
    }
  ],
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "8e9f926a-ce11-5aa9-95c1-bd26017668fc",
        "openai_ephemeral_user_id": "670508ae-3062-521d-b2f7-a8582dcb1409",
        "openai_subdivision1_iso_code": "PK-PB"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small",
    "noteable": {
      "last_delta_id": "ff48ef03-3b4f-44a3-b7d8-cd88f5e8735b"
    },
    "nteract": {
      "version": "noteable@2.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}