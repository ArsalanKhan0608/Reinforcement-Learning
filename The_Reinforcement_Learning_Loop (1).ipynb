{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Reinforcement Learning Loop\n",
        "\n",
        "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The core of RL is the RL loop, which involves the agent taking actions, receiving rewards, and updating its knowledge to make better future decisions.\n",
        "\n",
        "## Table of Contents\n",
        "1. [What is the Reinforcement Learning Loop?](#What-is-the-Reinforcement-Learning-Loop?)\n",
        "2. [Importance of the RL Loop](#Importance-of-the-RL-Loop)\n",
        "3. [Drawbacks and Limitations](#Drawbacks-and-Limitations)\n",
        "4. [Real-world Applications](#Real-world-Applications)\n",
        "5. [Exercises](#Exercises)\n",
        "6. [Solutions to Exercises](#Solutions-to-Exercises)"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "9da30612-3485-4318-9e0e-42c13e642c4f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is the Reinforcement Learning Loop?\n",
        "\n",
        "The Reinforcement Learning Loop is a cycle involving four main components:\n",
        "\n",
        "1. **Agent**: The learner or decision maker.\n",
        "2. **Environment**: The place where the agent operates.\n",
        "3. **Action**: What the agent does.\n",
        "4. **Reward**: The feedback from the environment.\n",
        "\n",
        "The loop operates as follows:\n",
        "\n",
        "- The agent observes the environment.\n",
        "- The agent takes an action.\n",
        "- The environment responds with a new state and a reward.\n",
        "- The agent updates its knowledge based on the reward.\n",
        "\n",
        "![Reinforcement Learning Loop](https://image.shutterstock.com/image-illustration/reinforcement-learning-diagram-machine-algorithm-260nw-1695310483.jpg)\n",
        "\n",
        "This loop continues until a termination condition is met, such as reaching a maximum number of steps or achieving a certain level of performance."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "064e8afd-bc3a-47a9-9f20-60e94e18154c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importance of the RL Loop\n",
        "\n",
        "The RL Loop is crucial for several reasons:\n",
        "\n",
        "- **Adaptability**: The agent learns from its experiences, allowing it to adapt to new situations.\n",
        "- **Optimization**: Over time, the agent learns to make decisions that maximize some notion of cumulative reward.\n",
        "- **Autonomy**: The agent makes decisions without human intervention, which is particularly useful in environments where human decision-making is impractical.\n",
        "\n",
        "Imagine a self-driving car navigating through traffic. It starts with limited knowledge but learns to adapt by observing other vehicles, traffic lights, and road conditions. Over time, it becomes proficient at driving safely while also reaching its destination efficiently."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "357ca2fc-0bfa-4733-ae6f-7704f69350fa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drawbacks and Limitations\n",
        "\n",
        "While the RL Loop is powerful, it has its limitations:\n",
        "\n",
        "- **Sample Inefficiency**: Learning from scratch can be time-consuming and resource-intensive.\n",
        "- **Exploration-Exploitation Dilemma**: Balancing the need to explore new actions versus exploiting known actions is challenging.\n",
        "- **Reward Shaping**: Incorrectly designed reward functions can lead the agent to undesired behavior.\n",
        "\n",
        "For instance, if a drone is programmed to maximize the time it stays airborne, it might learn to hover at low altitudes to avoid the risk of crashing, thereby not fulfilling its actual mission of surveillance."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "e42092f3-3f29-4cb1-bb99-d460ca3a3a76"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real-world Applications\n",
        "\n",
        "The RL Loop is not just a theoretical concept; it has practical applications in various fields:\n",
        "\n",
        "- **Healthcare**: Personalized treatment plans based on patient history and responses.\n",
        "- **Finance**: Algorithmic trading strategies that adapt to market conditions.\n",
        "- **Robotics**: Robots that can adapt to different tasks and environments.\n",
        "- **Gaming**: AI opponents that adapt to player behavior.\n",
        "\n",
        "In the gaming industry, for example, RL algorithms can create characters that learn from each interaction with the player, making the game more challenging and engaging."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "9a33a7b3-fbba-42a8-aac1-ecf2ac4a0b8f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Implement a Simple RL Loop**: Create a Python script that simulates a simple RL loop with a basic agent and environment. Observe how the agent's decisions evolve over time.\n",
        "\n",
        "2. **Exploration vs Exploitation**: Modify the above script to include both exploration and exploitation. Analyze how the agent's behavior changes.\n",
        "\n",
        "3. **Reward Shaping**: Experiment with different reward functions in the script. Observe how the agent's behavior changes with different rewards."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "cd88e41b-e8b7-4a80-8982-7f7e6691b2a0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: Implement a Simple RL Loop\n",
        "\n",
        "import random\n",
        "\n",
        "class SimpleAgent:\n",
        "    def __init__(self):\n",
        "        self.value = 0\n",
        "\n",
        "    def choose_action(self):\n",
        "        return random.choice(['left', 'right'])\n",
        "\n",
        "    def update_value(self, reward):\n",
        "        self.value += reward\n",
        "\n",
        "class SimpleEnvironment:\n",
        "    def get_reward(self, action):\n",
        "        return 1 if action == 'right' else -1\n",
        "\n",
        "agent = SimpleAgent()\n",
        "env = SimpleEnvironment()\n",
        "\n",
        "for i in range(10):\n",
        "    action = agent.choose_action()\n",
        "    reward = env.get_reward(action)\n",
        "    agent.update_value(reward)\n",
        "    print(f'Round {i+1}: Action = {action}, Reward = {reward}, Total Value = {agent.value}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "6c78d44e-45e3-450c-9cb0-a72f2edc0f72"
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: Exploration vs Exploitation\n",
        "\n",
        "class ExploringAgent(SimpleAgent):\n",
        "    def __init__(self, epsilon=0.1):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def choose_action(self):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(['left', 'right'])\n",
        "        return 'right' if self.value >= 0 else 'left'\n",
        "\n",
        "exploring_agent = ExploringAgent()\n",
        "\n",
        "for i in range(10):\n",
        "    action = exploring_agent.choose_action()\n",
        "    reward = env.get_reward(action)\n",
        "    exploring_agent.update_value(reward)\n",
        "    print(f'Round {i+1}: Action = {action}, Reward = {reward}, Total Value = {exploring_agent.value}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "0e26c417-6388-4b6f-923a-98e26cf8c7c6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: Reward Shaping\n",
        "\n",
        "class ShapedEnvironment(SimpleEnvironment):\n",
        "    def get_reward(self, action):\n",
        "        return 2 if action == 'right' else -2\n",
        "\n",
        "shaped_env = ShapedEnvironment()\n",
        "\n",
        "for i in range(10):\n",
        "    action = exploring_agent.choose_action()\n",
        "    reward = shaped_env.get_reward(action)\n",
        "    exploring_agent.update_value(reward)\n",
        "    print(f'Round {i+1}: Action = {action}, Reward = {reward}, Total Value = {exploring_agent.value}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "e2e5b7f4-7698-45e0-954f-f883f5d6ff07"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions to Exercises\n",
        "\n",
        "### Solution to Exercise 1\n",
        "\n",
        "In this exercise, we implemented a simple RL loop with a basic agent and environment. The agent randomly chooses between 'left' and 'right' actions, and the environment provides a reward of +1 for 'right' and -1 for 'left'. The agent's total value is updated based on the reward.\n",
        "\n",
        "### Solution to Exercise 2\n",
        "\n",
        "We extended the simple agent to include exploration and exploitation. The agent now has an `epsilon` parameter that controls the probability of taking a random action. If a random action is not taken, the agent chooses the action that corresponds to its current value (positive or negative).\n",
        "\n",
        "### Solution to Exercise 3\n",
        "\n",
        "In this exercise, we modified the environment's reward function to give a reward of +2 for 'right' and -2 for 'left'. This change in the reward function would likely make the agent more biased towards choosing the 'right' action, as the positive reward is now more significant."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "5eef9ab0-052d-4b94-b7b2-eb387bd8aeae"
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: Exploration vs Exploitation\n",
        "\n",
        "class ExploringAgent(SimpleAgent):\n",
        "    def choose_action(self):\n",
        "        if random.random() < 0.2:\n",
        "            return 'left'\n",
        "        else:\n",
        "            return 'right'\n",
        "\n",
        "exploring_agent = ExploringAgent()\n",
        "\n",
        "for i in range(10):\n",
        "    action = exploring_agent.choose_action()\n",
        "    reward = env.get_reward(action)\n",
        "    exploring_agent.update_value(reward)\n",
        "    print(f'Round {i+1}: Action = {action}, Reward = {reward}, Total Value = {exploring_agent.value}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "d3e7ebed-b023-46c7-a4e4-0a001fa4fb0b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: Reward Shaping\n",
        "\n",
        "class ShapedEnvironment(SimpleEnvironment):\n",
        "    def get_reward(self, action):\n",
        "        return 2 if action == 'right' else -2\n",
        "\n",
        "shaped_env = ShapedEnvironment()\n",
        "\n",
        "for i in range(10):\n",
        "    action = agent.choose_action()\n",
        "    reward = shaped_env.get_reward(action)\n",
        "    agent.update_value(reward)\n",
        "    print(f'Round {i+1}: Action = {action}, Reward = {reward}, Total Value = {agent.value}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "30da52aa-2fa3-41f0-94a0-e04f1265292e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions to Exercises\n",
        "\n",
        "### Solution to Exercise 1\n",
        "\n",
        "In this exercise, we implemented a simple RL loop with a basic agent and environment. The agent randomly chooses between two actions: 'left' and 'right'. The environment provides a reward of +1 for 'right' and -1 for 'left'.\n",
        "\n",
        "### Solution to Exercise 2\n",
        "\n",
        "We modified the agent to include exploration. Now, the agent chooses 'left' 20% of the time and 'right' 80% of the time. This allows the agent to explore other possibilities while mostly sticking to the action that it knows will yield a positive reward.\n",
        "\n",
        "### Solution to Exercise 3\n",
        "\n",
        "We experimented with reward shaping by changing the reward function. Now, the environment provides a reward of +2 for 'right' and -2 for 'left'. This amplifies the consequences of the agent's actions, making it more crucial for the agent to make the right decisions."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "f02def08-4065-4881-895e-4b2df9879323"
    }
  ],
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "8e9f926a-ce11-5aa9-95c1-bd26017668fc",
        "openai_ephemeral_user_id": "670508ae-3062-521d-b2f7-a8582dcb1409",
        "openai_subdivision1_iso_code": "PK-PB"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small",
    "noteable": {
      "last_delta_id": "0ec9e6cb-bd68-45f6-ab01-c861796cf6ad"
    },
    "nteract": {
      "version": "noteable@2.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}