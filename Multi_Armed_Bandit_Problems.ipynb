{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1456de2-2626-4787-8102-b2b3dcdef714",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "# Multi-Armed Bandit Problems\n",
    "## Introduction\n",
    "In this notebook, we will delve into the fascinating world of Multi-Armed Bandit Problems. We will explore what it is, its importance, drawbacks, and areas of application. To make the subject matter more relatable, we will use real-life examples and narratives. Additionally, we will provide three exercises along with their solutions to help solidify your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e2fe2-e746-4d2f-87f6-a136fac78eaa",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## What is a Multi-Armed Bandit Problem?\n",
    "Imagine you're in a casino, and you're faced with a row of slot machines, each with its own lever. You have a limited amount of money and time. How do you decide which machine to play to maximize your winnings?\n",
    "\n",
    "This scenario is a classic example of a Multi-Armed Bandit Problem. In a more formal definition, a Multi-Armed Bandit is a model for a decision problem where an agent (you, in this case) has to choose between multiple actions (the slot machines), each with an unknown reward. The agent's objective is to maximize the total reward over a series of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8704a80-6642-4d21-84f7-b0139a9db085",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Importance of Multi-Armed Bandit Problems\n",
    "The Multi-Armed Bandit Problem is not just a theoretical construct; it has practical applications in various fields. Here are some reasons why it's important:\n",
    "\n",
    "- **Resource Allocation**: In industries like healthcare, where resources are limited, Multi-Armed Bandit algorithms can help allocate resources more efficiently.\n",
    "- **Online Advertising**: Companies can use these algorithms to decide which ads to show to maximize click-through rates.\n",
    "- **A/B Testing**: It provides a more dynamic alternative to traditional A/B testing, adjusting in real-time to user behavior.\n",
    "- **Personalization**: In e-commerce, Multi-Armed Bandit algorithms can personalize user experiences, showing products or content that the user is more likely to engage with.\n",
    "\n",
    "In essence, Multi-Armed Bandit Problems help in making optimized decisions under uncertainty, which is a common scenario in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc5b0c6-e9b0-44fd-836a-0a40d98f4ef3",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Drawbacks of Multi-Armed Bandit Problems\n",
    "While Multi-Armed Bandit Problems offer many advantages, they are not without their drawbacks. Here are some limitations:\n",
    "\n",
    "- **Computational Complexity**: Some algorithms can be computationally intensive, making them unsuitable for real-time applications.\n",
    "- **Non-Stationarity**: In a changing environment, the algorithm may not adapt quickly enough to be effective.\n",
    "- **Initial Bias**: The algorithm might be biased towards the initial set of actions, especially if not enough data is available.\n",
    "\n",
    "Understanding these drawbacks is crucial for effectively implementing Multi-Armed Bandit algorithms in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e585ff-ee0f-4a1d-80de-968e7ae3ff0f",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Where is it Under Use?\n",
    "Multi-Armed Bandit Problems are being used in various sectors. Let's look at some real-world applications:\n",
    "\n",
    "- **Healthcare**: In clinical trials, these algorithms can help in deciding which treatment is more effective for different types of patients.\n",
    "- **Finance**: In stock trading, Multi-Armed Bandit algorithms can be used to decide which stocks to buy or sell.\n",
    "- **Retail**: These algorithms can optimize pricing strategies in real-time.\n",
    "- **Internet of Things (IoT)**: In sensor networks, these algorithms can help in deciding which sensors to activate to collect the most useful data.\n",
    "\n",
    "These are just a few examples. The versatility of Multi-Armed Bandit Problems makes them applicable in a wide range of fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d56d0-1d9f-4f1c-b152-ce06f9284b9c",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Exercises\n",
    "To deepen your understanding, let's go through some exercises. Each exercise will come with a solution for you to check your work.\n",
    "\n",
    "### Exercise 1: Greedy vs Epsilon-Greedy\n",
    "Implement a simple greedy and epsilon-greedy algorithm and compare their performance. Use Python for this exercise.\n",
    "\n",
    "### Exercise 2: Softmax Exploration\n",
    "Implement the Softmax Exploration strategy and compare it with the epsilon-greedy algorithm. Use Python for this exercise.\n",
    "\n",
    "### Exercise 3: Real-world Scenario\n",
    "Imagine you are a marketing manager, and you have three different marketing strategies to choose from. How would you use a Multi-Armed Bandit algorithm to decide which strategy is the most effective? Write a brief outline of your approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284722d7-bd0e-4f67-aaa5-d2937c0fd7bb",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Exercise Solutions\n",
    "\n",
    "### Solution to Exercise 1: Greedy vs Epsilon-Greedy\n",
    "Here's a Python code snippet that demonstrates a simple greedy and epsilon-greedy algorithm. The code compares their performance in a simulated environment.\n",
    "\n",
    "```python\n",
    "# Python code for Greedy vs Epsilon-Greedy\n",
    "# ...\n",
    "```\n",
    "\n",
    "### Solution to Exercise 2: Softmax Exploration\n",
    "Below is a Python code snippet that implements the Softmax Exploration strategy and compares it with the epsilon-greedy algorithm.\n",
    "\n",
    "```python\n",
    "# Python code for Softmax Exploration\n",
    "# ...\n",
    "```\n",
    "\n",
    "### Solution to Exercise 3: Real-world Scenario\n",
    "As a marketing manager, you can set up a Multi-Armed Bandit algorithm to dynamically allocate budget to different marketing strategies. Start with an equal budget for all strategies and adjust based on performance metrics like ROI or customer engagement.\n",
    "\n",
    "1. **Initialization**: Allocate an equal budget to all three marketing strategies.\n",
    "2. **Exploration**: Run all strategies for a short period.\n",
    "3. **Exploitation**: Allocate more budget to the strategy that shows the best performance.\n",
    "4. **Adjustment**: Continuously monitor performance and adjust the budget allocation dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0245943-b2a6-47d1-8190-2255a60f345d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T10:33:49.413581+00:00",
     "start_time": "2023-10-11T10:33:49.250399+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70.32927340438212, 57.28474172285307)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution to Exercise 1: Greedy vs Epsilon-Greedy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Simulated slot machines (bandit arms)\n",
    "true_means = [0.1, 0.5, 0.8]\n",
    "\n",
    "# Function to pull an arm\n",
    "def pull_arm(mean):\n",
    "    return np.random.normal(mean, 1)\n",
    "\n",
    "# Greedy Algorithm\n",
    "def greedy(true_means, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        best_arm = np.argmax(estimated_means)\n",
    "        reward = pull_arm(true_means[best_arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[best_arm] += 1\n",
    "        estimated_means[best_arm] = ((n_pulls[best_arm] - 1) * estimated_means[best_arm] + reward) / n_pulls[best_arm]\n",
    "    return np.sum(rewards)\n",
    "\n",
    "# Epsilon-Greedy Algorithm\n",
    "def epsilon_greedy(true_means, epsilon=0.1, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        if np.random.rand() < epsilon:\n",
    "            arm = np.random.randint(0, 3)\n",
    "        else:\n",
    "            arm = np.argmax(estimated_means)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "    return np.sum(rewards)\n",
    "\n",
    "# Compare Greedy and Epsilon-Greedy\n",
    "greedy_reward = greedy(true_means)\n",
    "epsilon_greedy_reward = epsilon_greedy(true_means)\n",
    "greedy_reward, epsilon_greedy_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e2b0e-7634-4f0a-8405-6e83e0a612e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T10:47:38.073734+00:00",
     "start_time": "2023-10-11T10:47:37.904642+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Reward: 55.35629473877116\n"
     ]
    }
   ],
   "source": [
    "# Required function from Exercise 1\n",
    "def pull_arm(mean):\n",
    "    return np.random.normal(mean, 1)\n",
    "\n",
    "# Solution to Exercise 2: Softmax Exploration\n",
    "import numpy as np\n",
    "\n",
    "# Softmax function\n",
    "def softmax(x, tau=1.0):\n",
    "    x = np.array(x)  # Convert list to NumPy array\n",
    "    exp_x = np.exp(x / tau)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# Softmax Exploration Algorithm\n",
    "def softmax_exploration(true_means, tau=1.0, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        probabilities = softmax(estimated_means, tau)\n",
    "        arm = np.random.choice([0, 1, 2], p=probabilities)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "    return np.sum(rewards)\n",
    "\n",
    "# True means for the simulated slot machines (bandit arms)\n",
    "true_means = [0.1, 0.5, 0.8]\n",
    "\n",
    "# Compare Epsilon-Greedy and Softmax Exploration\n",
    "softmax_reward = softmax_exploration(true_means)\n",
    "print(\"Softmax Reward:\", softmax_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a940b9b-d15b-4df8-b678-74481a8a00b4",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Solution to Exercise 3: Real-world Scenario\n",
    "\n",
    "As a marketing manager, you can use a Multi-Armed Bandit algorithm to optimize your marketing strategies. Here's how:\n",
    "\n",
    "#### Steps:\n",
    "1. **Initialization**: Start by allocating an equal budget to all three marketing strategies: Social Media Ads, Email Marketing, and SEO.\n",
    "\n",
    "2. **Data Collection**: Run all three strategies for a short period, say one week, and collect data on key performance indicators like click-through rate, conversion rate, and ROI.\n",
    "\n",
    "3. **Analysis**: Use the data to estimate the 'reward' or effectiveness of each strategy. The reward could be the ROI or conversion rate.\n",
    "\n",
    "4. **Exploration and Exploitation**: Use an epsilon-greedy or softmax exploration algorithm to decide which strategy to focus on for the next week. The algorithm will balance between exploring less effective strategies and exploiting the most effective one.\n",
    "\n",
    "5. **Budget Reallocation**: Based on the algorithm's recommendation, reallocate the budget for the next week.\n",
    "\n",
    "6. **Continuous Monitoring**: Keep monitoring the performance and adjust the budget dynamically based on the algorithm's recommendations.\n",
    "\n",
    "By following these steps, you can dynamically optimize your marketing strategies to get the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e1e52-bd2e-495c-ac0f-083350e3a08b",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Explanation of Solution to Exercise 1: Greedy vs Epsilon-Greedy\n",
    "\n",
    "In the provided Python code, we simulate a Multi-Armed Bandit problem with three arms, each having different true means of rewards: 0.1, 0.5, and 0.8. We then implement two algorithms to solve this problem: Greedy and Epsilon-Greedy.\n",
    "\n",
    "#### Greedy Algorithm:\n",
    "1. Initialize estimated means and number of pulls for each arm to zero.\n",
    "2. In each round, choose the arm with the highest estimated mean reward.\n",
    "3. Pull the chosen arm and update its estimated mean based on the observed reward.\n",
    "\n",
    "#### Epsilon-Greedy Algorithm:\n",
    "1. Initialize estimated means and number of pulls for each arm to zero.\n",
    "2. In each round, with probability \\(\\epsilon\\), choose a random arm; otherwise, choose the arm with the highest estimated mean reward.\n",
    "3. Pull the chosen arm and update its estimated mean based on the observed reward.\n",
    "\n",
    "#### Output:\n",
    "The output shows the total rewards obtained by running each algorithm for 100 rounds. In this particular run, the Greedy algorithm obtained a total reward of approximately 70.33, while the Epsilon-Greedy algorithm obtained a total reward of approximately 57.28.\n",
    "\n",
    "#### Interpretation:\n",
    "The Greedy algorithm performed better in this run, but it's important to note that the performance can vary in different runs due to the stochastic nature of the problem. The Epsilon-Greedy algorithm, with its exploration factor \\(\\epsilon\\), is generally more robust in scenarios where the reward distributions can change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92d37c-9637-44d0-9de7-6e61ebe44777",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Explanation of Solution to Exercise 2: Softmax Exploration\n",
    "\n",
    "#### Importing NumPy\n",
    "```python\n",
    "import numpy as np\n",
    "```\n",
    "NumPy is imported for numerical operations.\n",
    "\n",
    "#### The `pull_arm` Function\n",
    "```python\n",
    "def pull_arm(mean):\n",
    "    return np.random.normal(mean, 1)\n",
    "```\n",
    "This function simulates pulling an arm of a slot machine. It returns a reward drawn from a normal distribution with a given mean and a standard deviation of 1.\n",
    "\n",
    "#### The `softmax` Function\n",
    "```python\n",
    "def softmax(x, tau=1.0):\n",
    "    x = np.array(x)\n",
    "    exp_x = np.exp(x / tau)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "```\n",
    "This function calculates the softmax probabilities for a given array `x`. The temperature parameter `tau` controls the level of exploration. A lower `tau` makes the probabilities more extreme, favoring the arm with the highest estimated mean.\n",
    "\n",
    "#### The `softmax_exploration` Function\n",
    "```python\n",
    "def softmax_exploration(true_means, tau=1.0, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "...\n",
    "```\n",
    "This function implements the Softmax Exploration algorithm. It initializes `estimated_means` and `n_pulls` for each arm and runs the algorithm for `n_rounds`.\n",
    "\n",
    "#### Output Interpretation\n",
    "The output shows the total rewards obtained by running the Softmax Exploration algorithm for 100 rounds. In this particular run, the algorithm obtained a total reward of approximately 55.36.\n",
    "\n",
    "#### Conclusion\n",
    "The Softmax Exploration algorithm provides a way to balance exploration and exploitation by assigning probabilities to each arm based on their estimated means. The algorithm is particularly useful in scenarios where the reward distributions are not well understood initially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18572885-17f1-4d25-b1ee-3dc4935d5011",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Explanation of Solution to Estimating Action Values Through Sampling\n",
    "\n",
    "#### Importing NumPy\n",
    "```python\n",
    "import numpy as np\n",
    "```\n",
    "NumPy is imported for numerical operations.\n",
    "\n",
    "#### The `pull_arm` Function\n",
    "```python\n",
    "def pull_arm(mean):\n",
    "    return np.random.normal(mean, 1)\n",
    "```\n",
    "This function simulates pulling an arm of a slot machine. It returns a reward drawn from a normal distribution with a given mean and a standard deviation of 1.\n",
    "\n",
    "#### The `estimate_action_values` Function\n",
    "```python\n",
    "def estimate_action_values(true_means, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        arm = np.random.randint(0, 3)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "    return estimated_means\n",
    "```\n",
    "This function estimates the action values (means) through sampling. It initializes `estimated_means` and `n_pulls` for each arm and runs the algorithm for `n_rounds`. Each round, it randomly selects an arm to pull and updates the estimated mean for that arm based on the observed reward.\n",
    "\n",
    "#### Output Interpretation\n",
    "The output shows the estimated means for each arm after running the algorithm for 100 rounds. These estimates provide a basis for making decisions in a multi-armed bandit problem.\n",
    "\n",
    "#### Conclusion\n",
    "Estimating action values through sampling is a straightforward but effective method for understanding the reward distributions of different actions. It provides the foundation for more advanced algorithms that balance exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4afe059-cde0-4835-b8bb-ac0351746273",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "# Estimating Action Values Through Sampling\n",
    "\n",
    "In this section, we will explore how to estimate action values through sampling. This is a fundamental concept in reinforcement learning and Multi-Armed Bandit problems. We will implement a simple algorithm to estimate action values and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf9c5b2-bb1a-4f6c-ac96-f3a08ed18c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T10:55:09.087549+00:00",
     "start_time": "2023-10-11T10:55:08.927225+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0627463922351698,\n",
       "  0.5494126756953325,\n",
       "  0.5706450121186378,\n",
       "  1.013409007622408],\n",
       " 46.36915837461709)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python Code for Estimating Action Values Through Sampling\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Simulated slot machines (bandit arms)\n",
    "true_means = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "# Function to pull an arm\n",
    "def pull_arm(mean):\n",
    "    return np.random.normal(mean, 1)\n",
    "\n",
    "# Algorithm for Estimating Action Values Through Sampling\n",
    "def estimate_action_values(true_means, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0, 0]\n",
    "    n_pulls = [0, 0, 0, 0]\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        arm = np.random.randint(0, 4)  # Randomly select an arm\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "    return estimated_means, np.sum(rewards)\n",
    "\n",
    "# Run the algorithm and get the results\n",
    "estimated_means, total_reward = estimate_action_values(true_means)\n",
    "estimated_means, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ffc8ef-6962-485b-a3e4-51c612ea5125",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Explanation of Code and Evaluation of Result\n",
    "\n",
    "#### Code Explanation:\n",
    "1. **Initialization**: We initialize four slot machines (bandit arms) with true means of rewards as 0.2, 0.4, 0.6, and 0.8.\n",
    "2. **Pull Arm Function**: A function `pull_arm(mean)` simulates pulling an arm by generating a random reward from a normal distribution centered at the given mean.\n",
    "3. **Estimation Algorithm**: The function `estimate_action_values(true_means, n_rounds=100)` estimates the action values through sampling. It randomly selects an arm, pulls it, and updates the estimated mean for that arm.\n",
    "\n",
    "#### Output:\n",
    "The output shows the estimated means for each arm and the total reward after running the algorithm for 100 rounds. In this run, the estimated means are approximately [0.063, 0.549, 0.571, 1.013] and the total reward is approximately 46.37.\n",
    "\n",
    "#### Evaluation:\n",
    "1. **Estimated Means**: The estimated means are close to the true means, indicating that the algorithm is effective in estimating action values.\n",
    "2. **Total Reward**: The total reward of 46.37 suggests that the algorithm was able to accumulate a decent amount of reward over 100 rounds.\n",
    "\n",
    "#### Conclusion:\n",
    "The algorithm effectively estimates the action values through sampling and is capable of accumulating rewards. However, it's worth noting that this is a purely exploratory approach, as it randomly selects arms in each round."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053b2154-8300-4836-9481-cb1527c437ce",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "# Implementing And Analysing A Greedy Agent\n",
    "\n",
    "In this section, we will dive into the concept of a Greedy Agent, a fundamental concept in Reinforcement Learning. We will explore what it is, its importance, and its drawbacks. We will also look at real-world applications and provide exercises for a deeper understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af5fd2-2d54-46ec-84b4-9a7b16e7a454",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## What is a Greedy Agent?\n",
    "\n",
    "In the context of Reinforcement Learning, an agent is considered 'greedy' if it always chooses the action that it believes will yield the highest immediate reward. This is based on the data it has collected up to that point. The greedy agent does not explore other actions to see if they might lead to higher rewards in the long run; it simply exploits the best-known action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c976b26-0574-4b36-82a3-f90bc3fc771c",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Importance of a Greedy Agent\n",
    "\n",
    "1. **Simplicity**: Greedy agents are straightforward to implement. They don't require complex algorithms or data structures.\n",
    "\n",
    "2. **Efficiency**: Because they always choose the best-known action, greedy agents often perform well in stable and predictable environments.\n",
    "\n",
    "3. **Fast Decision Making**: Greedy agents make decisions quickly since they don't have to consider multiple future scenarios.\n",
    "\n",
    "4. **Resource-Friendly**: They are computationally less expensive as they don't require the agent to keep track of various probabilities or to solve complex optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc0e049-0d56-4b12-8762-a6713b5918dc",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Drawbacks of a Greedy Agent\n",
    "\n",
    "1. **Lack of Exploration**: Greedy agents can get stuck in local optima because they don't explore other actions that might lead to higher rewards in the long run.\n",
    "\n",
    "2. **Not Adaptable**: In changing environments, a greedy agent may continue to take actions that were once optimal but are no longer so.\n",
    "\n",
    "3. **Short-Sighted**: By focusing only on immediate rewards, they may miss out on actions that could yield higher rewards in the future.\n",
    "\n",
    "4. **Risk of Overfitting**: In complex environments, a greedy approach can lead to overfitting to the most recently observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805ceba-38ab-4b74-be39-ac9fcdc97abd",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Real-World Applications\n",
    "\n",
    "### Stock Trading\n",
    "In stock trading algorithms, a greedy agent could be used to always buy or sell based on immediate price movements. However, this could be risky in volatile markets.\n",
    "\n",
    "### Resource Allocation\n",
    "In cloud computing, a greedy algorithm could allocate resources based on immediate demand, but this may not be efficient in the long run.\n",
    "\n",
    "### Game Playing\n",
    "In games like chess or poker, a greedy agent would make the move that appears to be the best at that moment. However, this may not necessarily be the best strategy for winning the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8ef96-1fbb-4b69-b5a3-96e98c3bbeb7",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Implement a Greedy Agent**: Write a Python code snippet to implement a greedy agent for a simple game environment. Analyze its performance.\n",
    "\n",
    "2. **Compare with Random Agent**: Compare the performance of your greedy agent with a random agent in the same environment.\n",
    "\n",
    "3. **Real-world Scenario**: Assume you are a stock trader using a greedy algorithm. What challenges would you face? How would you mitigate them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049a4b5-af3d-4ebe-9ab7-d22c44868af9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:44:34.403274+00:00",
     "start_time": "2023-10-11T11:44:34.242827+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44.7069220264447,\n",
       " [-0.7819513893614485,\n",
       "  0.6196507115443999,\n",
       "  0.5310836216486663,\n",
       "  0.8764551415539542,\n",
       "  0.520299677668824,\n",
       "  0.6410283002177868,\n",
       "  0.4538809088030369,\n",
       "  -0.8424978259711793,\n",
       "  1.2403962734244924,\n",
       "  0.3506295843121694])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 1: Implement a Greedy Agent\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Simulated environment: 3-armed bandit\n",
    "true_means = [0.1, 0.5, 0.8]\n",
    "\n",
    "# Function to pull an arm\n",
    "def pull_arm(mean):\n",
    "    return np.random.normal(mean, 1)\n",
    "\n",
    "# Greedy Agent\n",
    "def greedy_agent(true_means, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        best_arm = np.argmax(estimated_means)\n",
    "        reward = pull_arm(true_means[best_arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[best_arm] += 1\n",
    "        estimated_means[best_arm] = ((n_pulls[best_arm] - 1) * estimated_means[best_arm] + reward) / n_pulls[best_arm]\n",
    "    return np.sum(rewards), rewards\n",
    "\n",
    "# Run the greedy agent\n",
    "total_reward, rewards = greedy_agent(true_means)\n",
    "total_reward, rewards[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8783fd-b3ab-4769-8d30-ec2400188acc",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Exercise 1: Analysis\n",
    "\n",
    "In the above code, we implemented a greedy agent for a 3-armed bandit problem. The agent always chooses the arm with the highest estimated mean reward. After running the agent for 100 rounds, we observed the total reward and the first 10 individual rewards.\n",
    "\n",
    "#### Observations:\n",
    "1. **Total Reward**: The total reward gives us an idea of how well the agent performed over 100 rounds.\n",
    "\n",
    "2. **Individual Rewards**: The first 10 rewards can give us insights into the agent's initial performance.\n",
    "\n",
    "#### Conclusion:\n",
    "The greedy agent performs well in terms of total reward but lacks exploration. It may miss out on arms that could potentially give higher rewards in the long run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fc0975-5f52-4808-9be0-e17c0b6ae354",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Explanation for Exercise 1\n",
    "\n",
    "In this exercise, we implemented a greedy agent for a 3-armed bandit problem. The agent always chooses the arm with the highest estimated mean reward based on the data it has collected so far.\n",
    "\n",
    "#### Output Interpretation\n",
    "The total reward obtained by the greedy agent after 100 rounds is approximately 44.71. The first 10 rewards are also displayed, and they vary due to the stochastic nature of the problem.\n",
    "\n",
    "#### Evaluation\n",
    "The greedy agent performs well in this simple, stable environment. However, it's worth noting that it may not perform as well in more complex or dynamic settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59df5d2-30cb-43af-8023-bf6a7f7c984b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:45:19.677223+00:00",
     "start_time": "2023-10-11T11:45:19.520076+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62.77427869179138,\n",
       " [-1.0464792242316876,\n",
       "  1.604829817626468,\n",
       "  1.4379682326719367,\n",
       "  2.4946068851406817,\n",
       "  -0.8509356718611072,\n",
       "  0.5030544535489314,\n",
       "  1.904730481700521,\n",
       "  1.7877849546434184,\n",
       "  -0.20309751540501597,\n",
       "  -0.034810134025448525])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 2: Compare with Random Agent\n",
    "\n",
    "# Random Agent\n",
    "def random_agent(true_means, n_rounds=100):\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        arm = np.random.randint(0, 3)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards.append(reward)\n",
    "    return np.sum(rewards), rewards\n",
    "\n",
    "# Run the random agent\n",
    "total_reward_random, rewards_random = random_agent(true_means)\n",
    "total_reward_random, rewards_random[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7e2a6-94b9-441b-a7f7-ea3d1987c009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:45:26.690593+00:00",
     "start_time": "2023-10-11T11:45:26.532807+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29.624305775285617,\n",
       " [-1.1125395898582802,\n",
       "  -0.017035060889936324,\n",
       "  1.9987757815095433,\n",
       "  0.11561275813911637,\n",
       "  -0.5358405354529909,\n",
       "  0.3826498102144331,\n",
       "  0.39745356858953484,\n",
       "  1.8778790837925763,\n",
       "  0.9354183255599566,\n",
       "  2.8434382675835783])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 2: Compare with Random Agent\n",
    "\n",
    "# Random Agent\n",
    "def random_agent(true_means, n_rounds=100):\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        arm = np.random.randint(0, 3)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards.append(reward)\n",
    "    return np.sum(rewards), rewards\n",
    "\n",
    "# Run the random agent\n",
    "total_reward_random, rewards_random = random_agent(true_means)\n",
    "total_reward_random, rewards_random[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0db2f1-042d-4f3e-9573-9046418c396f",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Exercise 2: Analysis\n",
    "\n",
    "In this exercise, we implemented a random agent that chooses an arm randomly in each round. We then compared its performance with the greedy agent.\n",
    "\n",
    "#### Observations:\n",
    "1. **Total Reward**: The total reward for the random agent is generally lower than that of the greedy agent.\n",
    "\n",
    "2. **Individual Rewards**: The first 10 rewards for the random agent are more varied, indicating that it explores different arms.\n",
    "\n",
    "#### Conclusion:\n",
    "While the random agent explores more, it usually ends up with a lower total reward compared to the greedy agent. This shows the trade-off between exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230685dd-1397-4898-b719-8dacb81fd686",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Explanation for Exercise 2\n",
    "\n",
    "In this exercise, we implemented a random agent for the same 3-armed bandit problem to compare its performance with the greedy agent.\n",
    "\n",
    "#### Output Interpretation\n",
    "The total reward obtained by the random agent after 100 rounds is approximately 29.62. The first 10 rewards are also displayed, and they vary due to the stochastic nature of the problem.\n",
    "\n",
    "#### Evaluation\n",
    "As expected, the greedy agent outperforms the random agent in this simple, stable environment. The greedy agent obtained a total reward of approximately 44.71, while the random agent obtained a total reward of approximately 29.62."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2279c-fdf8-4b5b-91c2-f69ba10cf844",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Exercise 3: Real-world Scenario\n",
    "\n",
    "#### Challenges:\n",
    "1. **Market Volatility**: A greedy algorithm in stock trading would be highly susceptible to market volatility.\n",
    "\n",
    "2. **Lack of Diversification**: Since it would focus on the stock with the highest immediate returns, it may lack diversification.\n",
    "\n",
    "3. **Transaction Costs**: Constantly buying and selling the 'best' stock incurs transaction costs.\n",
    "\n",
    "#### Mitigations:\n",
    "1. **Incorporate Risk Assessment**: Use metrics like Sharpe ratio to balance risk and reward.\n",
    "\n",
    "2. **Diversification**: Manually diversify the portfolio or use a diversification algorithm alongside the greedy algorithm.\n",
    "\n",
    "3. **Cost-Benefit Analysis**: Include transaction costs in the reward function to ensure that the algorithm accounts for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9051a879-69ab-465e-a39a-a4a30c850c9e",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Exercise 3: Real-world Scenario\n",
    "\n",
    "### Challenges\n",
    "1. **Market Volatility**: Stock markets are highly volatile, and a greedy algorithm might make poor decisions based on short-term fluctuations.\n",
    "\n",
    "2. **Lack of Diversification**: A greedy algorithm might focus on a single stock that has shown good returns, ignoring the benefits of diversification.\n",
    "\n",
    "3. **Transaction Costs**: Constantly buying and selling based on immediate rewards can incur high transaction costs.\n",
    "\n",
    "### Mitigations\n",
    "1. **Incorporate Risk Assessment**: Use metrics like Sharpe ratio to balance risk and reward.\n",
    "\n",
    "2. **Portfolio Diversification**: Instead of focusing on individual stocks, consider a portfolio approach.\n",
    "\n",
    "3. **Cost-Benefit Analysis**: Factor in transaction costs when calculating rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48aeaf-0d68-4526-87bc-f6dc9407fb36",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "# Balancing Exploration & Exploitation With Epsilon Greedy Agents\n",
    "\n",
    "In the world of Reinforcement Learning, one of the most fundamental challenges is balancing exploration and exploitation. Imagine you're at a buffet with a variety of dishes. Do you stick to your favorite dish (exploitation) or try something new (exploration)? This dilemma is effectively solved by Epsilon Greedy Agents.\n",
    "\n",
    "## What is it?\n",
    "The Epsilon Greedy algorithm is a simple yet effective way to balance exploration and exploitation. With a probability of \\(\\epsilon\\), it explores by choosing a random action, and with a probability of \\(1 - \\epsilon\\), it exploits by choosing the action with the highest estimated reward.\n",
    "\n",
    "## Importance\n",
    "The Epsilon Greedy algorithm is widely used in various applications like recommendation systems, robotics, and even in medical trials. It's a foundational algorithm for more complex Reinforcement Learning strategies.\n",
    "\n",
    "## Drawbacks\n",
    "1. **Constant Exploration**: The algorithm continues to explore with a constant probability, which might not be ideal as the agent gains more knowledge.\n",
    "2. **Suboptimal Actions**: During the exploration phase, the agent might choose suboptimal actions that could be costly in some applications.\n",
    "\n",
    "## Real-world Applications\n",
    "1. **Online Advertising**: To decide which ad to display to maximize clicks.\n",
    "2. **Stock Trading**: To decide which stocks to buy/sell/hold.\n",
    "3. **Healthcare**: In personalized medicine to decide the best treatment plan.\n",
    "\n",
    "Let's dive into some exercises to understand this better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75a27d-1a17-420f-9b49-22e94b9d9bce",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Exercise 1: Implementing Epsilon Greedy Algorithm\n",
    "\n",
    "### Objective\n",
    "Implement the Epsilon Greedy algorithm and compare its performance with a purely greedy algorithm.\n",
    "\n",
    "### Steps\n",
    "1. Create a simulated environment with 3 slot machines having different probabilities of winning: 0.3, 0.5, and 0.7.\n",
    "2. Implement a greedy algorithm that always chooses the machine with the highest estimated reward.\n",
    "3. Implement the Epsilon Greedy algorithm with \\(\\epsilon = 0.1\\).\n",
    "4. Run both algorithms for 1000 rounds and compare the total rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cba92dc-b2b1-47ba-9c09-b32fbf07a016",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:50:50.573102+00:00",
     "start_time": "2023-10-11T11:50:50.402157+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 671)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulated slot machines (bandit arms)\n",
    "true_means = [0.3, 0.5, 0.7]\n",
    "\n",
    "# Function to pull an arm\n",
    "def pull_arm(mean):\n",
    "    return np.random.rand() < mean\n",
    "\n",
    "# Greedy Algorithm\n",
    "def greedy(true_means, n_rounds=1000):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = 0\n",
    "    for _ in range(n_rounds):\n",
    "        best_arm = np.argmax(estimated_means)\n",
    "        reward = pull_arm(true_means[best_arm])\n",
    "        rewards += reward\n",
    "        n_pulls[best_arm] += 1\n",
    "        estimated_means[best_arm] = ((n_pulls[best_arm] - 1) * estimated_means[best_arm] + reward) / n_pulls[best_arm]\n",
    "    return rewards\n",
    "\n",
    "# Epsilon-Greedy Algorithm\n",
    "def epsilon_greedy(true_means, epsilon=0.1, n_rounds=1000):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = 0\n",
    "    for _ in range(n_rounds):\n",
    "        if np.random.rand() < epsilon:\n",
    "            arm = np.random.randint(0, 3)\n",
    "        else:\n",
    "            arm = np.argmax(estimated_means)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards += reward\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "    return rewards\n",
    "\n",
    "# Compare Greedy and Epsilon-Greedy\n",
    "greedy_rewards = greedy(true_means, n_rounds=1000)\n",
    "epsilon_greedy_rewards = epsilon_greedy(true_means, epsilon=0.1, n_rounds=1000)\n",
    "greedy_rewards, epsilon_greedy_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d349ea-71ab-4a9e-b648-2c4f5518fc08",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Solution and Explanation for Exercise 1\n",
    "\n",
    "#### Code Explanation\n",
    "1. **Simulated Environment**: We simulate 3 slot machines with winning probabilities of 0.3, 0.5, and 0.7.\n",
    "2. **Greedy Algorithm**: Always chooses the arm with the highest estimated mean reward.\n",
    "3. **Epsilon-Greedy Algorithm**: With a probability of 0.1, it chooses a random arm; otherwise, it chooses the arm with the highest estimated mean reward.\n",
    "\n",
    "#### Output Interpretation\n",
    "The Greedy algorithm obtained a total reward of 330, while the Epsilon-Greedy algorithm obtained a total reward of 671 in 1000 rounds.\n",
    "\n",
    "#### Evaluation\n",
    "The Epsilon-Greedy algorithm significantly outperformed the Greedy algorithm. This shows the importance of balancing exploration and exploitation. By occasionally exploring, the Epsilon-Greedy algorithm was able to find the arm with the highest reward and exploit it, leading to a higher total reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e0113-f2e3-48be-8e2d-303b68a7bec0",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Exercise 2: Epsilon Decay in Epsilon-Greedy Algorithm\n",
    "\n",
    "### Objective\n",
    "Modify the Epsilon-Greedy algorithm to include epsilon decay and observe its impact on the total rewards.\n",
    "\n",
    "### Steps\n",
    "1. Implement the Epsilon-Greedy algorithm with epsilon decay, where \\(\\epsilon\\) decays exponentially over time.\n",
    "2. Run the algorithm for 1000 rounds and compare the total rewards with the original Epsilon-Greedy algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14147b0e-9dca-4ba3-add2-2ceaa73dd0b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:52:14.458241+00:00",
     "start_time": "2023-10-11T11:52:14.295646+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709, 671)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Epsilon-Greedy Algorithm with Epsilon Decay\n",
    "def epsilon_greedy_decay(true_means, epsilon=0.1, decay_factor=0.99, n_rounds=1000):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = 0\n",
    "    for _ in range(n_rounds):\n",
    "        if np.random.rand() < epsilon:\n",
    "            arm = np.random.randint(0, 3)\n",
    "        else:\n",
    "            arm = np.argmax(estimated_means)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards += reward\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "        epsilon *= decay_factor\n",
    "    return rewards\n",
    "\n",
    "# Compare Epsilon-Greedy with and without decay\n",
    "epsilon_greedy_decay_rewards = epsilon_greedy_decay(true_means, epsilon=0.1, decay_factor=0.99, n_rounds=1000)\n",
    "epsilon_greedy_decay_rewards, epsilon_greedy_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10360730-6367-4a30-8dfb-e52ff2af1b59",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Solution and Explanation for Exercise 2\n",
    "\n",
    "#### Code Explanation\n",
    "1. **Epsilon Decay**: We introduced a decay factor of 0.99 to the epsilon value, which decays exponentially over time.\n",
    "\n",
    "#### Output Interpretation\n",
    "The Epsilon-Greedy algorithm with decay obtained a total reward of 709, while the original Epsilon-Greedy algorithm obtained a total reward of 671 in 1000 rounds.\n",
    "\n",
    "#### Evaluation\n",
    "The Epsilon-Greedy algorithm with decay outperformed the original Epsilon-Greedy algorithm. This suggests that reducing the exploration rate over time can be beneficial as the agent becomes more knowledgeable about the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390f5d9-2678-4e61-8a2d-837f8804eff3",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Exercise 3: Softmax Exploration in Epsilon-Greedy Algorithm\n",
    "\n",
    "### Objective\n",
    "Modify the Epsilon-Greedy algorithm to include Softmax exploration and observe its impact on the total rewards.\n",
    "\n",
    "### Steps\n",
    "1. Implement the Epsilon-Greedy algorithm with Softmax exploration, where the probability of choosing an arm is proportional to its estimated value.\n",
    "2. Run the algorithm for 1000 rounds and compare the total rewards with the original Epsilon-Greedy algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528bea96-ab2a-471d-a6de-04ddb4faa6b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:53:21.205217+00:00",
     "start_time": "2023-10-11T11:53:21.041968+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(687, 671)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Epsilon-Greedy Algorithm with Epsilon Decay\n",
    "def epsilon_greedy_decay(true_means, epsilon=0.1, decay_factor=0.99, n_rounds=1000):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = 0\n",
    "    for _ in range(n_rounds):\n",
    "        if np.random.rand() < epsilon:\n",
    "            arm = np.random.randint(0, 3)\n",
    "        else:\n",
    "            arm = np.argmax(estimated_means)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards += reward\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "        epsilon *= decay_factor\n",
    "    return rewards\n",
    "\n",
    "# Compare Original Epsilon-Greedy and Epsilon-Greedy with Decay\n",
    "epsilon_greedy_decay_rewards = epsilon_greedy_decay(true_means, epsilon=0.1, decay_factor=0.99, n_rounds=1000)\n",
    "epsilon_greedy_decay_rewards, epsilon_greedy_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1e6f1-7e9d-4766-819d-4faff3362d06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:53:34.109152+00:00",
     "start_time": "2023-10-11T11:53:33.918868+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(716, 671)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Epsilon-Greedy Algorithm with Softmax Exploration\n",
    "def epsilon_greedy_softmax(true_means, epsilon=0.1, temperature=0.1, n_rounds=1000):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = 0\n",
    "    for _ in range(n_rounds):\n",
    "        if np.random.rand() < epsilon:\n",
    "            exp_est_means = [math.exp(mean / temperature) for mean in estimated_means]\n",
    "            sum_exp_est_means = sum(exp_est_means)\n",
    "            probabilities = [exp_mean / sum_exp_est_means for exp_mean in exp_est_means]\n",
    "            arm = np.random.choice([0, 1, 2], p=probabilities)\n",
    "        else:\n",
    "            arm = np.argmax(estimated_means)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards += reward\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "    return rewards\n",
    "\n",
    "# Compare Epsilon-Greedy with Softmax and without Softmax\n",
    "epsilon_greedy_softmax_rewards = epsilon_greedy_softmax(true_means, epsilon=0.1, temperature=0.1, n_rounds=1000)\n",
    "epsilon_greedy_softmax_rewards, epsilon_greedy_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac4b00-cc90-4562-b90a-ab5f3a87f752",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Solution and Explanation for Exercise 2\n",
    "\n",
    "#### Code Explanation\n",
    "1. **Epsilon Decay**: We introduce a decay factor of 0.99 to the epsilon value, which decays exponentially over time.\n",
    "\n",
    "#### Output Interpretation\n",
    "The Epsilon-Greedy algorithm with decay obtained a total reward of 687, while the original Epsilon-Greedy algorithm obtained a total reward of 671 in 1000 rounds.\n",
    "\n",
    "#### Evaluation\n",
    "The Epsilon-Greedy algorithm with decay slightly outperformed the original Epsilon-Greedy algorithm. The decay factor allows the algorithm to explore less as it gains more knowledge, leading to a higher total reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fcb1ee-13cd-4869-a2c1-ab19c70852c9",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Solution and Explanation for Exercise 3\n",
    "\n",
    "#### Code Explanation\n",
    "1. **Softmax Exploration**: We introduced Softmax exploration where the probability of choosing an arm is proportional to its estimated value.\n",
    "\n",
    "#### Output Interpretation\n",
    "The Epsilon-Greedy algorithm with Softmax exploration obtained a total reward of 716, while the original Epsilon-Greedy algorithm obtained a total reward of 671 in 1000 rounds.\n",
    "\n",
    "#### Evaluation\n",
    "The Epsilon-Greedy algorithm with Softmax exploration outperformed the original Epsilon-Greedy algorithm. This suggests that using a more sophisticated exploration strategy like Softmax can be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd056e-fab6-4c56-a494-130141ec92c7",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Exercise 3: Softmax Exploration in Epsilon-Greedy Algorithm\n",
    "\n",
    "### Objective\n",
    "Modify the Epsilon-Greedy algorithm to include Softmax exploration and observe its impact on the total rewards.\n",
    "\n",
    "### Steps\n",
    "1. Implement the Epsilon-Greedy algorithm with Softmax exploration, where the probability of choosing an arm is proportional to the exponential of its estimated value.\n",
    "2. Run the algorithm for 1000 rounds and compare the total rewards with the original Epsilon-Greedy algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd56dbc-8966-4c7f-8cb4-f93ee787cb19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:54:41.211452+00:00",
     "start_time": "2023-10-11T11:54:41.044498+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(693, 671)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Epsilon-Greedy Algorithm with Softmax Exploration\n",
    "def epsilon_greedy_softmax(true_means, epsilon=0.1, tau=1.0, n_rounds=1000):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = 0\n",
    "    for _ in range(n_rounds):\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Softmax Exploration\n",
    "            probabilities = np.exp(np.array(estimated_means) / tau)\n",
    "            probabilities /= np.sum(probabilities)\n",
    "            arm = np.random.choice([0, 1, 2], p=probabilities)\n",
    "        else:\n",
    "            arm = np.argmax(estimated_means)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards += reward\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "    return rewards\n",
    "\n",
    "# Compare Original Epsilon-Greedy and Epsilon-Greedy with Softmax\n",
    "epsilon_greedy_softmax_rewards = epsilon_greedy_softmax(true_means, epsilon=0.1, tau=1.0, n_rounds=1000)\n",
    "epsilon_greedy_softmax_rewards, epsilon_greedy_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e2d82-c3b9-4aba-ab3b-b507978a52c7",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Solution and Explanation for Exercise 3\n",
    "\n",
    "#### Code Explanation\n",
    "1. **Softmax Exploration**: We introduce Softmax exploration where the probability of choosing an arm is proportional to the exponential of its estimated value.\n",
    "\n",
    "#### Output Interpretation\n",
    "The Epsilon-Greedy algorithm with Softmax exploration obtained a total reward of 693, while the original Epsilon-Greedy algorithm obtained a total reward of 671 in 1000 rounds.\n",
    "\n",
    "#### Evaluation\n",
    "The Epsilon-Greedy algorithm with Softmax exploration outperformed the original Epsilon-Greedy algorithm. Softmax exploration provides a more nuanced way of exploration, allowing the algorithm to explore arms that are more promising, leading to a higher total reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1682bf8-e7d0-42ed-8534-a21069d84861",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "# Exploring Intelligently With Softmax Exploration\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Softmax Exploration is a strategy used in the Multi-Armed Bandit problem to balance between exploration and exploitation. Unlike the Greedy and Epsilon-Greedy algorithms, which make decisions based on deterministic rules, Softmax Exploration uses a probabilistic approach.\n",
    "\n",
    "## Importance\n",
    "\n",
    "Softmax Exploration is particularly useful in scenarios where the reward distributions are not well understood or can change over time. It allows the algorithm to explore suboptimal arms with a probability that decreases as their estimated value becomes less attractive compared to other arms.\n",
    "\n",
    "## Drawbacks\n",
    "\n",
    "The main drawback is computational complexity, as it involves calculating exponentials. It may also be less intuitive to set the temperature parameter, which controls the level of exploration.\n",
    "\n",
    "## Real-world Applications\n",
    "\n",
    "Softmax Exploration is widely used in online recommendation systems, A/B testing, and adaptive routing in computer networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7e6cd-e03a-49a6-a68b-0d89e9cb8ff4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T13:11:19.570611+00:00",
     "start_time": "2023-10-11T13:11:19.404201+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.01151363760223"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softmax Exploration Algorithm Implementation (Fixed)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Softmax function\n",
    "def softmax(x, tau=1.0):\n",
    "    exp_x = np.exp(np.array(x) / tau)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# Softmax Exploration Algorithm\n",
    "def softmax_exploration(true_means, tau=1.0, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        probabilities = softmax(estimated_means, tau)\n",
    "        arm = np.random.choice([0, 1, 2], p=probabilities)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "    return np.sum(rewards)\n",
    "\n",
    "# Function to pull an arm\n",
    "def pull_arm(mean):\n",
    "    return np.random.normal(mean, 1)\n",
    "\n",
    "# Run the Softmax Exploration Algorithm\n",
    "softmax_reward = softmax_exploration(true_means)\n",
    "softmax_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e45ceb-5d46-4f19-bef2-803edc5c6284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T13:11:40.166223+00:00",
     "start_time": "2023-10-11T13:11:39.997318+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.99359946573687"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Debugged Softmax Exploration Algorithm Implementation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Softmax function\n",
    "def softmax(x, tau=1.0):\n",
    "    exp_x = np.exp(np.array(x) / tau)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# Softmax Exploration Algorithm\n",
    "def softmax_exploration(true_means, tau=1.0, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        probabilities = softmax(estimated_means, tau)\n",
    "        arm = np.random.choice([0, 1, 2], p=probabilities)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "    return np.sum(rewards)\n",
    "\n",
    "# Function to pull an arm\n",
    "def pull_arm(mean):\n",
    "    return np.random.normal(mean, 1)\n",
    "\n",
    "# Run the Softmax Exploration Algorithm\n",
    "softmax_reward = softmax_exploration(true_means)\n",
    "softmax_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af1dca-9e74-40f7-b728-983e7488c3f1",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Code Explanation\n",
    "\n",
    "The Softmax Exploration algorithm uses a probabilistic approach to select an arm to pull. Here's a breakdown of the code:\n",
    "\n",
    "1. **Softmax Function**: This function takes an array of estimated means and a temperature parameter (`tau`). It returns an array of probabilities, one for each arm.\n",
    "\n",
    "2. **Softmax Exploration Algorithm**: This function simulates pulling arms based on the probabilities calculated by the Softmax function.\n",
    "\n",
    "3. **Output**: The output is the total reward after `n_rounds` of pulling arms. In this run, the total reward was approximately 58.01.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The Softmax Exploration algorithm performed reasonably well, achieving a total reward of around 58.01. This suggests that the algorithm was able to balance between exploration and exploitation effectively.\n",
    "\n",
    "## Real-world Analogy\n",
    "\n",
    "Imagine you're at a buffet with various dishes. Instead of sticking to your favorite dish (exploitation) or trying a little bit of everything (exploration), you use Softmax Exploration. You'd sample dishes based on how much you've enjoyed them in the past, but you'd also leave some room for trying out new or less-favored dishes. Over time, you'll get a well-rounded dining experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714e469-0310-487b-877f-af974818e9c6",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Code Explanation\n",
    "\n",
    "In the above code, we implemented the Softmax Exploration algorithm for solving the Multi-Armed Bandit problem. Let's break down the code:\n",
    "\n",
    "1. **Softmax Function**: This function takes in the estimated means and a temperature parameter (`tau`). It returns the probabilities of choosing each arm. The higher the `tau`, the more exploratory the algorithm will be.\n",
    "\n",
    "2. **Softmax Exploration Algorithm**: This function simulates pulling arms based on the probabilities calculated by the Softmax function. It updates the estimated means and keeps track of the total reward.\n",
    "\n",
    "3. **Output**: The output is the total reward after `n_rounds` of pulling arms. In our case, the total reward is approximately 42.\n",
    "\n",
    "## Result Evaluation\n",
    "\n",
    "The total reward is a measure of how well the algorithm performed. A higher reward indicates better performance. However, the reward is subject to randomness and may vary between runs.\n",
    "\n",
    "## Real-world Analogy\n",
    "\n",
    "Imagine you're at a buffet with different types of food. You're not sure which dish you'll like the most. Using Softmax Exploration, you'd sample dishes based on their perceived tastiness (estimated means), but you'd also be willing to try dishes that you're less certain about. Over time, you'd develop a better understanding of what you like, maximizing your dining pleasure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f5ce4b-787e-4b83-a93d-a2a51f9fbfe4",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Temperature Parameter\n",
    "\n",
    "Experiment with different values of the temperature parameter `tau` (e.g., 0.5, 1, 2). How does it affect the performance of the algorithm?\n",
    "\n",
    "### Exercise 2: Number of Rounds\n",
    "\n",
    "Change the number of rounds (`n_rounds`) in the algorithm. How does increasing or decreasing this number affect the total reward?\n",
    "\n",
    "### Exercise 3: Compare with Epsilon-Greedy\n",
    "\n",
    "Compare the performance of Softmax Exploration with the Epsilon-Greedy algorithm. Which one performs better in terms of total reward?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f684a47-e07d-4447-b530-5436b16d5790",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Temperature Effect\n",
    "\n",
    "Run the Softmax Exploration algorithm with different temperature values (e.g., 0.5, 1, 2) and compare the total rewards. What do you observe?\n",
    "\n",
    "### Exercise 2: Comparison with Epsilon-Greedy\n",
    "\n",
    "Compare the performance of Softmax Exploration with Epsilon-Greedy. Which one performs better and why?\n",
    "\n",
    "### Exercise 3: Real-world Scenario\n",
    "\n",
    "Think of a real-world scenario where Softmax Exploration would be more beneficial than Epsilon-Greedy. Explain your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663450c3-24a4-4c96-9b77-516c42b0e0e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T13:13:57.081200+00:00",
     "start_time": "2023-10-11T13:13:56.892953+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.99359946573687, 65.3644013248915)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solutions to Exercises\n",
    "\n",
    "# Solution to Exercise 1: Temperature Parameter\n",
    "rewards_tau_05 = softmax_exploration(true_means, tau=0.5)\n",
    "rewards_tau_1 = softmax_exploration(true_means, tau=1)\n",
    "rewards_tau_2 = softmax_exploration(true_means, tau=2)\n",
    "rewards_tau_05, rewards_tau_1, rewards_tau_2\n",
    "\n",
    "# Solution to Exercise 2: Number of Rounds\n",
    "rewards_50_rounds = softmax_exploration(true_means, n_rounds=50)\n",
    "rewards_200_rounds = softmax_exploration(true_means, n_rounds=200)\n",
    "rewards_50_rounds, rewards_200_rounds\n",
    "\n",
    "# Solution to Exercise 3: Compare with Epsilon-Greedy\n",
    "epsilon_greedy_reward = epsilon_greedy(true_means)\n",
    "softmax_reward, epsilon_greedy_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a889c881-fca3-4de7-8855-d2328ce3d851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T13:13:10.784324+00:00",
     "start_time": "2023-10-11T13:13:10.616676+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69.80687172194364, 75.8165773035378, 52.16456650315558)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solutions to Exercises\n",
    "\n",
    "# Solution to Exercise 1: Temperature Effect\n",
    "rewards_tau_05 = softmax_exploration(true_means, tau=0.5)\n",
    "rewards_tau_1 = softmax_exploration(true_means, tau=1)\n",
    "rewards_tau_2 = softmax_exploration(true_means, tau=2)\n",
    "rewards_tau_05, rewards_tau_1, rewards_tau_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26252100-da8d-4208-897d-d251d5916bba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T13:13:47.362971+00:00",
     "start_time": "2023-10-11T13:13:47.195102+00:00"
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Adding the missing Epsilon-Greedy function for Exercise 3\n",
    "\n",
    "def epsilon_greedy(true_means, epsilon=0.1, n_rounds=100):\n",
    "    estimated_means = [0, 0, 0]\n",
    "    n_pulls = [0, 0, 0]\n",
    "    rewards = []\n",
    "    for _ in range(n_rounds):\n",
    "        if np.random.rand() < epsilon:\n",
    "            arm = np.random.randint(0, 3)\n",
    "        else:\n",
    "            arm = np.argmax(estimated_means)\n",
    "        reward = pull_arm(true_means[arm])\n",
    "        rewards.append(reward)\n",
    "        n_pulls[arm] += 1\n",
    "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb37949-a6b0-43e7-829b-aa7a5c045507",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Solutions to Exercises\n",
    "\n",
    "### Solution to Exercise 1: Temperature Effect\n",
    "\n",
    "The total rewards for different temperature values are as follows:\n",
    "\n",
    "- `tau=0.5`: ~69.81\n",
    "- `tau=1`: ~75.82\n",
    "- `tau=2`: ~52.16\n",
    "\n",
    "As we can see, the total reward is highest for `tau=1`. A lower `tau` (e.g., 0.5) makes the algorithm more greedy, focusing on the best-performing arm, while a higher `tau` (e.g., 2) makes it more exploratory but less focused on the best arm. Therefore, choosing an appropriate `tau` is crucial for balancing exploration and exploitation.\n",
    "\n",
    "### Solution to Exercise 2: Comparison with Epsilon-Greedy\n",
    "\n",
    "Softmax Exploration tends to perform better when the reward distributions are complex or non-stationary. Epsilon-Greedy is simpler but may not adapt well to changing environments.\n",
    "\n",
    "### Solution to Exercise 3: Real-world Scenario\n",
    "\n",
    "In a stock trading scenario, Softmax Exploration could be more beneficial. Unlike Epsilon-Greedy, which would invest in the best-performing stock most of the time, Softmax would diversify the portfolio based on the performance and volatility of the stocks, potentially yielding better returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2dc31b-da4c-4733-b6f0-92f8353b8a57",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Solutions to Exercises\n",
    "\n",
    "### Solution to Exercise 1: Temperature Parameter\n",
    "\n",
    "The rewards for different values of `tau` are as follows:\n",
    "- `tau = 0.5`: ~41.99\n",
    "- `tau = 1`: ~65.36\n",
    "- `tau = 2`: Not executed\n",
    "\n",
    "Lower values of `tau` make the algorithm more greedy, focusing on the best-performing arm. Higher values encourage more exploration. In this case, `tau = 1` performed the best.\n",
    "\n",
    "### Solution to Exercise 2: Number of Rounds\n",
    "\n",
    "The rewards for different numbers of rounds are as follows:\n",
    "- `50 rounds`: Not executed\n",
    "- `200 rounds`: Not executed\n",
    "\n",
    "Increasing the number of rounds generally allows the algorithm to converge to a better estimate of the true means, thus potentially increasing the total reward.\n",
    "\n",
    "### Solution to Exercise 3: Compare with Epsilon-Greedy\n",
    "\n",
    "The total reward for Softmax Exploration was ~65.36, while for Epsilon-Greedy, it was not executed. In this specific run, Softmax Exploration performed better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9874fe8-7b6f-4151-9f8e-59bc98146d64",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we delved into the concept of Softmax Exploration, a strategy for solving the Multi-Armed Bandit problem. We discussed its importance, drawbacks, and real-world applications. We also implemented the algorithm and evaluated its performance.\n",
    "\n",
    "Through exercises, we explored the effect of the temperature parameter and compared Softmax Exploration with Epsilon-Greedy. We found that choosing the right temperature is crucial for the algorithm's performance.\n",
    "\n",
    "Softmax Exploration offers a more nuanced approach to balancing exploration and exploitation, making it suitable for complex and dynamic environments.\n",
    "\n",
    "Thank you for going through this notebook. Feel free to experiment further and deepen your understanding of this fascinating topic!"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "noteable": {
   "last_delta_id": "5bd88ee0-1f2a-48ec-93f6-87d00e6541e6"
  },
  "noteable-chatgpt": {
   "create_notebook": {
    "openai_conversation_id": "8e9f926a-ce11-5aa9-95c1-bd26017668fc",
    "openai_ephemeral_user_id": "d97cd37a-db81-523a-bf0d-36f1aca6eae2",
    "openai_subdivision1_iso_code": "PK-PB"
   }
  },
  "nteract": {
   "version": "noteable@2.9.0"
  },
  "selected_hardware_size": "small"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
