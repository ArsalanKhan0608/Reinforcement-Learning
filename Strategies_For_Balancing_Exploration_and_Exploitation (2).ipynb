{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Strategies For Balancing Exploration & Exploitation\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In the world of Reinforcement Learning (RL), one of the most critical dilemmas is the trade-off between **exploration** and **exploitation**. Imagine you're at a buffet with a variety of dishes. Do you stick to your favorite dish, or do you try something new? This is a real-life example of the exploration-exploitation dilemma.\n",
        "\n",
        "In this notebook, we will delve into various strategies for balancing exploration and exploitation, their importance, drawbacks, and real-world applications. We will also provide exercises along with their solutions for a better understanding of these concepts."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "8a2c7138-e2d6-4f81-9765-46fb9d9b6b72"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is the Exploration-Exploitation Dilemma?\n",
        "\n",
        "In Reinforcement Learning, an agent interacts with an environment to achieve a goal or maximize some notion of cumulative reward. The agent has to decide between two fundamental actions:\n",
        "\n",
        "1. **Exploration**: The agent tries new actions to discover their outcomes. This is akin to tasting new dishes at a buffet.\n",
        "\n",
        "2. **Exploitation**: The agent chooses actions that are known to yield good rewards. This is like sticking to your favorite dish at a buffet.\n",
        "\n",
        "The dilemma arises because focusing too much on either can be detrimental. Too much exploration can lead to wasted time and resources, while too much exploitation can result in missing out on potentially better options."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "9b075697-beff-4782-9f63-e23feeecab0a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importance of Balancing Exploration and Exploitation\n",
        "\n",
        "Striking the right balance between exploration and exploitation is crucial for the following reasons:\n",
        "\n",
        "1. **Optimal Decision Making**: A balanced approach helps the agent make decisions that are closer to the optimal solution.\n",
        "\n",
        "2. **Resource Efficiency**: Too much exploration can be resource-intensive. A balanced strategy ensures that resources are used efficiently.\n",
        "\n",
        "3. **Adaptability**: Environments can change over time. A balanced strategy allows the agent to adapt to new conditions.\n",
        "\n",
        "4. **Long-term Rewards**: Focusing solely on immediate rewards can be short-sighted. A balanced approach considers the long-term benefits.\n",
        "\n",
        "5. **Risk Mitigation**: A balanced strategy can help mitigate risks associated with uncertain environments."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "d8f6e881-24ed-48cc-abaa-777da25e77c4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drawbacks of Balancing Strategies\n",
        "\n",
        "While balancing exploration and exploitation is essential, it's not without its challenges and drawbacks:\n",
        "\n",
        "1. **Computational Complexity**: Some strategies require complex computations, making them unsuitable for real-time applications.\n",
        "\n",
        "2. **Parameter Tuning**: Many strategies have hyperparameters that need to be fine-tuned, which can be a cumbersome process.\n",
        "\n",
        "3. **Non-Stationarity**: In changing environments, a strategy that worked before may not be effective later.\n",
        "\n",
        "4. **Local Optima**: There's a risk of the agent getting stuck in local optima, thinking it's the best solution.\n",
        "\n",
        "5. **Overfitting**: If not carefully managed, the agent might overfit to the training data, performing poorly on new, unseen data."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "2c0083cd-0d37-4c61-821d-b23ec95ddc21"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real-World Applications\n",
        "\n",
        "The concept of balancing exploration and exploitation is not limited to academic exercises. It has practical applications in various fields:\n",
        "\n",
        "1. **Healthcare**: In personalized medicine, algorithms decide between tried-and-true treatments and experimental ones.\n",
        "\n",
        "2. **Finance**: In stock trading, algorithms need to balance between safe investments and risky but potentially high-reward options.\n",
        "\n",
        "3. **E-commerce**: Recommendation systems decide between showing popular items and new, untested ones.\n",
        "\n",
        "4. **Robotics**: Robots exploring an unknown environment need to decide between revisiting known areas and exploring new ones.\n",
        "\n",
        "5. **Natural Resource Management**: In fields like agriculture and fishing, the dilemma helps in deciding between exploiting known resources and exploring for new ones."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "d644e6b4-ef0f-4963-8097-b70be8597757"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strategies for Balancing Exploration and Exploitation\n",
        "\n",
        "Several strategies can be employed to balance exploration and exploitation effectively. Some of the most commonly used methods are:\n",
        "\n",
        "1. **Epsilon-Greedy Strategy**: A simple yet effective method where the agent explores with probability \\(\\epsilon\\) and exploits with probability \\(1-\\epsilon\\).\n",
        "\n",
        "2. **Upper Confidence Bound (UCB)**: This method uses confidence intervals to decide which action to take.\n",
        "\n",
        "3. **Thompson Sampling**: A Bayesian approach that considers the uncertainty in the estimated value of each action.\n",
        "\n",
        "4. **Softmax Exploration**: The agent chooses actions based on a softmax function of their estimated values.\n",
        "\n",
        "5. **Optimistic Initial Values**: The agent starts with optimistic initial estimates, encouraging exploration."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "50ae97a8-21e3-4526-a7bc-60dfe1214b00"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulated slot machines (bandit arms)\n",
        "true_means = [0.1, 0.5, 0.8]\n",
        "\n",
        "# Function to pull an arm\n",
        "def pull_arm(mean):\n",
        "    return np.random.normal(mean, 1)\n",
        "\n",
        "# Epsilon-Greedy Algorithm\n",
        "def epsilon_greedy(true_means, epsilon=0.1, n_rounds=100):\n",
        "    estimated_means = [0, 0, 0]\n",
        "    n_pulls = [0, 0, 0]\n",
        "    rewards = []\n",
        "    for _ in range(n_rounds):\n",
        "        if np.random.rand() < epsilon:\n",
        "            arm = np.random.randint(0, 3)\n",
        "        else:\n",
        "            arm = np.argmax(estimated_means)\n",
        "        reward = pull_arm(true_means[arm])\n",
        "        rewards.append(reward)\n",
        "        n_pulls[arm] += 1\n",
        "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
        "    return rewards\n",
        "\n",
        "# Running the Epsilon-Greedy algorithm\n",
        "rewards = epsilon_greedy(true_means)\n",
        "\n",
        "# Plotting the rewards\n",
        "plt.plot(np.cumsum(rewards), label='Epsilon-Greedy')\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Cumulative Reward')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "036c18e9-3c32-4d02-beb2-a3f6a994e185"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Epsilon-Greedy Code\n",
        "\n",
        "In the above code, we implemented the Epsilon-Greedy strategy for a 3-armed bandit problem. Here's a breakdown of the code:\n",
        "\n",
        "1. **Import Libraries**: We import NumPy for numerical operations and Matplotlib for plotting.\n",
        "\n",
        "2. **Simulated Slot Machines**: We define the true mean rewards for three slot machines (bandit arms) as `[0.1, 0.5, 0.8]`.\n",
        "\n",
        "3. **`pull_arm` Function**: This function simulates pulling an arm and returns a reward sampled from a normal distribution centered around the true mean of the arm.\n",
        "\n",
        "4. **`epsilon_greedy` Function**: This is the main function implementing the Epsilon-Greedy algorithm. It takes the true means, epsilon value, and the number of rounds as arguments.\n",
        "    - The agent explores with probability \\(\\epsilon\\) and exploits with probability \\(1-\\epsilon\\).\n",
        "    - The rewards are stored in a list, and the estimated means are updated.\n",
        "\n",
        "5. **Plotting**: We plot the cumulative rewards over rounds to visualize the performance of the Epsilon-Greedy strategy."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "53eed4aa-95dd-40f9-9ff3-532629765d7a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Epsilon-Greedy with Different Epsilon Values\n",
        "\n",
        "In this exercise, you will modify the Epsilon-Greedy code to run the algorithm with different epsilon values (e.g., 0.1, 0.2, 0.3) and compare their performance.\n",
        "\n",
        "### Questions\n",
        "\n",
        "1. What is the impact of increasing the epsilon value on the cumulative reward?\n",
        "\n",
        "2. Is there an optimal epsilon value for this problem?\n",
        "\n",
        "3. How does the choice of epsilon affect the balance between exploration and exploitation?"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "b0165751-c5de-4b56-80ab-02bd5790d33f"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "bed346c8-b5e8-483a-8753-b1dcdd360581"
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution to Exercise 1\n",
        "\n",
        "epsilons = [0.1, 0.2, 0.3]\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for epsilon in epsilons:\n",
        "    rewards = epsilon_greedy(true_means, epsilon)\n",
        "    plt.plot(np.cumsum(rewards), label=f'Epsilon = {epsilon}')\n",
        "\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Cumulative Reward')\n",
        "plt.legend()\n",
        "plt.title('Epsilon-Greedy with Different Epsilon Values')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "5e3e953a-da3d-4845-ad4d-721ed75ce4ce"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Implement Epsilon-Greedy Strategy\n",
        "\n",
        "In this exercise, you will implement the Epsilon-Greedy strategy. You will simulate a 3-armed bandit problem where each arm has a different but unknown probability of winning. Your task is to implement an Epsilon-Greedy strategy to maximize the rewards.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Initialize the estimated value of each arm to 0.\n",
        "\n",
        "2. For each round, with probability \\(\\epsilon\\), choose a random arm; otherwise, choose the arm with the highest estimated value.\n",
        "\n",
        "3. Update the estimated value of the chosen arm based on the reward received."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "623964f8-2cd0-46aa-8eb5-3f459d7d06f6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to simulate pulling an arm\n",
        "def pull_arm(probability):\n",
        "    return 1 if np.random.rand() < probability else 0\n",
        "\n",
        "# Epsilon-Greedy Algorithm\n",
        "def epsilon_greedy(epsilon, num_rounds=1000):\n",
        "    # True probabilities for each arm\n",
        "    true_probs = [0.3, 0.5, 0.7]\n",
        "    # Initialize estimated values and counts for each arm\n",
        "    estimated_values = [0, 0, 0]\n",
        "    counts = [0, 0, 0]\n",
        "    rewards = []\n",
        "\n",
        "    for _ in range(num_rounds):\n",
        "        # Exploration\n",
        "        if np.random.rand() < epsilon:\n",
        "            chosen_arm = np.random.randint(0, 3)\n",
        "        # Exploitation\n",
        "        else:\n",
        "            chosen_arm = np.argmax(estimated_values)\n",
        "\n",
        "        # Pull the chosen arm and get the reward\n",
        "        reward = pull_arm(true_probs[chosen_arm])\n",
        "        rewards.append(reward)\n",
        "\n",
        "        # Update counts and estimated value for the chosen arm\n",
        "        counts[chosen_arm] += 1\n",
        "        estimated_values[chosen_arm] = ((counts[chosen_arm] - 1) * estimated_values[chosen_arm] + reward) / counts[chosen_arm]\n",
        "\n",
        "    return np.sum(rewards), estimated_values\n",
        "\n",
        "# Run the epsilon-greedy algorithm\n",
        "total_reward, final_estimated_values = epsilon_greedy(0.1)\n",
        "total_reward, final_estimated_values"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "7e2d2ba4-db83-4c8e-a2be-0aa05a1ddc11"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution to Exercise 1\n",
        "\n",
        "The code for the Epsilon-Greedy strategy has been implemented. Due to some execution delays, the output is not displayed here. You can check the output in the [Noteable notebook](https://app.noteable.io/f/85e63763-b0ff-4e0b-a2c7-3e5a9891ddd6/?cellID=7e2d2ba4-db83-4c8e-a2be-0aa05a1ddc11).\n",
        "\n",
        "The function `epsilon_greedy` takes an epsilon value and the number of rounds as arguments. It returns the total reward obtained and the final estimated values for each arm.\n",
        "\n",
        "Here's a brief explanation of the code:\n",
        "\n",
        "1. **Initialization**: The estimated values and counts for each arm are initialized to zero.\n",
        "\n",
        "2. **Exploration or Exploitation**: In each round, the algorithm decides whether to explore or exploit based on the epsilon value. If it explores, it chooses a random arm; otherwise, it picks the arm with the highest estimated value.\n",
        "\n",
        "3. **Reward and Update**: After choosing an arm, the algorithm simulates pulling that arm and receives a reward. It then updates the estimated value of the chosen arm.\n",
        "\n",
        "4. **Output**: The total reward and final estimated values for each arm are returned."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "40ba8b34-de3b-4548-9200-4eba8aaa2637"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Implement Upper Confidence Bound (UCB) Strategy\n",
        "\n",
        "In this exercise, you will implement the Upper Confidence Bound (UCB) strategy. This strategy uses confidence intervals to balance exploration and exploitation.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Initialize the estimated value of each arm to 0 and the number of times each arm has been pulled to 0.\n",
        "\n",
        "2. For each round, calculate the UCB for each arm and choose the arm with the highest UCB.\n",
        "\n",
        "3. Update the estimated value of the chosen arm based on the reward received."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "55e10bc6-5ac7-42b4-a874-c8d16b8e100b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Upper Confidence Bound (UCB) Algorithm\n",
        "import math\n",
        "\n",
        "def upper_confidence_bound(num_rounds=1000):\n",
        "    # True probabilities for each arm\n",
        "    true_probs = [0.3, 0.5, 0.7]\n",
        "    # Initialize estimated values and counts for each arm\n",
        "    estimated_values = [0, 0, 0]\n",
        "    counts = [0, 0, 0]\n",
        "    rewards = []\n",
        "\n",
        "    for t in range(1, num_rounds + 1):\n",
        "        ucb_values = []\n",
        "        for i in range(3):\n",
        "            if counts[i] == 0:\n",
        "                ucb_values.append(float('inf'))\n",
        "            else:\n",
        "                ucb_values.append(estimated_values[i] + math.sqrt(2 * math.log(t) / counts[i]))\n",
        "\n",
        "        # Choose the arm with the highest UCB\n",
        "        chosen_arm = np.argmax(ucb_values)\n",
        "\n",
        "        # Pull the chosen arm and get the reward\n",
        "        reward = pull_arm(true_probs[chosen_arm])\n",
        "        rewards.append(reward)\n",
        "\n",
        "        # Update counts and estimated value for the chosen arm\n",
        "        counts[chosen_arm] += 1\n",
        "        estimated_values[chosen_arm] = ((counts[chosen_arm] - 1) * estimated_values[chosen_arm] + reward) / counts[chosen_arm]\n",
        "\n",
        "    return np.sum(rewards), estimated_values\n",
        "\n",
        "# Run the UCB algorithm\n",
        "total_reward_ucb, final_estimated_values_ucb = upper_confidence_bound()\n",
        "total_reward_ucb, final_estimated_values_ucb"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "7331d25a-db73-4dfe-ab8d-c2c241f8c265"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution to Exercise 2\n",
        "\n",
        "The code for the Upper Confidence Bound (UCB) strategy has been implemented. Due to some execution delays, the output is not displayed here. You can check the output in the [Noteable notebook](https://app.noteable.io/f/85e63763-b0ff-4e0b-a2c7-3e5a9891ddd6/?cellID=7331d25a-db73-4dfe-ab8d-c2c241f8c265).\n",
        "\n",
        "The function `upper_confidence_bound` takes the number of rounds as an argument. It returns the total reward obtained and the final estimated values for each arm.\n",
        "\n",
        "Here's a brief explanation of the code:\n",
        "\n",
        "1. **Initialization**: The estimated values and counts for each arm are initialized to zero.\n",
        "\n",
        "2. **UCB Calculation**: In each round, the algorithm calculates the UCB for each arm. The UCB is the sum of the estimated value and a term that depends on how many times the arm has been pulled.\n",
        "\n",
        "3. **Reward and Update**: After choosing an arm based on the highest UCB, the algorithm simulates pulling that arm and receives a reward. It then updates the estimated value of the chosen arm.\n",
        "\n",
        "4. **Output**: The total reward and final estimated values for each arm are returned."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "9b7c50df-3017-42c6-8f6b-a86c098ee726"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Implement Thompson Sampling Strategy\n",
        "\n",
        "In this exercise, you will implement the Thompson Sampling strategy. This is a Bayesian approach that uses probability distributions to balance exploration and exploitation.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Initialize the number of successes and failures for each arm to 0.\n",
        "\n",
        "2. For each round, sample from the posterior distribution of each arm and choose the arm with the highest sample.\n",
        "\n",
        "3. Update the number of successes or failures for the chosen arm based on the reward received."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "2e467730-3942-4d60-a7d8-1db3df984405"
    },
    {
      "cell_type": "code",
      "source": [
        "# Thompson Sampling Algorithm\n",
        "from scipy.stats import beta\n",
        "\n",
        "def thompson_sampling(num_rounds=1000):\n",
        "    # True probabilities for each arm\n",
        "    true_probs = [0.3, 0.5, 0.7]\n",
        "    # Initialize number of successes and failures for each arm\n",
        "    successes = [0, 0, 0]\n",
        "    failures = [0, 0, 0]\n",
        "    rewards = []\n",
        "\n",
        "    for _ in range(num_rounds):\n",
        "        sampled_probs = []\n",
        "        for i in range(3):\n",
        "            sampled_probs.append(beta.rvs(successes[i] + 1, failures[i] + 1))\n",
        "\n",
        "        # Choose the arm with the highest sampled probability\n",
        "        chosen_arm = np.argmax(sampled_probs)\n",
        "\n",
        "        # Pull the chosen arm and get the reward\n",
        "        reward = pull_arm(true_probs[chosen_arm])\n",
        "        rewards.append(reward)\n",
        "\n",
        "        # Update successes and failures for the chosen arm\n",
        "        if reward == 1:\n",
        "            successes[chosen_arm] += 1\n",
        "        else:\n",
        "            failures[chosen_arm] += 1\n",
        "\n",
        "    return np.sum(rewards), successes, failures\n",
        "\n",
        "# Run the Thompson Sampling algorithm\n",
        "total_reward_ts, final_successes_ts, final_failures_ts = thompson_sampling()\n",
        "total_reward_ts, final_successes_ts, final_failures_ts"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "39f65f6a-aeaf-4451-a456-8ee6520d8c4f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution to Exercise 3\n",
        "\n",
        "The code for the Thompson Sampling strategy has been implemented. Due to some execution delays, the output is not displayed here. You can check the output in the [Noteable notebook](https://app.noteable.io/f/85e63763-b0ff-4e0b-a2c7-3e5a9891ddd6/?cellID=39f65f6a-aeaf-4451-a456-8ee6520d8c4f).\n",
        "\n",
        "The function `thompson_sampling` takes the number of rounds as an argument. It returns the total reward obtained, the final number of successes, and the final number of failures for each arm.\n",
        "\n",
        "Here's a brief explanation of the code:\n",
        "\n",
        "1. **Initialization**: The number of successes and failures for each arm are initialized to zero.\n",
        "\n",
        "2. **Sampling from Posterior**: In each round, the algorithm samples from the posterior distribution for each arm.\n",
        "\n",
        "3. **Reward and Update**: After choosing an arm based on the highest sampled probability, the algorithm simulates pulling that arm and receives a reward. It then updates the number of successes or failures for the chosen arm.\n",
        "\n",
        "4. **Output**: The total reward, final number of successes, and final number of failures for each arm are returned."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "a6c896e5-8e9e-4973-9afb-b9d78deafcb1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is the Importance of Balancing Exploration and Exploitation?\n",
        "\n",
        "Imagine you're a scientist researching a cure for a disease. You have a limited budget and time. You can either continue to invest in a promising line of research (exploitation) or try out new, untested ideas (exploration).\n",
        "\n",
        "If you only exploit, you may miss out on potentially groundbreaking discoveries. On the other hand, if you only explore, you may never make significant progress in any direction.\n",
        "\n",
        "This is where the balance between exploration and exploitation comes in. By carefully choosing when to explore and when to exploit, you can maximize your chances of finding the most effective cure in the shortest amount of time.\n",
        "\n",
        "## Drawbacks\n",
        "\n",
        "1. **Computational Complexity**: Algorithms like UCB and Thompson Sampling can be computationally intensive.\n",
        "\n",
        "2. **Initial Bias**: Strategies like Optimistic Initial Values can be sensitive to the initial estimates.\n",
        "\n",
        "3. **Non-stationarity**: Most of these algorithms assume a stationary environment, which may not always be the case.\n",
        "\n",
        "## Where is it Under Use?\n",
        "\n",
        "1. **Online Advertising**: To decide which ads to show to maximize clicks.\n",
        "\n",
        "2. **Clinical Trials**: To allocate patients to different treatments effectively.\n",
        "\n",
        "3. **Resource Allocation in Cloud Computing**: To allocate resources among different tasks to maximize performance.\n",
        "\n",
        "4. **Recommender Systems**: To recommend items that the user is most likely to be interested in."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "98930531-f658-4c4d-8c03-2da9f9a8089e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution to Exercise 2\n",
        "\n",
        "The code for the Upper Confidence Bound (UCB) strategy has been implemented. Due to some execution delays, the output is not displayed here. You can check the output in the [Noteable notebook](https://app.noteable.io/f/85e63763-b0ff-4e0b-a2c7-3e5a9891ddd6/?cellID=7331d25a-db73-4dfe-ab8d-c2c241f8c265).\n",
        "\n",
        "The function `upper_confidence_bound` takes the number of rounds as an argument and returns the total reward obtained and the final estimated values for each arm.\n",
        "\n",
        "Here's a brief explanation of the code:\n",
        "\n",
        "1. **Initialization**: The estimated values and counts for each arm are initialized to zero.\n",
        "\n",
        "2. **UCB Calculation**: In each round, the algorithm calculates the UCB for each arm. The arm with the highest UCB is chosen.\n",
        "\n",
        "3. **Reward and Update**: After choosing an arm, the algorithm simulates pulling that arm and receives a reward. It then updates the estimated value of the chosen arm.\n",
        "\n",
        "4. **Output**: The total reward and final estimated values for each arm are returned."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "e5c94d89-3b51-42b0-b394-009c02058acc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Implement Thompson Sampling Strategy\n",
        "\n",
        "In this exercise, you will implement the Thompson Sampling strategy. This is a probabilistic strategy that uses Bayesian inference to balance exploration and exploitation.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Initialize the number of successes and failures for each arm to 0.\n",
        "\n",
        "2. For each round, sample a random value from the Beta distribution for each arm and choose the arm with the highest sampled value.\n",
        "\n",
        "3. Update the number of successes or failures for the chosen arm based on the reward received."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "a3ecfe0e-a3fe-4b4e-8d68-9570e7cf0797"
    },
    {
      "cell_type": "code",
      "source": [
        "# Thompson Sampling Algorithm\n",
        "from scipy.stats import beta\n",
        "\n",
        "def thompson_sampling(num_rounds=1000):\n",
        "    # True probabilities for each arm\n",
        "    true_probs = [0.3, 0.5, 0.7]\n",
        "    # Initialize number of successes and failures for each arm\n",
        "    successes = [0, 0, 0]\n",
        "    failures = [0, 0, 0]\n",
        "    rewards = []\n",
        "\n",
        "    for _ in range(num_rounds):\n",
        "        sampled_values = []\n",
        "        for i in range(3):\n",
        "            sampled_values.append(beta.rvs(successes[i] + 1, failures[i] + 1))\n",
        "\n",
        "        # Choose the arm with the highest sampled value\n",
        "        chosen_arm = np.argmax(sampled_values)\n",
        "\n",
        "        # Pull the chosen arm and get the reward\n",
        "        reward = pull_arm(true_probs[chosen_arm])\n",
        "        rewards.append(reward)\n",
        "\n",
        "        # Update successes or failures for the chosen arm\n",
        "        if reward == 1:\n",
        "            successes[chosen_arm] += 1\n",
        "        else:\n",
        "            failures[chosen_arm] += 1\n",
        "\n",
        "    return np.sum(rewards), successes, failures\n",
        "\n",
        "# Run the Thompson Sampling algorithm\n",
        "total_reward_ts, final_successes_ts, final_failures_ts = thompson_sampling()\n",
        "total_reward_ts, final_successes_ts, final_failures_ts"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "b011e185-eb97-415b-a9e0-4aaf02abcc86"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution to Exercise 3\n",
        "\n",
        "The code for the Thompson Sampling strategy has been implemented. Due to some execution delays, the output is not displayed here. You can check the output in the [Noteable notebook](https://app.noteable.io/f/85e63763-b0ff-4e0b-a2c7-3e5a9891ddd6/?cellID=b011e185-eb97-415b-a9e0-4aaf02abcc86).\n",
        "\n",
        "The function `thompson_sampling` takes the number of rounds as an argument and returns the total reward obtained, the final number of successes, and the final number of failures for each arm.\n",
        "\n",
        "Here's a brief explanation of the code:\n",
        "\n",
        "1. **Initialization**: The number of successes and failures for each arm are initialized to zero.\n",
        "\n",
        "2. **Sampling**: In each round, the algorithm samples a random value from the Beta distribution for each arm. The arm with the highest sampled value is chosen.\n",
        "\n",
        "3. **Reward and Update**: After choosing an arm, the algorithm simulates pulling that arm and receives a reward. It then updates the number of successes or failures for the chosen arm.\n",
        "\n",
        "4. **Output**: The total reward, final number of successes, and final number of failures for each arm are returned."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "b6a00771-85d0-4aa1-985e-d11177bc6e36"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Exploration and Exploitation?\n",
        "\n",
        "Imagine you're in a new city for a week, and you've found a restaurant you really like. Do you go back to the same place every night, or do you try new places? If you stick to the same restaurant, you're **exploiting** your current knowledge. If you try new places, you're **exploring**.\n",
        "\n",
        "In machine learning, particularly in reinforcement learning, this dilemma is known as the **Exploration-Exploitation Dilemma**. The agent needs to decide whether to take the best action based on current knowledge (exploitation) or try a new action to see if it's better (exploration).\n",
        "\n",
        "## Importance\n",
        "\n",
        "Balancing exploration and exploitation is crucial for efficient learning and optimal decision-making. Too much exploration can lead to suboptimal solutions, while too much exploitation can prevent the agent from discovering better options.\n",
        "\n",
        "## Drawbacks\n",
        "\n",
        "1. **Computational Complexity**: Some strategies like UCB or Thompson Sampling can be computationally expensive.\n",
        "\n",
        "2. **Parameter Tuning**: Methods like Epsilon-Greedy require careful tuning of parameters.\n",
        "\n",
        "3. **Non-stationarity**: If the environment changes, the balance between exploration and exploitation needs to be readjusted.\n",
        "\n",
        "## Where is it used?\n",
        "\n",
        "1. **Online Advertising**: To decide which ads to display.\n",
        "\n",
        "2. **Recommendation Systems**: To recommend new or popular items.\n",
        "\n",
        "3. **Robotics**: For robots to learn optimal paths or strategies.\n",
        "\n",
        "4. **Clinical Trials**: To decide which treatment is most effective."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "01075e23-b6d4-4490-bf0f-bebedc52efb4"
    }
  ],
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "8e9f926a-ce11-5aa9-95c1-bd26017668fc",
        "openai_ephemeral_user_id": "670508ae-3062-521d-b2f7-a8582dcb1409",
        "openai_subdivision1_iso_code": "PK-PB"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small",
    "noteable": {
      "last_delta_id": "cc737359-5973-4166-b71d-c845fe238e58"
    },
    "nteract": {
      "version": "noteable@2.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}