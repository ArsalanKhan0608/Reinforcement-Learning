{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Being Optimistic Under Uncertainties\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In the world of decision-making, especially in uncertain environments, being optimistic can be a strategic advantage. This concept is often employed in Reinforcement Learning under the strategy known as **Optimistic Initialization**.\n",
        "\n",
        "### What is it?\n",
        "\n",
        "Being optimistic under uncertainties involves initializing the estimated value of each action to be optimistically high. This encourages the agent to explore each action at least once before settling into a more exploitative behavior.\n",
        "\n",
        "### Importance\n",
        "\n",
        "1. **Encourages Exploration**: The agent is more likely to try all available actions.\n",
        "2. **Quick Convergence**: Helps the agent to quickly find the action that yields the highest reward.\n",
        "3. **Robustness**: Makes the agent robust to non-stationary environments where the reward distribution can change over time.\n",
        "\n",
        "### Drawbacks\n",
        "\n",
        "1. **Over-exploration**: The agent might waste time exploring obviously suboptimal actions.\n",
        "2. **Computational Overhead**: Requires additional computation to keep track of optimistic values.\n",
        "\n",
        "### Real-World Applications\n",
        "\n",
        "1. **Stock Market**: Traders often use optimistic strategies to explore new investment opportunities.\n",
        "2. **Healthcare**: In drug discovery, an optimistic approach can lead to the exploration of new molecular structures.\n",
        "3. **Marketing**: Marketers use it to test various strategies before committing to one.\n",
        "\n",
        "In this notebook, we will go through exercises to understand this concept better."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "fa32adc5-b512-46ee-b0a2-8115fc80b4c8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "6650a5b1-d8e9-43e8-8274-290ccd4ab11b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Implementing Optimistic Initialization\n",
        "\n",
        "In this exercise, you will implement the optimistic initialization strategy in a simple 3-armed bandit problem. The true mean rewards for the arms are [0.2, 0.5, 0.8].\n",
        "\n",
        "### Task\n",
        "\n",
        "1. Initialize the estimated mean rewards for each arm to 1 (an optimistic value).\n",
        "2. In each round, choose the arm with the highest estimated mean reward.\n",
        "3. Pull the chosen arm and update its estimated mean based on the observed reward.\n",
        "\n",
        "Compare the total rewards obtained with and without optimistic initialization."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "ad84a25d-b150-4aee-a65e-09b3c195f883"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to pull an arm\n",
        "def pull_arm(mean):\n",
        "    return np.random.normal(mean, 1)\n",
        "\n",
        "# Optimistic Initialization Algorithm\n",
        "def optimistic_initialization(true_means, initial_value=1, n_rounds=100):\n",
        "    estimated_means = [initial_value, initial_value, initial_value]\n",
        "    n_pulls = [0, 0, 0]\n",
        "    rewards = []\n",
        "    for _ in range(n_rounds):\n",
        "        best_arm = np.argmax(estimated_means)\n",
        "        reward = pull_arm(true_means[best_arm])\n",
        "        rewards.append(reward)\n",
        "        n_pulls[best_arm] += 1\n",
        "        estimated_means[best_arm] = ((n_pulls[best_arm] - 1) * estimated_means[best_arm] + reward) / n_pulls[best_arm]\n",
        "    return np.sum(rewards), estimated_means\n",
        "\n",
        "# True mean rewards for the arms\n",
        "true_means = [0.2, 0.5, 0.8]\n",
        "\n",
        "# Running the algorithm\n",
        "total_reward, final_estimated_means = optimistic_initialization(true_means)\n",
        "total_reward, final_estimated_means"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "540b1704-3eb1-41e6-8c2d-5c7478d17506"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of Exercise 1: Implementing Optimistic Initialization\n",
        "\n",
        "It seems that the code cell did not run as expected. However, let's go through what it aims to achieve.\n",
        "\n",
        "### Algorithm Steps:\n",
        "1. **Initialize estimated means**: We start with an optimistic initial value of 1 for the estimated mean rewards of each arm.\n",
        "2. **Choose the Best Arm**: In each round, the arm with the highest estimated mean reward is selected.\n",
        "3. **Pull the Arm**: The chosen arm is pulled, and the reward is observed.\n",
        "4. **Update Estimates**: The estimated mean reward of the pulled arm is updated based on the observed reward.\n",
        "\n",
        "### Expected Output:\n",
        "The output should show the total rewards obtained after 100 rounds and the final estimated mean rewards for each arm.\n",
        "\n",
        "### Real-world Analogy:\n",
        "Imagine you are at a buffet with three types of dishes. Being optimistic, you initially assume all are delicious. As you try each dish, you update your 'mental rating' for them. Eventually, you find the dish that satisfies your taste buds the most, much like how the algorithm finds the arm with the highest reward."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "c384a0dd-5de7-40d4-8106-bf41a9aaada1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Comparing Optimistic Initialization with Epsilon-Greedy\n",
        "\n",
        "In this exercise, you will compare the performance of the optimistic initialization strategy with the epsilon-greedy strategy.\n",
        "\n",
        "### Task\n",
        "\n",
        "1. Implement the epsilon-greedy strategy with an epsilon value of 0.1.\n",
        "2. Run both the optimistic initialization and epsilon-greedy strategies for 100 rounds.\n",
        "3. Plot the total rewards obtained in each round for both strategies.\n",
        "\n",
        "Analyze which strategy performs better and why."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "38558406-6bc6-4b44-af1a-4c2cb506d12b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Epsilon-Greedy Algorithm\n",
        "def epsilon_greedy(true_means, epsilon=0.1, n_rounds=100):\n",
        "    estimated_means = [0, 0, 0]\n",
        "    n_pulls = [0, 0, 0]\n",
        "    rewards = []\n",
        "    for _ in range(n_rounds):\n",
        "        if np.random.rand() < epsilon:\n",
        "            arm = np.random.randint(0, 3)\n",
        "        else:\n",
        "            arm = np.argmax(estimated_means)\n",
        "        reward = pull_arm(true_means[arm])\n",
        "        rewards.append(reward)\n",
        "        n_pulls[arm] += 1\n",
        "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
        "    return np.sum(rewards), estimated_means\n",
        "\n",
        "# Running both algorithms\n",
        "total_reward_optimistic, _ = optimistic_initialization(true_means)\n",
        "total_reward_epsilon, _ = epsilon_greedy(true_means)\n",
        "\n",
        "# Plotting the results\n",
        "plt.bar(['Optimistic Initialization', 'Epsilon-Greedy'], [total_reward_optimistic, total_reward_epsilon])\n",
        "plt.ylabel('Total Rewards')\n",
        "plt.title('Comparison of Optimistic Initialization and Epsilon-Greedy')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "eaf8425e-1d1e-44d3-9a54-2ae9cbdea4e1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of Exercise 2: Comparing Optimistic Initialization with Epsilon-Greedy\n",
        "\n",
        "The code cell didn't run, but let's discuss what it aims to do.\n",
        "\n",
        "### Algorithm Steps:\n",
        "1. **Epsilon-Greedy Algorithm**: It uses a small probability (epsilon) to explore random arms and a high probability (1-epsilon) to exploit the best-known arm.\n",
        "2. **Optimistic Initialization**: It starts with an optimistic initial value for each arm and exploits the best-known arm.\n",
        "\n",
        "### Expected Output:\n",
        "The output should be a bar chart comparing the total rewards obtained by both strategies after 100 rounds.\n",
        "\n",
        "### Real-world Analogy:\n",
        "Imagine you're trying to decide between two investment strategies. One is more conservative, diversifying your portfolio (Epsilon-Greedy), while the other is more optimistic, putting more money into what seems to be the best option (Optimistic Initialization). Over time, you'd compare the returns from both to decide which strategy is more effective."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "f0ab4bd6-4cf8-4f45-beda-30ff63167ad1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Optimistic Initialization in Non-Stationary Environments\n",
        "\n",
        "In this exercise, you will explore the performance of optimistic initialization in a non-stationary environment, where the true mean rewards of the arms change over time.\n",
        "\n",
        "### Task\n",
        "\n",
        "1. Modify the `pull_arm` function to make the environment non-stationary. For example, add a small random value to the mean reward of each arm in each round.\n",
        "2. Run the optimistic initialization strategy in this non-stationary environment for 100 rounds.\n",
        "3. Plot the total rewards obtained in each round.\n",
        "\n",
        "Analyze how well the optimistic initialization strategy adapts to the changing environment."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "cd096737-f843-4448-86d5-6ad8cce5e7f3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified pull_arm function for non-stationary environment\n",
        "def pull_arm_non_stationary(mean):\n",
        "    return np.random.normal(mean + np.random.normal(0, 0.1), 1)\n",
        "\n",
        "# Optimistic Initialization Algorithm for Non-Stationary Environment\n",
        "def optimistic_initialization_non_stationary(true_means, initial_value=1, n_rounds=100):\n",
        "    estimated_means = [initial_value, initial_value, initial_value]\n",
        "    n_pulls = [0, 0, 0]\n",
        "    rewards = []\n",
        "    for _ in range(n_rounds):\n",
        "        best_arm = np.argmax(estimated_means)\n",
        "        reward = pull_arm_non_stationary(true_means[best_arm])\n",
        "        rewards.append(reward)\n",
        "        n_pulls[best_arm] += 1\n",
        "        estimated_means[best_arm] = ((n_pulls[best_arm] - 1) * estimated_means[best_arm] + reward) / n_pulls[best_arm]\n",
        "    return np.sum(rewards), estimated_means\n",
        "\n",
        "# Running the algorithm in a non-stationary environment\n",
        "total_reward_non_stationary, final_estimated_means_non_stationary = optimistic_initialization_non_stationary(true_means)\n",
        "total_reward_non_stationary, final_estimated_means_non_stationary"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "c75583c9-096c-45fc-b299-d17b88fc4c14"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of Exercise 3: Optimistic Initialization in Non-Stationary Environments\n",
        "\n",
        "The code cell didn't run, but let's discuss what it aims to do.\n",
        "\n",
        "### Algorithm Steps:\n",
        "1. **Modified Pull Arm Function**: The `pull_arm_non_stationary` function adds a small random value to the mean reward of each arm, making the environment non-stationary.\n",
        "2. **Optimistic Initialization**: The algorithm starts with an optimistic initial value for each arm and exploits the best-known arm.\n",
        "\n",
        "### Expected Output:\n",
        "The output should show the total rewards obtained after 100 rounds and the final estimated mean rewards for each arm in a non-stationary environment.\n",
        "\n",
        "### Real-world Analogy:\n",
        "Imagine you're a stock trader in a volatile market. The value of stocks (arms) changes frequently (non-stationary environment). Being optimistic, you initially assume high returns for each stock. As you trade, you update your expectations based on the actual returns, allowing you to adapt to the market's volatility."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "7d241d3f-3a3c-44c7-bf92-975c25e41382"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of Exercise 2: Comparing Optimistic Initialization with Epsilon-Greedy\n",
        "\n",
        "### Algorithm Steps:\n",
        "1. **Epsilon-Greedy**: In this strategy, with probability \\(\\epsilon\\), we choose a random arm, and with probability \\(1-\\epsilon\\), we choose the arm with the highest estimated mean.\n",
        "2. **Optimistic Initialization**: Here, we start with an optimistic initial value for each arm and always choose the arm with the highest estimated mean.\n",
        "\n",
        "### Expected Output:\n",
        "The output should be a bar chart comparing the total rewards obtained by both strategies after 100 rounds.\n",
        "\n",
        "### Analysis:\n",
        "Optimistic Initialization tends to perform better in the early rounds as it encourages exploration. However, Epsilon-Greedy can catch up as it balances exploration and exploitation.\n",
        "\n",
        "### Real-world Analogy:\n",
        "Imagine you are choosing between two investment strategies. One is optimistic, assuming high returns but diversifying quickly (Optimistic Initialization). The other is more balanced, taking calculated risks (Epsilon-Greedy). Over time, you'll notice that while the optimistic strategy may give quick initial gains, the balanced approach could yield more stable returns."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "af646fb6-e8b6-47e6-aa65-53eac1e65fb1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Optimistic Initialization in Non-Stationary Environments\n",
        "\n",
        "In this exercise, you will explore how optimistic initialization performs in non-stationary environments, where the true mean rewards for the arms can change over time.\n",
        "\n",
        "### Task\n",
        "\n",
        "1. Modify the `pull_arm` function to add a small random value to the mean reward of each arm in each round.\n",
        "2. Run the optimistic initialization strategy for 200 rounds.\n",
        "3. Plot the estimated mean rewards for each arm over time.\n",
        "\n",
        "Analyze how well the strategy adapts to the changing environment."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "80d19ab7-70da-4841-85be-a868fcdc02ec"
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified pull_arm function for non-stationary environment\n",
        "def pull_arm_non_stationary(mean):\n",
        "    return np.random.normal(mean + np.random.normal(0, 0.1), 1)\n",
        "\n",
        "# Optimistic Initialization Algorithm for non-stationary environment\n",
        "def optimistic_initialization_non_stationary(true_means, initial_value=1, n_rounds=200):\n",
        "    estimated_means = [initial_value, initial_value, initial_value]\n",
        "    n_pulls = [0, 0, 0]\n",
        "    rewards = []\n",
        "    estimated_means_over_time = [[] for _ in range(3)]\n",
        "    for _ in range(n_rounds):\n",
        "        best_arm = np.argmax(estimated_means)\n",
        "        reward = pull_arm_non_stationary(true_means[best_arm])\n",
        "        rewards.append(reward)\n",
        "        n_pulls[best_arm] += 1\n",
        "        estimated_means[best_arm] = ((n_pulls[best_arm] - 1) * estimated_means[best_arm] + reward) / n_pulls[best_arm]\n",
        "        for i in range(3):\n",
        "            estimated_means_over_time[i].append(estimated_means[i])\n",
        "    return estimated_means_over_time\n",
        "\n",
        "# Running the algorithm\n",
        "estimated_means_over_time = optimistic_initialization_non_stationary(true_means)\n",
        "\n",
        "# Plotting the estimated means over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(3):\n",
        "    plt.plot(estimated_means_over_time[i], label=f'Arm {i+1}')\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Estimated Mean Reward')\n",
        "plt.title('Estimated Mean Rewards Over Time in Non-Stationary Environment')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "7601437d-16ae-4e3b-b678-4f9a5f910998"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of Exercise 3: Optimistic Initialization in Non-Stationary Environments\n",
        "\n",
        "### Algorithm Steps:\n",
        "1. **Modified Pull Arm**: In this version of the `pull_arm` function, a small random value is added to the mean reward of each arm in each round to simulate a non-stationary environment.\n",
        "2. **Optimistic Initialization**: The algorithm is similar to the one in Exercise 1 but adapted for a non-stationary environment.\n",
        "\n",
        "### Expected Output:\n",
        "The output should be a line chart showing how the estimated mean rewards for each arm change over 200 rounds.\n",
        "\n",
        "### Analysis:\n",
        "Optimistic Initialization can adapt to non-stationary environments but may be slower to react to changes compared to more sophisticated algorithms.\n",
        "\n",
        "### Real-world Analogy:\n",
        "Imagine you're a farmer who is optimistic about the weather. You plant crops based on this optimism. However, the weather is non-stationary; it changes. Your optimism might help you take risks and plant various crops, but you'll need to adapt your strategies as the seasons change."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "50f54132-aee4-41ad-ac31-99fbf17ee461"
    }
  ],
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "8e9f926a-ce11-5aa9-95c1-bd26017668fc",
        "openai_ephemeral_user_id": "670508ae-3062-521d-b2f7-a8582dcb1409",
        "openai_subdivision1_iso_code": "PK-PB"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small",
    "noteable": {
      "last_delta_id": "5ceeb1cf-c95c-4a6a-9151-19f9255f23f4"
    },
    "nteract": {
      "version": "noteable@2.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}