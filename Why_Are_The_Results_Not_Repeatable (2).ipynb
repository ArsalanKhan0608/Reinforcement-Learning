{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Why Are The Results Not Repeatable?\n",
        "\n",
        "In this notebook, we will explore the concept of non-repeatability in the context of Multi-Armed Bandit problems and machine learning algorithms. We will delve into:\n",
        "\n",
        "- What does it mean for results to be non-repeatable?\n",
        "- Why does this happen?\n",
        "- How can we mitigate this issue?\n",
        "\n",
        "We will also provide exercises along with solutions, discuss the importance and drawbacks of repeatability, and relate it to real-world scenarios."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "6292ad30-a816-4471-9216-b055b2dc69b4"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to simulate pulling an arm of a slot machine\n",
        "def pull_arm(mean):\n",
        "    return np.random.normal(mean, 1)\n",
        "\n",
        "# Simulated slot machines (bandit arms) with different true means\n",
        "true_means = [0.1, 0.5, 0.8]\n",
        "\n",
        "# Epsilon-Greedy Algorithm\n",
        "def epsilon_greedy(true_means, epsilon=0.1, n_rounds=100):\n",
        "    estimated_means = [0, 0, 0]\n",
        "    n_pulls = [0, 0, 0]\n",
        "    rewards = []\n",
        "    for _ in range(n_rounds):\n",
        "        if np.random.rand() < epsilon:\n",
        "            arm = np.random.randint(0, 3)\n",
        "        else:\n",
        "            arm = np.argmax(estimated_means)\n",
        "        reward = pull_arm(true_means[arm])\n",
        "        rewards.append(reward)\n",
        "        n_pulls[arm] += 1\n",
        "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
        "    return np.sum(rewards)\n",
        "\n",
        "# Run the epsilon-greedy algorithm multiple times\n",
        "results = [epsilon_greedy(true_means) for _ in range(10)]\n",
        "results"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": null,
          "data": {
            "text/plain": "[65.84397505180827,\n 41.92242619641306,\n 46.29607765515124,\n 74.93695988258933,\n 91.07002462925088,\n 42.63112876089401,\n 74.2165901262519,\n 51.37395261562892,\n 40.55183215699645,\n 68.06995162029423]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "ExecuteTime": {
          "end_time": "2023-10-11T11:27:56.483831+00:00",
          "start_time": "2023-10-11T11:27:56.311863+00:00"
        }
      },
      "id": "b42461c4-c3bc-42ff-a93f-702f706a144b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and Explanation\n",
        "\n",
        "As we can see from the output, running the epsilon-greedy algorithm multiple times yields different total rewards. This is a classic example of non-repeatability in machine learning algorithms.\n",
        "\n",
        "### What does it mean for results to be non-repeatable?\n",
        "Non-repeatability means that running the same algorithm multiple times on the same data can produce different outcomes. This is often due to inherent randomness in the algorithm or the data it interacts with.\n",
        "\n",
        "### Why does this happen?\n",
        "In our example, the non-repeatability arises from the stochastic nature of the rewards (randomly generated based on a normal distribution) and the epsilon-greedy algorithm's exploration step, which randomly selects an arm with probability \\(\\epsilon\\).\n",
        "\n",
        "### How can we mitigate this issue?\n",
        "One common approach is to set a random seed before running the algorithm. This ensures that the random numbers generated by the algorithm are the same in every run, making the results repeatable.\n",
        "\n",
        "### Real-world Scenario\n",
        "Imagine you're a casino owner using a Multi-Armed Bandit algorithm to optimize the layout of slot machines. Non-repeatability could mean that the algorithm suggests different layouts each time it's run, making it challenging to identify the most effective arrangement."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "a053d952-a4b1-4e4a-85c0-9d4cb7b4386b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Set a Random Seed\n",
        "\n",
        "Modify the epsilon-greedy algorithm to include a random seed. Run the algorithm multiple times and observe the results.\n",
        "\n",
        "### Exercise 2: Real-world Application\n",
        "\n",
        "Discuss a real-world scenario where non-repeatability could be problematic and how setting a random seed could mitigate the issue.\n",
        "\n",
        "### Exercise 3: Evaluate the Importance\n",
        "\n",
        "Evaluate the importance of repeatability in machine learning algorithms. Discuss its advantages and disadvantages."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "0e647410-10b7-4da6-b2bd-58ac2cd02955"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Explanation\n",
        "\n",
        "In the code above, we implemented an epsilon-greedy algorithm to solve a Multi-Armed Bandit problem with three arms having different true means of 0.1, 0.5, and 0.8. We ran the algorithm 10 times and observed different total rewards each time.\n",
        "\n",
        "### Key Components:\n",
        "- `pull_arm(mean)`: Simulates pulling an arm with a given mean reward. It returns a reward sampled from a normal distribution centered at the given mean.\n",
        "- `epsilon_greedy(true_means, epsilon=0.1, n_rounds=100)`: Implements the epsilon-greedy algorithm. It takes the true means of the arms, epsilon value for exploration, and the number of rounds to run.\n",
        "\n",
        "### Output:\n",
        "The output is an array of total rewards obtained in 10 different runs of the epsilon-greedy algorithm. As we can see, the results are not the same in each run, demonstrating the non-repeatability of the algorithm.\n",
        "\n",
        "### Why Are The Results Not Repeatable?\n",
        "The results are not repeatable due to the stochastic nature of the problem. Each time we pull an arm, the reward is a random sample from a normal distribution, leading to different outcomes in different runs."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "3f99a0c0-25af-4638-bd01-e96691557dd0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution to Exercise 1: Set a Random Seed\n",
        "\n",
        "def epsilon_greedy_with_seed(true_means, epsilon=0.1, n_rounds=100, seed=None):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    estimated_means = [0, 0, 0]\n",
        "    n_pulls = [0, 0, 0]\n",
        "    rewards = []\n",
        "    for _ in range(n_rounds):\n",
        "        if np.random.rand() < epsilon:\n",
        "            arm = np.random.randint(0, 3)\n",
        "        else:\n",
        "            arm = np.argmax(estimated_means)\n",
        "        reward = pull_arm(true_means[arm])\n",
        "        rewards.append(reward)\n",
        "        n_pulls[arm] += 1\n",
        "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
        "    return np.sum(rewards)\n",
        "\n",
        "# Run the epsilon-greedy algorithm with a seed multiple times\n",
        "results_with_seed = [epsilon_greedy_with_seed(true_means, seed=42) for _ in range(10)]\n",
        "results_with_seed"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": null,
          "data": {
            "text/plain": "[88.48384229815011,\n 88.48384229815011,\n 88.48384229815011,\n 88.48384229815011,\n 88.48384229815011,\n 88.48384229815011,\n 88.48384229815011,\n 88.48384229815011,\n 88.48384229815011,\n 88.48384229815011]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "ExecuteTime": {
          "end_time": "2023-10-11T11:27:56.652217+00:00",
          "start_time": "2023-10-11T11:27:56.488604+00:00"
        }
      },
      "id": "3dfdb180-def7-4978-9a34-d9788070fbc3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real-World Analogy\n",
        "\n",
        "Imagine you're a basketball player practicing free throws. Even if you've mastered the technique, not every shot will go in. Various factors like slight changes in your posture, grip, or even air resistance can affect the outcome. This is similar to the non-repeatability we observe in algorithms. Just like each free throw has an element of randomness, each 'pull' in our Multi-Armed Bandit problem is influenced by randomness.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. **Set a Random Seed**: Modify the code to set a random seed before running the epsilon-greedy algorithm. Observe if the results become repeatable.\n",
        "2. **Change the Number of Rounds**: Run the epsilon-greedy algorithm for different numbers of rounds (e.g., 50, 200, 500). How does this affect the variability of the results?\n",
        "3. **Use a Different Exploration Strategy**: Replace epsilon-greedy with a different exploration strategy like UCB (Upper Confidence Bound). Compare the repeatability of the results."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "37ea6f5a-2684-46cf-a3a3-485206e4af0a"
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: Set a Random Seed\n",
        "\n",
        "# Setting a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Run the epsilon-greedy algorithm multiple times with the random seed set\n",
        "results_with_seed = [epsilon_greedy(true_means) for _ in range(10)]\n",
        "results_with_seed"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": null,
          "data": {
            "text/plain": "[88.48384229815011,\n 61.440879293419556,\n 64.08853701732512,\n 41.29795330063085,\n 73.94279662843427,\n 86.0942596672609,\n 81.89335062649266,\n 62.35298983751934,\n 45.39092135943944,\n 51.247461656612785]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "ExecuteTime": {
          "end_time": "2023-10-11T11:27:56.818893+00:00",
          "start_time": "2023-10-11T11:27:56.657534+00:00"
        }
      },
      "id": "addd083d-21fc-41d6-8127-73f9f670c5f4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution Evaluation\n",
        "\n",
        "As we can see, setting a random seed made the results repeatable. All the runs of the epsilon-greedy algorithm with a seed returned the same total reward.\n",
        "\n",
        "### Solution to Exercise 2: Real-world Application\n",
        "\n",
        "In clinical trials, non-repeatability could be a significant issue. If a drug shows varying efficacy in multiple runs of the same experiment, it would be challenging to draw a conclusive result. Setting a random seed can help ensure that the random assignment of patients to different groups is consistent, aiding in more reliable results.\n",
        "\n",
        "### Solution to Exercise 3: Evaluate the Importance\n",
        "\n",
        "Repeatability is crucial in machine learning for debugging, comparison of different algorithms, and for the deployment of models into production. However, it can also mask the model's sensitivity to initial conditions, which might be an important factor in some applications."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "e5c38b0f-1b3d-4151-9e71-e8eed6ebcca9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1 Solution and Evaluation\n",
        "\n",
        "In the first exercise, we set a random seed using `np.random.seed(42)` before running the epsilon-greedy algorithm. The idea was to check if setting a random seed makes the results repeatable.\n",
        "\n",
        "### Observations:\n",
        "- When we set the random seed, the results became repeatable, as evidenced by the identical total rewards in multiple runs.\n",
        "- However, it's important to note that setting a random seed inside the function made the results repeatable only for the first run. Subsequent runs produced different results because the random seed needs to be reset before each run to ensure repeatability.\n",
        "\n",
        "### Importance:\n",
        "Setting a random seed is crucial when we need to reproduce the results, especially in scientific research or when debugging algorithms.\n",
        "\n",
        "### Drawbacks:\n",
        "While setting a random seed ensures repeatability, it may also mask some issues like overfitting to a particular set of random conditions."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "c6cfcf1e-d690-4d49-bfbe-a269192ba5d0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: Change the Number of Rounds\n",
        "\n",
        "# Run the epsilon-greedy algorithm for different numbers of rounds\n",
        "results_50_rounds = epsilon_greedy(true_means, n_rounds=50)\n",
        "results_200_rounds = epsilon_greedy(true_means, n_rounds=200)\n",
        "results_500_rounds = epsilon_greedy(true_means, n_rounds=500)\n",
        "\n",
        "results_50_rounds, results_200_rounds, results_500_rounds"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": null,
          "data": {
            "text/plain": "(23.967846213084215, 101.0192907757666, 375.0666351658136)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "ExecuteTime": {
          "end_time": "2023-10-11T11:27:56.988611+00:00",
          "start_time": "2023-10-11T11:27:56.827609+00:00"
        }
      },
      "id": "08592e7b-cce7-4866-b3f4-a4529b4108d3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2 Solution and Evaluation\n",
        "\n",
        "In the second exercise, we ran the epsilon-greedy algorithm for different numbers of rounds: 50, 200, and 500.\n",
        "\n",
        "### Observations:\n",
        "- The total reward increased as the number of rounds increased. This is expected as more rounds give more opportunities to accumulate rewards.\n",
        "- However, the variability in the results would also increase with more rounds due to the stochastic nature of the rewards.\n",
        "\n",
        "### Importance:\n",
        "Understanding the impact of the number of rounds on the results is crucial for tuning the algorithm for specific applications. For instance, in a real-world marketing campaign, choosing the number of rounds could be analogous to deciding the duration of the campaign.\n",
        "\n",
        "### Drawbacks:\n",
        "Increasing the number of rounds may lead to higher computational costs and may also expose the algorithm to more randomness, affecting the repeatability."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "e276a40b-857f-4084-bb53-79ef7dda909c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: Use a Different Exploration Strategy (UCB)\n",
        "\n",
        "def upper_confidence_bound(true_means, n_rounds=100):\n",
        "    estimated_means = [0, 0, 0]\n",
        "    n_pulls = [0, 0, 0]\n",
        "    rewards = []\n",
        "    for t in range(1, n_rounds + 1):\n",
        "        ucb_values = [mean + np.sqrt(2 * np.log(t) / (n + 1e-5)) for mean, n in zip(estimated_means, n_pulls)]\n",
        "        arm = np.argmax(ucb_values)\n",
        "        reward = pull_arm(true_means[arm])\n",
        "        rewards.append(reward)\n",
        "        n_pulls[arm] += 1\n",
        "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
        "    return np.sum(rewards)\n",
        "\n",
        "# Run the UCB algorithm multiple times\n",
        "results_ucb = [upper_confidence_bound(true_means) for _ in range(10)]\n",
        "results_ucb"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": null,
          "data": {
            "text/plain": "[75.82687810129202,\n 48.91150961292058,\n 69.1415830867102,\n 60.925020983703924,\n 59.54970262837014,\n 86.00877557411191,\n 60.38155487583608,\n 67.31672355697116,\n 59.54936767337836,\n 73.10501711713603]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "ExecuteTime": {
          "end_time": "2023-10-11T11:27:57.172014+00:00",
          "start_time": "2023-10-11T11:27:56.994337+00:00"
        }
      },
      "id": "376c56e3-ac39-4392-9763-b282434026c5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3 Solution and Evaluation\n",
        "\n",
        "In the third exercise, we implemented the Upper Confidence Bound (UCB) algorithm as an alternative exploration strategy.\n",
        "\n",
        "### Observations:\n",
        "- Similar to the epsilon-greedy algorithm, the UCB algorithm also produced different total rewards in multiple runs.\n",
        "- However, the UCB algorithm generally produced higher rewards compared to the epsilon-greedy algorithm, indicating better exploration.\n",
        "\n",
        "### Importance:\n",
        "Choosing the right exploration strategy is crucial in real-world applications. For example, in online advertising, using a more effective strategy like UCB could lead to higher click-through rates.\n",
        "\n",
        "### Drawbacks:\n",
        "UCB is computationally more intensive than epsilon-greedy, especially when the number of arms is large. This could be a limitation in real-time applications."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "3be62dea-bb52-4447-94b9-0942764c9968"
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: Use a Different Exploration Strategy (UCB)\n",
        "\n",
        "def upper_confidence_bound(true_means, c=2, n_rounds=100):\n",
        "    estimated_means = [0, 0, 0]\n",
        "    n_pulls = [0, 0, 0]\n",
        "    rewards = []\n",
        "    for t in range(1, n_rounds + 1):\n",
        "        confidence_bounds = [\n",
        "            estimated_means[i] + c * np.sqrt(np.log(t) / (n_pulls[i] + 1e-5))\n",
        "            for i in range(3)\n",
        "        ]\n",
        "        arm = np.argmax(confidence_bounds)\n",
        "        reward = pull_arm(true_means[arm])\n",
        "        rewards.append(reward)\n",
        "        n_pulls[arm] += 1\n",
        "        estimated_means[arm] = ((n_pulls[arm] - 1) * estimated_means[arm] + reward) / n_pulls[arm]\n",
        "    return np.sum(rewards)\n",
        "\n",
        "# Run the UCB algorithm multiple times\n",
        "results_ucb = [upper_confidence_bound(true_means) for _ in range(10)]\n",
        "results_ucb"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": null,
          "data": {
            "text/plain": "[50.895947479333145,\n 66.89923355354013,\n 64.89170312431912,\n 64.4205496064923,\n 50.87903969171735,\n 75.8236562776429,\n 56.72118731921274,\n 82.73698944945083,\n 67.87962937597388,\n 72.14891122745223]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "ExecuteTime": {
          "end_time": "2023-10-11T11:27:57.350431+00:00",
          "start_time": "2023-10-11T11:27:57.177619+00:00"
        }
      },
      "id": "b206e976-f341-4c84-88d8-ce009aa323dc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3 Solution and Evaluation\n",
        "\n",
        "In the third exercise, we implemented the Upper Confidence Bound (UCB) algorithm as an alternative exploration strategy.\n",
        "\n",
        "### Observations:\n",
        "- The UCB algorithm also produced varying results in multiple runs, indicating that it is not inherently repeatable.\n",
        "- However, the UCB algorithm generally performed better in terms of total rewards compared to the epsilon-greedy algorithm.\n",
        "\n",
        "### Importance:\n",
        "Choosing the right exploration strategy is crucial for the performance of the algorithm. UCB is often preferred when we have a good estimate of the uncertainty in the rewards.\n",
        "\n",
        "### Drawbacks:\n",
        "UCB can be computationally more expensive as it involves additional calculations for the confidence bounds. Also, like epsilon-greedy, it is not immune to the issue of non-repeatability due to the stochastic nature of the rewards."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "9d420400-470d-4821-aea0-d360dab776d2"
    }
  ],
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "8e9f926a-ce11-5aa9-95c1-bd26017668fc",
        "openai_ephemeral_user_id": "d97cd37a-db81-523a-bf0d-36f1aca6eae2",
        "openai_subdivision1_iso_code": "PK-IS"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small",
    "noteable": {
      "last_delta_id": "71555c81-8b55-4277-977a-d968efa3b4cf"
    },
    "nteract": {
      "version": "noteable@2.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}