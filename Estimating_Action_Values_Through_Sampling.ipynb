{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Estimating Action Values Through Sampling\n",
        "\n",
        "In this notebook, we'll explore the concept of estimating action values through sampling in the context of Reinforcement Learning. We'll delve into what it is, its importance, drawbacks, and real-world applications. We'll also provide exercises along with their solutions for a comprehensive understanding.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Introduction](#Introduction)\n",
        "2. [Importance](#Importance)\n",
        "3. [Drawbacks](#Drawbacks)\n",
        "4. [Real-world Applications](#Real-world-Applications)\n",
        "5. [Exercises](#Exercises)\n",
        "6. [Exercise Solutions](#Exercise-Solutions)"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "34d8c48d-a3b5-42d9-8db4-866b30d19830"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Estimating action values through sampling is a fundamental concept in Reinforcement Learning (RL). In RL, an agent interacts with an environment to achieve a goal. The agent takes actions, and the environment responds by providing rewards and new states. The agent's objective is to find a policy—a mapping from states to actions—that maximizes the expected sum of rewards.\n",
        "\n",
        "Action values, also known as Q-values, represent the expected return (sum of rewards) of taking a particular action from a given state and then following a specific policy. Estimating these Q-values accurately is crucial for the agent to make informed decisions.\n",
        "\n",
        "Sampling is one of the methods to estimate these action values. In this method, the agent takes an action multiple times and averages the observed rewards to estimate the action value.\n",
        "\n",
        "Let's consider a simple example to understand this better."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "82fe65c0-b03a-4a3f-af00-02499f7be2ba"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the true action values\n",
        "true_action_values = [1.2, 0.8, 1.5, 1.3, 0.9]\n",
        "\n",
        "# Function to simulate pulling an arm of a bandit\n",
        "def pull_arm(action):\n",
        "    return np.random.normal(true_action_values[action], 0.1)\n",
        "\n",
        "# Function to estimate action values through sampling\n",
        "def estimate_action_values(n_samples=1000):\n",
        "    estimated_values = [0] * 5\n",
        "    for action in range(5):\n",
        "        samples = [pull_arm(action) for _ in range(n_samples)]\n",
        "        estimated_values[action] = np.mean(samples)\n",
        "    return estimated_values\n",
        "\n",
        "# Estimate action values\n",
        "estimated_values = estimate_action_values()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(5), true_action_values, alpha=0.6, label='True Action Values')\n",
        "plt.bar(range(5), estimated_values, alpha=0.6, label='Estimated Action Values')\n",
        "plt.xlabel('Actions')\n",
        "plt.ylabel('Action Value')\n",
        "plt.legend()\n",
        "plt.title('True vs Estimated Action Values')\n",
        "plt.show()\n",
        "\n",
        "estimated_values"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "382f38c4-3af6-4874-86a6-adc50f968a93"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Explanation\n",
        "\n",
        "In the code above, we simulated a 5-armed bandit problem where each arm has a different true action value. We then estimated these action values through sampling.\n",
        "\n",
        "### Key Components:\n",
        "1. **True Action Values**: These are the true means of the reward distributions for each arm. We set them as `[1.2, 0.8, 1.5, 1.3, 0.9]`.\n",
        "2. **`pull_arm(action)` Function**: This function simulates pulling an arm and returns a reward sampled from a normal distribution centered around the true action value of the pulled arm.\n",
        "3. **`estimate_action_values(n_samples)` Function**: This function estimates the action values by pulling each arm `n_samples` times and averaging the rewards.\n",
        "\n",
        "### Results:\n",
        "The bar chart compares the true action values with the estimated action values. As we can see, the estimated values are close to the true values, demonstrating the effectiveness of sampling as a method for estimating action values.\n",
        "\n",
        "### Evaluation:\n",
        "The estimated action values are close to the true action values, indicating that our sampling method is effective. However, it's worth noting that the accuracy of these estimates depends on the number of samples. More samples will generally lead to more accurate estimates but at the cost of computational time."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "729f7967-667b-47be-a5c8-ca3a3a9fc68f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importance\n",
        "\n",
        "Estimating action values accurately is crucial for the success of any RL agent. Here's why:\n",
        "\n",
        "1. **Informed Decision-Making**: Accurate estimates allow the agent to make decisions that are more likely to result in higher rewards.\n",
        "2. **Efficiency**: With accurate estimates, the agent can quickly identify the best actions, reducing the need for excessive exploration.\n",
        "3. **Adaptability**: In non-stationary environments where reward distributions can change, having a reliable estimation method helps the agent adapt more quickly.\n",
        "\n",
        "### Real-world Analogy\n",
        "\n",
        "Imagine you're trying to find the best coffee shop in town. You could visit each shop multiple times, sampling different types of coffee. By averaging your experiences (rewards), you can estimate the quality (action value) of each shop. This way, you can make an informed decision about which shop to frequent, similar to how an RL agent estimates action values to choose the best actions."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "48f60b23-e82e-4eb2-a8ce-3aedb0ea0339"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drawbacks\n",
        "\n",
        "While estimating action values through sampling is effective, it has its limitations:\n",
        "\n",
        "1. **Computational Cost**: Taking multiple samples for each action can be computationally expensive, especially in large action spaces.\n",
        "2. **Non-Stationarity**: In environments where the reward distributions change over time, the agent needs to continuously update its estimates, which can be challenging.\n",
        "3. **Initial Bias**: If the initial samples are not representative, the estimates can be biased, leading to suboptimal decisions.\n",
        "\n",
        "### Real-world Analogy\n",
        "\n",
        "Continuing with the coffee shop example, if a shop recently changed its coffee beans, your previous samples might not be representative of the current quality. Also, continuously sampling from all shops to keep your estimates updated would be time-consuming and impractical."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "efc31985-9bcf-45b8-811b-1debb4c10cd9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real-world Applications\n",
        "\n",
        "Estimating action values through sampling has various real-world applications:\n",
        "\n",
        "1. **Finance**: In algorithmic trading, agents can use sampling to estimate the expected returns of different trading strategies.\n",
        "2. **Healthcare**: In personalized medicine, algorithms can sample different treatment options to estimate their effectiveness for individual patients.\n",
        "3. **Robotics**: Robots can use sampling methods to estimate the success rates of different actions, such as picking up objects or navigating through a space.\n",
        "\n",
        "### Real-world Analogy\n",
        "\n",
        "In a warehouse, a robotic arm sorts packages onto different conveyor belts. By sampling, it can estimate which actions (e.g., speed and angle of movement) result in the most efficient sorting, thereby optimizing its performance."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "b8845bd9-8520-49d1-a606-661ee482e0d3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n",
        "\n",
        "To deepen your understanding, here are some exercises:\n",
        "\n",
        "1. **Exercise 1**: Modify the code to estimate action values using a different number of samples (e.g., 500, 2000). Compare the results with the original estimates.\n",
        "2. **Exercise 2**: Implement a method to update the action value estimates in a non-stationary environment. Simulate a changing environment and observe how well your method adapts.\n",
        "3. **Exercise 3**: Add a confidence interval to the estimated action values in the bar chart. Use the standard error of the mean as a measure of uncertainty."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "8afa6a41-c8bd-4a15-b7e8-88015562bbf1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise Solutions\n",
        "\n",
        "Below are the solutions to the exercises:\n",
        "\n",
        "### Solution to Exercise 1\n",
        "\n",
        "To modify the code for different numbers of samples, you can change the `n_samples` parameter in the `estimate_action_values` function. Here's how you can do it for 500 and 2000 samples."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "29cdfff1-4b7c-4f71-8454-16d1691a6ac1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate action values with 500 samples\n",
        "estimated_values_500 = estimate_action_values(n_samples=500)\n",
        "\n",
        "# Estimate action values with 2000 samples\n",
        "estimated_values_2000 = estimate_action_values(n_samples=2000)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.bar(range(5), estimated_values, alpha=0.6, label='Estimated with 1000 samples')\n",
        "plt.xlabel('Actions')\n",
        "plt.ylabel('Action Value')\n",
        "plt.title('Estimated with 1000 samples')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.bar(range(5), estimated_values_500, alpha=0.6, label='Estimated with 500 samples', color='g')\n",
        "plt.xlabel('Actions')\n",
        "plt.title('Estimated with 500 samples')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.bar(range(5), estimated_values_2000, alpha=0.6, label='Estimated with 2000 samples', color='r')\n",
        "plt.xlabel('Actions')\n",
        "plt.title('Estimated with 2000 samples')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "estimated_values_500, estimated_values_2000"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "f54ebc26-db09-42a9-8aa1-eca9f5a19dfe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Exercise 1 Solution\n",
        "\n",
        "As we can see from the bar charts, the estimates become more accurate as we increase the number of samples. The estimates with 2000 samples are closer to the original estimates made with 1000 samples, demonstrating the benefit of having more samples for better accuracy.\n",
        "\n",
        "However, it's important to note that increasing the number of samples also increases the computational cost. Therefore, there's a trade-off between accuracy and computational efficiency."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "ea17cf9a-cc86-4181-accd-2b7639313d43"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution to Exercise 2\n",
        "\n",
        "To handle a non-stationary environment, we can use a moving average to update the action value estimates. This allows the agent to adapt to changes in the reward distribution."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "9dce894c-bc49-4fbd-ba20-1552ed13734e"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to estimate action values in a non-stationary environment using moving average\n",
        "def estimate_action_values_moving_avg(alpha=0.1, n_rounds=1000):\n",
        "    estimated_values = [0] * 5\n",
        "    for action in range(5):\n",
        "        for _ in range(n_rounds):\n",
        "            reward = pull_arm(action)\n",
        "            estimated_values[action] = (1 - alpha) * estimated_values[action] + alpha * reward\n",
        "    return estimated_values\n",
        "\n",
        "# Simulate a non-stationary environment by changing the true action values\n",
        "true_action_values = [1.5, 0.9, 1.2, 1.6, 1.0]\n",
        "\n",
        "# Estimate action values using moving average\n",
        "estimated_values_moving_avg = estimate_action_values_moving_avg()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(5), true_action_values, alpha=0.6, label='True Action Values (Changed)')\n",
        "plt.bar(range(5), estimated_values_moving_avg, alpha=0.6, label='Estimated Action Values (Moving Avg)', color='r')\n",
        "plt.xlabel('Actions')\n",
        "plt.ylabel('Action Value')\n",
        "plt.legend()\n",
        "plt.title('True vs Estimated Action Values in Non-Stationary Environment')\n",
        "plt.show()\n",
        "\n",
        "estimated_values_moving_avg"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "60608e35-142f-44c7-ab5f-b1d6f05ff73c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real-world Applications\n",
        "\n",
        "Estimating action values through sampling has various applications:\n",
        "\n",
        "1. **Healthcare**: In personalized medicine, it helps in choosing the most effective treatment for individual patients.\n",
        "2. **Finance**: Used in portfolio optimization to estimate the expected returns of different assets.\n",
        "3. **E-commerce**: Helps in recommending the most relevant products to users.\n",
        "4. **Robotics**: In robotic arms, it assists in selecting the most efficient movements.\n",
        "\n",
        "### Real-world Analogy\n",
        "\n",
        "Think of it like a talent scout for a sports team. The scout watches multiple games (samples) to estimate the skill level (action value) of each player. Based on these estimates, the team can make informed decisions on which players to recruit."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "21b71870-0cde-4cda-b597-cfd5f6efc42a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n",
        "\n",
        "### Exercise 1: Implement Sampling for a 3-Armed Bandit\n",
        "\n",
        "Implement a function that estimates the action values of a 3-armed bandit through sampling. The true action values are `[0.9, 0.8, 0.7]`. Run the function and plot the estimated values.\n",
        "\n",
        "### Exercise 2: Vary the Number of Samples\n",
        "\n",
        "Modify the function from Exercise 1 to accept the number of samples as an argument. Run the function with different numbers of samples and observe how the estimates change.\n",
        "\n",
        "### Exercise 3: Non-Stationary Bandit\n",
        "\n",
        "Implement a function that estimates the action values for a non-stationary 3-armed bandit. The true action values change over time. Run the function and plot the estimated values at different time steps."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "a824197d-dca7-4de1-8406-dfbc0db769c1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Exercise 2 Solution\n",
        "\n",
        "The bar chart shows the estimated action values using a moving average in a non-stationary environment where the true action values have changed. The estimates are close to the new true action values, indicating that the moving average method is effective in adapting to changes in the environment.\n",
        "\n",
        "This adaptability is crucial in real-world applications where conditions can change over time."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "29305393-06d3-49cc-abe6-e6ec3a04ffd1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution to Exercise 3\n",
        "\n",
        "To add confidence intervals to the estimated action values, we can use the standard error of the mean. The standard error is calculated as the standard deviation of the samples divided by the square root of the number of samples."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "210d5db3-6263-4fe8-99ca-22c4633708f1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: Implement Sampling for a 3-Armed Bandit\n",
        "\n",
        "true_action_values_3arm = [0.9, 0.8, 0.7]\n",
        "\n",
        "def pull_arm_3arm(action):\n",
        "    return np.random.normal(true_action_values_3arm[action], 0.1)\n",
        "\n",
        "def estimate_action_values_3arm(n_samples=1000):\n",
        "    estimated_values = [0] * 3\n",
        "    for action in range(3):\n",
        "        samples = [pull_arm_3arm(action) for _ in range(n_samples)]\n",
        "        estimated_values[action] = np.mean(samples)\n",
        "    return estimated_values\n",
        "\n",
        "# Estimate action values\n",
        "estimated_values_3arm = estimate_action_values_3arm()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(3), true_action_values_3arm, alpha=0.6, label='True Action Values')\n",
        "plt.bar(range(3), estimated_values_3arm, alpha=0.6, label='Estimated Action Values')\n",
        "plt.xlabel('Actions')\n",
        "plt.ylabel('Action Value')\n",
        "plt.legend()\n",
        "plt.title('True vs Estimated Action Values for 3-Armed Bandit')\n",
        "plt.show()\n",
        "\n",
        "estimated_values_3arm"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "96e4425b-b788-4735-a6da-c44a92dcede8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to estimate action values and standard errors through sampling\n",
        "def estimate_action_values_with_error(n_samples=1000):\n",
        "    estimated_values = [0] * 5\n",
        "    standard_errors = [0] * 5\n",
        "    for action in range(5):\n",
        "        samples = [pull_arm(action) for _ in range(n_samples)]\n",
        "        estimated_values[action] = np.mean(samples)\n",
        "        standard_errors[action] = np.std(samples) / np.sqrt(n_samples)\n",
        "    return estimated_values, standard_errors\n",
        "\n",
        "# Estimate action values and standard errors\n",
        "estimated_values, standard_errors = estimate_action_values_with_error()\n",
        "\n",
        "# Plotting with confidence intervals\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(5), estimated_values, yerr=standard_errors, alpha=0.6, label='Estimated Action Values with Error', color='b')\n",
        "plt.xlabel('Actions')\n",
        "plt.ylabel('Action Value')\n",
        "plt.title('Estimated Action Values with Confidence Intervals')\n",
        "plt.show()\n",
        "\n",
        "estimated_values, standard_errors"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "b5d349b6-157c-4d25-bcd4-6a16ebfd953b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: Vary the Number of Samples\n",
        "\n",
        "def estimate_action_values_vary_samples(n_samples=1000):\n",
        "    estimated_values = [0] * 3\n",
        "    for action in range(3):\n",
        "        samples = [pull_arm_3arm(action) for _ in range(n_samples)]\n",
        "        estimated_values[action] = np.mean(samples)\n",
        "    return estimated_values\n",
        "\n",
        "# Estimate action values with different number of samples\n",
        "estimated_values_100 = estimate_action_values_vary_samples(100)\n",
        "estimated_values_500 = estimate_action_values_vary_samples(500)\n",
        "estimated_values_2000 = estimate_action_values_vary_samples(2000)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(3), true_action_values_3arm, alpha=0.6, label='True Action Values')\n",
        "plt.bar(np.array(range(3))-0.2, estimated_values_100, alpha=0.6, width=0.2, label='Estimated with 100 samples')\n",
        "plt.bar(np.array(range(3)), estimated_values_500, alpha=0.6, width=0.2, label='Estimated with 500 samples')\n",
        "plt.bar(np.array(range(3))+0.2, estimated_values_2000, alpha=0.6, width=0.2, label='Estimated with 2000 samples')\n",
        "plt.xlabel('Actions')\n",
        "plt.ylabel('Action Value')\n",
        "plt.legend()\n",
        "plt.title('True vs Estimated Action Values with Varying Samples')\n",
        "plt.show()\n",
        "\n",
        "estimated_values_100, estimated_values_500, estimated_values_2000"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "47fa4d10-acbb-4ff8-bc61-a1bd0783233e"
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: Non-Stationary Bandit\n",
        "\n",
        "def pull_arm_non_stationary(action, time_step):\n",
        "    return np.random.normal(true_action_values_3arm[action] + 0.01 * time_step, 0.1)\n",
        "\n",
        "def estimate_action_values_non_stationary(n_samples=1000, time_step=0):\n",
        "    estimated_values = [0] * 3\n",
        "    for action in range(3):\n",
        "        samples = [pull_arm_non_stationary(action, time_step) for _ in range(n_samples)]\n",
        "        estimated_values[action] = np.mean(samples)\n",
        "    return estimated_values\n",
        "\n",
        "# Estimate action values at different time steps\n",
        "estimated_values_t0 = estimate_action_values_non_stationary(time_step=0)\n",
        "estimated_values_t10 = estimate_action_values_non_stationary(time_step=10)\n",
        "estimated_values_t20 = estimate_action_values_non_stationary(time_step=20)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(3), estimated_values_t0, alpha=0.6, label='Estimated at t=0')\n",
        "plt.bar(np.array(range(3))-0.2, estimated_values_t10, alpha=0.6, width=0.2, label='Estimated at t=10')\n",
        "plt.bar(np.array(range(3))+0.2, estimated_values_t20, alpha=0.6, width=0.2, label='Estimated at t=20')\n",
        "plt.xlabel('Actions')\n",
        "plt.ylabel('Action Value')\n",
        "plt.legend()\n",
        "plt.title('Estimated Action Values at Different Time Steps for Non-Stationary Bandit')\n",
        "plt.show()\n",
        "\n",
        "estimated_values_t0, estimated_values_t10, estimated_values_t20"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "12897b83-a3cb-438a-88f6-0f40c773e87d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise Solutions\n",
        "\n",
        "### Solution for Exercise 1\n",
        "\n",
        "In this exercise, we implemented a function to estimate the action values of a 3-armed bandit with true action values `[0.9, 0.8, 0.7]`. The function pulls each arm multiple times and averages the rewards to estimate the action values. The plot shows that the estimated action values are close to the true values.\n",
        "\n",
        "### Solution for Exercise 2\n",
        "\n",
        "We modified the function to accept the number of samples as an argument. We then ran the function with different numbers of samples (100, 500, 2000) and plotted the estimates. As expected, more samples lead to more accurate estimates.\n",
        "\n",
        "### Solution for Exercise 3\n",
        "\n",
        "In this exercise, we dealt with a non-stationary 3-armed bandit where the true action values change over time. We implemented a function that estimates the action values at different time steps (t=0, t=10, t=20). The plot shows how the estimates change over time."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "dac5879d-de5c-477e-b571-929aa94f171d"
    }
  ],
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "8e9f926a-ce11-5aa9-95c1-bd26017668fc",
        "openai_ephemeral_user_id": "d97cd37a-db81-523a-bf0d-36f1aca6eae2",
        "openai_subdivision1_iso_code": "PK-PB"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small",
    "noteable": {
      "last_delta_id": "40b47855-ea81-4230-bc8b-c8240042b200"
    },
    "nteract": {
      "version": "noteable@2.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}